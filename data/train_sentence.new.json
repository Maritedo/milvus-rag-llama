[{"sentence": "English is shown to be trans-context-free on the basis of coordinations of the respectively type that involve strictly syntactic cross-serial agreement .", "ner": [["English", 0, 0, "Material"], ["coordinations", 10, 10, "OtherScientificTerm"], ["strictly syntactic cross-serial agreement", 17, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .", "ner": [["agreement", 1, 1, "Generic"], ["nouns", 7, 7, "OtherScientificTerm"], ["reflexive pronouns", 9, 10, "OtherScientificTerm"], ["grammatical number", 20, 21, "OtherScientificTerm"], ["English", 23, 23, "Material"], ["grammatical gender", 26, 27, "OtherScientificTerm"], ["languages", 29, 29, "Material"], ["French", 32, 32, "Material"]], "relations": [[7, 7, 9, 10, "CONJUNCTION"], [26, 27, 29, 29, "FEATURE-OF"], [32, 32, 29, 29, "HYPONYM-OF"]]},
{"sentence": "The formal proof , which makes crucial use of the Interchange Lemma of Ogden et al. , is so constructed as to be valid even if English is presumed to contain grammatical sentences in which respectively operates across a pair of coordinate phrases one of whose members has fewer conjuncts than the other ; it thus goes through whatever the facts may be regarding constructions with unequal numbers of conjuncts in the scope of respectively , whereas other arguments have foundered on this problem .", "ner": [["Interchange Lemma", 10, 11, "Method"], ["English", 26, 26, "Material"]], "relations": []},
{"sentence": "In this paper , a novel method to learn the intrinsic object structure for robust visual tracking is proposed .", "ner": [["method", 6, 6, "Method"], ["intrinsic object structure", 10, 12, "OtherScientificTerm"], ["robust visual tracking", 14, 16, "Task"]], "relations": [[6, 6, 10, 12, "USED-FOR"], [10, 12, 14, 16, "USED-FOR"]]},
{"sentence": "The basic assumption is that the parameterized object state lies on a low dimensional manifold and can be learned from training data .", "ner": [["parameterized object state", 6, 8, "OtherScientificTerm"], ["low dimensional manifold", 12, 14, "OtherScientificTerm"]], "relations": [[12, 14, 6, 8, "FEATURE-OF"]]},
{"sentence": "Based on this assumption , firstly we derived the dimensionality reduction and density estimation algorithm for unsupervised learning of object intrinsic representation , the obtained non-rigid part of object state reduces even to 2 dimensions .", "ner": [["dimensionality reduction and density estimation algorithm", 9, 14, "Method"], ["unsupervised learning of object intrinsic representation", 16, 21, "Task"], ["object intrinsic representation", 19, 21, "Method"], ["non-rigid part of object state", 25, 29, "OtherScientificTerm"]], "relations": [[9, 14, 16, 21, "USED-FOR"]]},
{"sentence": "Secondly the dynamical model is derived and trained based on this intrinsic representation .", "ner": [["dynamical model", 2, 3, "Method"], ["intrinsic representation", 11, 12, "Method"]], "relations": [[11, 12, 2, 3, "USED-FOR"]]},
{"sentence": "Thirdly the learned intrinsic object structure is integrated into a particle-filter style tracker .", "ner": [["intrinsic object structure", 3, 5, "OtherScientificTerm"], ["particle-filter style tracker", 10, 12, "Method"]], "relations": [[3, 5, 10, 12, "PART-OF"]]},
{"sentence": "We will show that this intrinsic object representation has some interesting properties and based on which the newly derived dynamical model makes particle-filter style tracker more robust and reliable .", "ner": [["intrinsic object representation", 5, 7, "Method"], ["dynamical model", 19, 20, "Method"], ["particle-filter style tracker", 22, 24, "Method"]], "relations": [[19, 20, 22, 24, "USED-FOR"]]},
{"sentence": "Experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as fish twisting with self-occlusion and large inter-frame lip motion .", "ner": [["tracker", 5, 5, "Generic"], ["trackers", 11, 11, "Generic"], ["tracking of complex non-rigid motions", 14, 18, "Task"], ["complex non-rigid motions", 16, 18, "OtherScientificTerm"], ["fish twisting", 21, 22, "OtherScientificTerm"], ["self-occlusion", 24, 24, "OtherScientificTerm"], ["inter-frame lip motion", 27, 29, "OtherScientificTerm"]], "relations": [[5, 5, 11, 11, "COMPARE"], [5, 5, 14, 18, "USED-FOR"], [11, 11, 14, 18, "USED-FOR"], [21, 22, 16, 18, "HYPONYM-OF"], [24, 24, 21, 22, "FEATURE-OF"], [24, 24, 27, 29, "CONJUNCTION"], [27, 29, 21, 22, "FEATURE-OF"]]},
{"sentence": "The proposed method also has the potential to solve other type of tracking problems .", "ner": [["method", 2, 2, "Generic"], ["tracking problems", 12, 13, "Task"]], "relations": [[2, 2, 12, 13, "USED-FOR"]]},
{"sentence": "In this paper , we present a digital signal processor -LRB- DSP -RRB- implementation of real-time statistical voice conversion -LRB- VC -RRB- for silent speech enhancement and electrolaryngeal speech enhancement .", "ner": [["digital signal processor -LRB- DSP -RRB- implementation", 7, 13, "Method"], ["real-time statistical voice conversion -LRB- VC -RRB-", 15, 21, "Method"], ["silent speech enhancement", 23, 25, "Task"], ["electrolaryngeal speech enhancement", 27, 29, "Task"]], "relations": [[7, 13, 15, 21, "USED-FOR"], [15, 21, 23, 25, "USED-FOR"], [15, 21, 27, 29, "USED-FOR"], [23, 25, 27, 29, "CONJUNCTION"]]},
{"sentence": "As a silent speech interface , we focus on non-audible murmur -LRB- NAM -RRB- , which can be used in situations where audible speech is not acceptable .", "ner": [["silent speech interface", 2, 4, "OtherScientificTerm"], ["non-audible murmur -LRB- NAM -RRB-", 9, 13, "Material"], ["audible speech", 22, 23, "Material"]], "relations": []},
{"sentence": "Electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative speaking method for laryngectomees .", "ner": [["Electrolaryngeal speech", 0, 1, "Material"], ["alaryngeal speech", 9, 10, "Material"], ["speaking method", 15, 16, "Method"], ["laryngectomees", 18, 18, "OtherScientificTerm"]], "relations": [[0, 1, 9, 10, "HYPONYM-OF"], [15, 16, 9, 10, "USED-FOR"], [15, 16, 18, 18, "USED-FOR"]]},
{"sentence": "However , the sound quality of NAM and electrolaryngeal speech suffers from lack of naturalness .", "ner": [["sound quality", 3, 4, "Metric"], ["NAM", 6, 6, "Material"], ["NAM and electrolaryngeal speech", 6, 9, "Material"]], "relations": [[3, 4, 6, 9, "EVALUATE-FOR"]]},
{"sentence": "VC has proven to be one of the promising approaches to address this problem , and it has been successfully implemented on devices with sufficient computational resources .", "ner": [["VC", 0, 0, "Method"], ["it", 16, 16, "Generic"], ["devices", 22, 22, "Generic"], ["sufficient computational resources", 24, 26, "Material"]], "relations": [[22, 22, 16, 16, "USED-FOR"], [24, 26, 22, 22, "FEATURE-OF"]]},
{"sentence": "An implementation on devices that are highly portable but have limited computational resources would greatly contribute to its practical use .", "ner": [["devices", 3, 3, "Generic"], ["limited computational resources", 10, 12, "Material"]], "relations": [[10, 12, 3, 3, "FEATURE-OF"]]},
{"sentence": "In this paper we further implement real-time VC on a DSP .", "ner": [["real-time VC", 6, 7, "Method"], ["DSP", 10, 10, "OtherScientificTerm"]], "relations": [[10, 10, 6, 7, "USED-FOR"]]},
{"sentence": "To implement the two speech enhancement systems based on real-time VC , one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .", "ner": [["speech enhancement systems", 4, 6, "Method"], ["real-time VC", 9, 10, "Method"], ["one", 12, 12, "Generic"], ["NAM", 14, 14, "Material"], ["whispered voice", 17, 18, "Material"], ["other", 21, 21, "Generic"], ["electrolaryngeal speech", 23, 24, "Material"], ["natural voice", 27, 28, "Material"], ["methods", 33, 33, "Generic"], ["computational cost", 36, 37, "Metric"], ["conversion accuracy", 40, 41, "Metric"]], "relations": [[9, 10, 4, 6, "USED-FOR"], [12, 12, 4, 6, "HYPONYM-OF"], [12, 12, 21, 21, "CONJUNCTION"], [21, 21, 4, 6, "HYPONYM-OF"], [36, 37, 33, 33, "EVALUATE-FOR"], [36, 37, 40, 41, "CONJUNCTION"], [40, 41, 33, 33, "EVALUATE-FOR"]]},
{"sentence": "We conduct experimental evaluations and show that real-time VC is capable of running on a DSP with little degradation .", "ner": [["real-time VC", 7, 8, "Method"], ["DSP", 15, 15, "OtherScientificTerm"]], "relations": [[15, 15, 7, 8, "USED-FOR"]]},
{"sentence": "We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST .", "ner": [["method", 3, 3, "Generic"], ["paraphrase", 7, 7, "OtherScientificTerm"], ["machine translation evaluation measures", 20, 23, "Metric"], ["BLEU", 25, 25, "Metric"], ["NIST", 27, 27, "Metric"]], "relations": [[3, 3, 7, 7, "USED-FOR"], [7, 7, 20, 23, "USED-FOR"], [25, 25, 20, 23, "HYPONYM-OF"], [25, 25, 27, 27, "CONJUNCTION"], [27, 27, 20, 23, "HYPONYM-OF"]]},
{"sentence": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their equivalence in meaning : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .", "ner": [["paraphrases", 6, 6, "OtherScientificTerm"], ["grammaticality", 18, 18, "Metric"], ["equivalence in meaning", 31, 33, "Metric"], ["paraphrases", 40, 40, "OtherScientificTerm"], ["meaning equivalence", 43, 44, "Method"], ["entailment", 46, 46, "Method"], ["internal lexical and syntactical variation", 56, 60, "Metric"], ["paraphrases", 65, 65, "OtherScientificTerm"], ["hand-produced sets", 72, 73, "OtherScientificTerm"]], "relations": [[31, 33, 18, 18, "CONJUNCTION"], [43, 44, 40, 40, "USED-FOR"], [43, 44, 46, 46, "CONJUNCTION"], [46, 46, 40, 40, "USED-FOR"], [56, 60, 31, 33, "CONJUNCTION"], [65, 65, 72, 73, "COMPARE"]]},
{"sentence": "The paraphrase sets produced by this method thus seem adequate as reference sets to be used for MT evaluation .", "ner": [["paraphrase", 1, 1, "OtherScientificTerm"], ["method", 6, 6, "Generic"], ["MT evaluation", 17, 18, "Task"]], "relations": [[6, 6, 1, 1, "USED-FOR"]]},
{"sentence": "Graph unification remains the most expensive part of unification-based grammar parsing .", "ner": [["Graph unification", 0, 1, "Task"], ["unification-based grammar parsing", 8, 10, "Task"]], "relations": [[0, 1, 8, 10, "PART-OF"]]},
{"sentence": "We focus on one speed-up element in the design of unification algorithms : avoidance of copying of unmodified subgraphs .", "ner": [["speed-up element", 4, 5, "Method"], ["unification algorithms", 10, 11, "Method"], ["copying of unmodified subgraphs", 15, 18, "OtherScientificTerm"], ["unmodified subgraphs", 17, 18, "OtherScientificTerm"]], "relations": [[4, 5, 10, 11, "PART-OF"]]},
{"sentence": "We propose a method of attaining such a design through a method of structure-sharing which avoids log -LRB- d -RRB- overheads often associated with structure-sharing of graphs without any use of costly dependency pointers .", "ner": [["method", 3, 3, "Generic"], ["structure-sharing", 13, 13, "Method"], ["log -LRB- d -RRB- overheads", 16, 20, "OtherScientificTerm"], ["structure-sharing of graphs", 24, 26, "OtherScientificTerm"], ["dependency pointers", 32, 33, "Method"]], "relations": [[13, 13, 3, 3, "USED-FOR"]]},
{"sentence": "The proposed scheme eliminates redundant copying while maintaining the quasi-destructive scheme 's ability to avoid over copying and early copying combined with its ability to handle cyclic structures without algorithmic additions .", "ner": [["scheme", 2, 2, "Generic"], ["redundant copying", 4, 5, "OtherScientificTerm"], ["quasi-destructive scheme 's ability", 9, 12, "OtherScientificTerm"], ["over copying", 15, 16, "OtherScientificTerm"], ["early copying", 18, 19, "OtherScientificTerm"], ["cyclic structures", 26, 27, "OtherScientificTerm"]], "relations": [[2, 2, 26, 27, "USED-FOR"], [9, 12, 2, 2, "FEATURE-OF"], [15, 16, 18, 19, "CONJUNCTION"]]},
{"sentence": "We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora .", "ner": [["system", 7, 7, "Generic"], ["subcategorization dictionary", 11, 12, "OtherScientificTerm"], ["textual corpora", 14, 15, "Material"]], "relations": [[7, 7, 11, 12, "USED-FOR"], [14, 15, 7, 7, "USED-FOR"]]},
{"sentence": "Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English .", "ner": [["relative frequency of occurrence", 5, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "An initial experiment , on a sample of 14 verbs which exhibit multiple complementation patterns , demonstrates that the technique achieves accuracy comparable to previous approaches , which are all limited to a highly restricted set of subcategorization classes .", "ner": [], "relations": []},
{"sentence": "We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount", "ner": [["subcategorization dictionary", 5, 6, "OtherScientificTerm"], ["system", 10, 10, "Generic"], ["accuracy", 13, 13, "Metric"], ["parser", 16, 16, "Method"]], "relations": [[10, 10, 5, 6, "USED-FOR"], [13, 13, 16, 16, "EVALUATE-FOR"], [16, 16, 5, 6, "EVALUATE-FOR"]]},
{"sentence": "The introduction of prior knowledge has greatly enhanced numerous purely low-level driven image processing algorithms .", "ner": [["prior knowledge", 3, 4, "OtherScientificTerm"], ["low-level driven image processing algorithms", 10, 14, "Method"]], "relations": []},
{"sentence": "In this work , we focus on the problem of non-rigid image registration .", "ner": [["non-rigid image registration", 10, 12, "Task"]], "relations": []},
{"sentence": "A number of powerful registration criteria have been developed in the last decade , most prominently the criterion of maximum mutual information .", "ner": [["registration criteria", 4, 5, "Metric"], ["maximum mutual information", 19, 21, "Metric"]], "relations": [[19, 21, 4, 5, "HYPONYM-OF"]]},
{"sentence": "Although this criterion provides for good registration results in many applications , it remains a purely low-level criterion .", "ner": [["criterion", 2, 2, "Generic"], ["it", 12, 12, "Generic"], ["low-level criterion", 16, 17, "Metric"]], "relations": [[16, 17, 12, 12, "FEATURE-OF"]]},
{"sentence": "As a consequence , registration results will deteriorate once this low-level information is corrupted , due to noise , partial occlusions or missing image structure .", "ner": [["registration", 4, 4, "Task"], ["low-level information", 10, 11, "OtherScientificTerm"], ["noise", 17, 17, "OtherScientificTerm"], ["partial occlusions", 19, 20, "OtherScientificTerm"], ["missing image structure", 22, 24, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper , we will develop a Bayesian framework that allows to impose statistically learned prior knowledge about the joint intensity distribution into image registration methods .", "ner": [["Bayesian framework", 8, 9, "Method"], ["statistically learned prior knowledge", 14, 17, "OtherScientificTerm"], ["joint intensity distribution", 20, 22, "OtherScientificTerm"], ["image registration methods", 24, 26, "Method"]], "relations": [[8, 9, 24, 26, "USED-FOR"], [14, 17, 24, 26, "USED-FOR"], [20, 22, 14, 17, "FEATURE-OF"]]},
{"sentence": "The prior is given by a kernel density estimate on the space of joint intensity distributions computed from a representative set of pre-registered image pairs .", "ner": [["prior", 1, 1, "Generic"], ["kernel density estimate", 6, 8, "Method"], ["joint intensity distributions", 13, 15, "OtherScientificTerm"], ["pre-registered image pairs", 22, 24, "Material"]], "relations": [[6, 8, 1, 1, "USED-FOR"], [6, 8, 13, 15, "USED-FOR"], [22, 24, 13, 15, "USED-FOR"]]},
{"sentence": "This nonparametric prior accurately models previously learned intensity relations between various image modalities and slice locations .", "ner": [["intensity relations", 7, 8, "OtherScientificTerm"], ["image modalities", 11, 12, "OtherScientificTerm"], ["slice locations", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "Experimental results demonstrate that the resulting registration process is more robust to missing low-level information as it favors intensity correspondences statistically consistent with the learned intensity distributions .", "ner": [["registration process", 6, 7, "Method"], ["missing low-level information", 12, 14, "OtherScientificTerm"], ["it", 16, 16, "OtherScientificTerm"], ["intensity correspondences", 18, 19, "OtherScientificTerm"], ["intensity distributions", 25, 26, "OtherScientificTerm"]], "relations": [[6, 7, 12, 14, "USED-FOR"], [16, 16, 18, 19, "USED-FOR"]]},
{"sentence": "There is an increased interest in the efficient creation of city models , be it virtual or as-built .", "ner": [["creation of city models", 8, 11, "Task"]], "relations": []},
{"sentence": "We present a method for synthesizing complex , photo-realistic facade images , from a single example .", "ner": [["method", 3, 3, "Generic"], ["synthesizing complex , photo-realistic facade images", 5, 10, "Task"]], "relations": [[3, 3, 5, 10, "USED-FOR"]]},
{"sentence": "After parsing the example image into its semantic components , a tiling for it is generated .", "ner": [["semantic components", 7, 8, "Method"], ["tiling", 11, 11, "OtherScientificTerm"]], "relations": [[11, 11, 7, 8, "USED-FOR"]]},
{"sentence": "Novel tilings can then be created , yielding facade textures with different dimensions or with occluded parts inpainted .", "ner": [["tilings", 1, 1, "OtherScientificTerm"], ["facade textures", 8, 9, "OtherScientificTerm"], ["occluded parts inpainted", 15, 17, "OtherScientificTerm"]], "relations": [[15, 17, 8, 9, "FEATURE-OF"]]},
{"sentence": "A genetic algorithm guides the novel facades as well as inpainted parts to be consistent with the example , both in terms of their overall structure and their detailed textures .", "ner": [["genetic algorithm", 1, 2, "Generic"], ["facades", 6, 6, "OtherScientificTerm"], ["inpainted parts", 10, 11, "OtherScientificTerm"]], "relations": [[1, 2, 6, 6, "USED-FOR"], [1, 2, 10, 11, "USED-FOR"]]},
{"sentence": "Promising results for multiple standard datasets -- in particular for the different building styles they contain -- demonstrate the potential of the method .", "ner": [["multiple standard datasets", 3, 5, "Generic"], ["method", 22, 22, "Generic"]], "relations": [[3, 5, 22, 22, "EVALUATE-FOR"]]},
{"sentence": "We introduce a new interactive corpus exploration tool called InfoMagnets .", "ner": [["interactive corpus exploration tool", 4, 7, "Method"], ["InfoMagnets", 9, 9, "Method"]], "relations": [[9, 9, 4, 7, "HYPONYM-OF"]]},
{"sentence": "InfoMagnets aims at making exploratory corpus analysis accessible to researchers who are not experts in text mining .", "ner": [["InfoMagnets", 0, 0, "Method"], ["exploratory corpus analysis", 4, 6, "Task"], ["text mining", 15, 16, "Task"]], "relations": [[0, 0, 4, 6, "USED-FOR"]]},
{"sentence": "As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains : tutorial dialogue -LRB- Kumar et al. , submitted -RRB- and on-line communities -LRB- Arguello et al. , 2006 -RRB- .", "ner": [["it", 8, 8, "Generic"], ["domains", 28, 28, "Generic"], ["tutorial dialogue", 30, 31, "Material"], ["on-line communities", 40, 41, "Material"]], "relations": [[8, 8, 28, 28, "USED-FOR"], [30, 31, 28, 28, "HYPONYM-OF"], [30, 31, 40, 41, "CONJUNCTION"], [40, 41, 28, 28, "HYPONYM-OF"]]},
{"sentence": "As an educational tool , it has been used as part of a unit on protocol analysis in an Educational Research Methods course .", "ner": [["educational tool", 2, 3, "Method"], ["it", 5, 5, "Generic"], ["protocol analysis", 15, 16, "Task"]], "relations": [[2, 3, 15, 16, "USED-FOR"]]},
{"sentence": "Sources of training data suitable for language modeling of conversational speech are limited .", "ner": [["language modeling", 6, 7, "Method"], ["conversational speech", 9, 10, "Material"]], "relations": [[9, 10, 6, 7, "USED-FOR"]]},
{"sentence": "In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger performance gains from the data by using class-dependent interpolation of N-grams .", "ner": [["recognition task", 27, 28, "Task"], ["class-dependent interpolation of N-grams", 46, 49, "Method"]], "relations": [[46, 49, 27, 28, "USED-FOR"]]},
{"sentence": "We present a method for detecting 3D objects using multi-modalities .", "ner": [["method", 3, 3, "Generic"], ["detecting 3D objects", 5, 7, "Task"], ["multi-modalities", 9, 9, "OtherScientificTerm"]], "relations": [[3, 3, 5, 7, "USED-FOR"], [9, 9, 3, 3, "USED-FOR"]]},
{"sentence": "While it is generic , we demonstrate it on the combination of an image and a dense depth map which give complementary object information .", "ner": [["it", 1, 1, "Generic"], ["it", 7, 7, "Generic"], ["image", 13, 13, "OtherScientificTerm"], ["dense depth map", 16, 18, "OtherScientificTerm"], ["complementary object information", 21, 23, "OtherScientificTerm"]], "relations": [[1, 1, 7, 7, "USED-FOR"], [13, 13, 7, 7, "USED-FOR"], [13, 13, 16, 18, "CONJUNCTION"], [16, 18, 7, 7, "USED-FOR"], [21, 23, 16, 18, "FEATURE-OF"]]},
{"sentence": "It works in real-time , under heavy clutter , does not require a time consuming training stage , and can handle untextured objects .", "ner": [["It", 0, 0, "Generic"], ["time consuming training stage", 13, 16, "OtherScientificTerm"], ["untextured objects", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "It is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities .", "ner": [["It", 0, 0, "Generic"], ["templates", 8, 8, "OtherScientificTerm"], ["modalities", 13, 13, "OtherScientificTerm"], ["commodity hardware", 22, 23, "OtherScientificTerm"], ["approach", 26, 26, "Generic"], ["state-of-the-art methods", 29, 30, "Generic"], ["single modalities", 32, 33, "OtherScientificTerm"]], "relations": [[8, 8, 13, 13, "USED-FOR"], [26, 26, 29, 30, "COMPARE"], [26, 26, 32, 33, "USED-FOR"], [29, 30, 32, 33, "USED-FOR"]]},
{"sentence": "The compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .", "ner": [["compact description of a video sequence", 1, 6, "Task"], ["image map", 10, 11, "OtherScientificTerm"], ["dominant motion", 14, 15, "OtherScientificTerm"], ["domains", 20, 20, "Generic"], ["video browsing and retrieval", 23, 26, "Task"], ["compression", 28, 28, "Task"], ["mosaicing", 30, 30, "Task"], ["visual summarization", 33, 34, "Task"]], "relations": [[1, 6, 20, 20, "USED-FOR"], [10, 11, 1, 6, "USED-FOR"], [10, 11, 14, 15, "CONJUNCTION"], [14, 15, 1, 6, "USED-FOR"], [23, 26, 20, 20, "HYPONYM-OF"], [23, 26, 28, 28, "CONJUNCTION"], [28, 28, 20, 20, "HYPONYM-OF"], [28, 28, 30, 30, "CONJUNCTION"], [30, 30, 20, 20, "HYPONYM-OF"], [30, 30, 33, 34, "CONJUNCTION"]]},
{"sentence": "Building such a representation requires the capability to register all the frames with respect to the dominant object in the scene , a task which has been , in the past , addressed through temporally localized motion estimates .", "ner": [["representation", 3, 3, "Generic"], ["task", 23, 23, "Generic"], ["localized motion estimates", 35, 37, "Method"]], "relations": [[35, 37, 23, 23, "USED-FOR"]]},
{"sentence": "In this paper , we show how the lack of temporal consistency associated with such estimates can undermine the validity of the dominant motion assumption , leading to oscillation between different scene interpretations and poor registration .", "ner": [["lack of temporal consistency", 8, 11, "OtherScientificTerm"], ["temporal consistency", 10, 11, "OtherScientificTerm"], ["estimates", 15, 15, "Generic"], ["validity of the dominant motion assumption", 19, 24, "OtherScientificTerm"], ["dominant motion assumption", 22, 24, "OtherScientificTerm"], ["oscillation between different scene interpretations and poor registration", 28, 35, "OtherScientificTerm"]], "relations": []},
{"sentence": "To avoid this oscillation , we augment the motion model with a generic temporal constraint which increases the robustness against competing interpretations , leading to more meaningful content summarization .", "ner": [["oscillation", 3, 3, "Generic"], ["motion model", 8, 9, "Method"], ["generic temporal constraint", 12, 14, "OtherScientificTerm"], ["robustness", 18, 18, "Metric"], ["content summarization", 27, 28, "Task"]], "relations": [[12, 14, 8, 9, "USED-FOR"], [12, 14, 27, 28, "USED-FOR"], [18, 18, 12, 14, "EVALUATE-FOR"]]},
{"sentence": "In cross-domain learning , there is a more challenging problem that the domain divergence involves more than one dominant factors , e.g. , different viewpoints , various resolutions and changing illuminations .", "ner": [["cross-domain learning", 1, 2, "Task"], ["domain divergence", 12, 13, "OtherScientificTerm"], ["dominant factors", 18, 19, "Generic"], ["viewpoints", 24, 24, "OtherScientificTerm"], ["resolutions", 27, 27, "OtherScientificTerm"], ["illuminations", 30, 30, "OtherScientificTerm"]], "relations": [[18, 19, 12, 13, "PART-OF"], [24, 24, 18, 19, "HYPONYM-OF"], [24, 24, 27, 27, "CONJUNCTION"], [27, 27, 18, 19, "HYPONYM-OF"], [27, 27, 30, 30, "CONJUNCTION"]]},
{"sentence": "Fortunately , an intermediate domain could often be found to build a bridge across them to facilitate the learning problem .", "ner": [["intermediate domain", 3, 4, "OtherScientificTerm"], ["learning problem", 18, 19, "Task"]], "relations": [[3, 4, 18, 19, "USED-FOR"]]},
{"sentence": "In this paper , we propose a Coupled Marginalized Denoising Auto-encoders framework to address the cross-domain problem .", "ner": [["Coupled Marginalized Denoising Auto-encoders framework", 7, 11, "Method"], ["cross-domain problem", 15, 16, "Task"]], "relations": [[7, 11, 15, 16, "USED-FOR"]]},
{"sentence": "Specifically , we design two marginalized denoising auto-encoders , one for the target and the other for source as well as the intermediate one .", "ner": [["marginalized denoising auto-encoders", 5, 7, "Method"], ["one", 9, 9, "Generic"], ["other", 15, 15, "Generic"]], "relations": [[9, 9, 5, 7, "HYPONYM-OF"], [9, 9, 15, 15, "CONJUNCTION"], [15, 15, 5, 7, "HYPONYM-OF"]]},
{"sentence": "To better couple the two denoising auto-encoders learning , we incorporate a feature mapping , which tends to transfer knowledge between the intermediate domain and the target one .", "ner": [["denoising auto-encoders learning", 5, 7, "Method"], ["feature mapping", 12, 13, "Method"], ["intermediate domain", 22, 23, "OtherScientificTerm"]], "relations": [[12, 13, 5, 7, "PART-OF"], [12, 13, 22, 23, "USED-FOR"]]},
{"sentence": "Furthermore , the maximum margin criterion , e.g. , intra-class com-pactness and inter-class penalty , on the output layer is imposed to seek more discriminative features across different domains .", "ner": [["maximum margin criterion", 3, 5, "OtherScientificTerm"], ["intra-class com-pactness", 9, 10, "OtherScientificTerm"], ["inter-class penalty", 12, 13, "OtherScientificTerm"], ["discriminative features", 24, 25, "OtherScientificTerm"]], "relations": [[9, 10, 3, 5, "HYPONYM-OF"], [9, 10, 12, 13, "CONJUNCTION"], [12, 13, 3, 5, "HYPONYM-OF"]]},
{"sentence": "Extensive experiments on two tasks have demonstrated the superiority of our method over the state-of-the-art methods .", "ner": [["tasks", 4, 4, "Generic"], ["method", 11, 11, "Generic"], ["state-of-the-art methods", 14, 15, "Generic"]], "relations": [[4, 4, 11, 11, "EVALUATE-FOR"], [11, 11, 14, 15, "COMPARE"]]},
{"sentence": "In this paper , we aim to automatically render aging faces in a personalized way .", "ner": [["aging faces", 9, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "Basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a linear combination of these patterns expresses a particular personalized aging process .", "ner": [["age-group specific dictionaries", 5, 7, "Method"], ["dictionary bases", 13, 14, "OtherScientificTerm"], ["aging process pattern", 27, 29, "OtherScientificTerm"], ["linear combination", 37, 38, "Method"], ["patterns", 41, 41, "Generic"], ["personalized aging process", 45, 47, "Task"]], "relations": [[13, 14, 5, 7, "PART-OF"], [37, 38, 45, 47, "USED-FOR"], [41, 41, 37, 38, "USED-FOR"]]},
{"sentence": "Moreover , two factors are taken into consideration in the dictionary learning process .", "ner": [["dictionary learning process", 10, 12, "Method"]], "relations": []},
{"sentence": "First , beyond the aging dictionaries , each subject may have extra personalized facial characteristics , e.g. mole , which are invariant in the aging process .", "ner": [["aging dictionaries", 4, 5, "Method"], ["personalized facial characteristics", 12, 14, "OtherScientificTerm"], ["mole", 17, 17, "OtherScientificTerm"], ["aging process", 24, 25, "Task"]], "relations": [[17, 17, 12, 14, "HYPONYM-OF"]]},
{"sentence": "Second , it is challenging or even impossible to collect faces of all age groups for a particular subject , yet much easier and more practical to get face pairs from neighboring age groups .", "ner": [], "relations": []},
{"sentence": "Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups .", "ner": [["personality-aware coupled reconstruction loss", 2, 5, "OtherScientificTerm"], ["dictionaries", 11, 11, "Generic"]], "relations": [[2, 5, 11, 11, "USED-FOR"]]},
{"sentence": "Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression , as well as the performance gain for cross-age face verification by synthesizing aging faces .", "ner": [["solution", 9, 9, "Generic"], ["state-of-the-arts", 12, 12, "Generic"], ["personalized aging progression", 16, 18, "OtherScientificTerm"], ["cross-age face verification", 27, 29, "Task"], ["synthesizing aging faces", 31, 33, "Method"]], "relations": [[9, 9, 12, 12, "COMPARE"], [9, 9, 16, 18, "USED-FOR"], [12, 12, 16, 18, "USED-FOR"], [31, 33, 27, 29, "USED-FOR"]]},
{"sentence": "We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction .", "ner": [["model", 7, 7, "Generic"], ["structure of communicative context", 10, 13, "OtherScientificTerm"], ["dialogue interaction", 15, 16, "Material"]], "relations": [[7, 7, 10, 13, "USED-FOR"], [15, 16, 10, 13, "FEATURE-OF"]]},
{"sentence": "The relationships between the interacting partners are considered as system of three automata representing the partners of the dialogue and environment .", "ner": [], "relations": []},
{"sentence": "Visitors who browse the web from wireless PDAs , cell phones , and pagers are frequently stymied by web interfaces optimized for desktop PCs .", "ner": [["web interfaces", 18, 19, "Method"], ["desktop PCs", 22, 23, "OtherScientificTerm"]], "relations": [[18, 19, 22, 23, "USED-FOR"]]},
{"sentence": "Simply replacing graphics with text and reformatting tables does not solve the problem , because deep link structures can still require minutes to traverse .", "ner": [["deep link structures", 15, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper we develop an algorithm , MINPATH , that automatically improves wireless web navigation by suggesting useful shortcut links in real time .", "ner": [["algorithm", 6, 6, "Generic"], ["MINPATH", 8, 8, "Method"], ["wireless web navigation", 13, 15, "Task"]], "relations": [[6, 6, 13, 15, "USED-FOR"], [6, 6, 13, 15, "USED-FOR"]]},
{"sentence": "MINPATH finds shortcuts by using a learned model of web visitor behavior to estimate the savings of shortcut links , and suggests only the few best links .", "ner": [["MINPATH", 0, 0, "Method"], ["model", 7, 7, "Generic"], ["web visitor behavior", 9, 11, "OtherScientificTerm"], ["savings of shortcut links", 15, 18, "Task"]], "relations": [[7, 7, 0, 0, "USED-FOR"], [7, 7, 9, 11, "USED-FOR"], [7, 7, 15, 18, "USED-FOR"]]},
{"sentence": "We explore a variety of predictive models , including Na \u00a8 \u0131ve Bayes mixture models and mixtures of Markov models , and report empirical evidence that MINPATH finds useful shortcuts that save substantial navigational effort .", "ner": [["predictive models", 5, 6, "Generic"], ["Na \u00a8 \u0131ve Bayes mixture models", 9, 14, "Method"], ["mixtures of Markov models", 16, 19, "Method"], ["MINPATH", 26, 26, "Method"]], "relations": [[9, 14, 5, 6, "HYPONYM-OF"], [9, 14, 16, 19, "CONJUNCTION"], [16, 19, 5, 6, "HYPONYM-OF"]]},
{"sentence": "This paper describes a particular approach to parsing that utilizes recent advances in unification-based parsing and in classification-based knowledge representation .", "ner": [["approach", 5, 5, "Generic"], ["parsing", 7, 7, "Task"], ["unification-based parsing", 13, 14, "Task"], ["classification-based knowledge representation", 17, 19, "Task"]], "relations": [[5, 5, 7, 7, "USED-FOR"], [13, 14, 5, 5, "USED-FOR"], [17, 19, 5, 5, "USED-FOR"], [17, 19, 13, 14, "CONJUNCTION"]]},
{"sentence": "As unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information , they begin to share many of the properties that have been developed in KL-ONE-like knowledge representation systems .", "ner": [["unification-based grammatical frameworks", 1, 3, "Method"], ["linguistic information", 11, 12, "OtherScientificTerm"], ["they", 14, 14, "Generic"], ["KL-ONE-like knowledge representation systems", 27, 30, "Method"]], "relations": [[1, 3, 11, 12, "USED-FOR"], [27, 30, 14, 14, "USED-FOR"]]},
{"sentence": "This commonality suggests that some of the classification-based representation techniques can be applied to unification-based linguistic descriptions .", "ner": [["classification-based representation techniques", 7, 9, "Method"], ["unification-based linguistic descriptions", 14, 16, "OtherScientificTerm"]], "relations": [[7, 9, 14, 16, "USED-FOR"]]},
{"sentence": "This merging supports the integration of semantic and syntactic information into the same system , simultaneously subject to the same types of processes , in an efficient manner .", "ner": [["semantic and syntactic information", 6, 9, "OtherScientificTerm"], ["system", 13, 13, "Generic"]], "relations": [[6, 9, 13, 13, "USED-FOR"]]},
{"sentence": "The result is expected to be more efficient parsing due to the increased organization of knowledge .", "ner": [["parsing", 8, 8, "Method"]], "relations": []},
{"sentence": "The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which parsing is characterized as an inference process called incremental description refinement .", "ner": [["KL-ONE style representation", 4, 6, "Method"], ["parsing", 8, 8, "Task"], ["semantic interpretation", 10, 11, "Task"], ["PSI-KLONE system", 17, 18, "Method"], ["parsing", 25, 25, "Task"], ["inference process", 30, 31, "Method"], ["incremental description refinement", 33, 35, "Method"]], "relations": [[4, 6, 8, 8, "USED-FOR"], [4, 6, 10, 11, "USED-FOR"], [8, 8, 10, 11, "CONJUNCTION"], [17, 18, 4, 6, "USED-FOR"], [33, 35, 25, 25, "USED-FOR"], [33, 35, 30, 31, "HYPONYM-OF"]]},
{"sentence": "In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system , a language tutoring application for deaf learners of written English .", "ner": [["user knowledge modeling architecture", 7, 10, "Method"], ["ICICLE system", 13, 14, "Task"], ["language tutoring application", 17, 19, "Task"], ["deaf learners", 21, 22, "OtherScientificTerm"], ["written English", 24, 25, "Material"]], "relations": [[7, 10, 13, 14, "USED-FOR"], [13, 14, 17, 19, "HYPONYM-OF"], [17, 19, 21, 22, "USED-FOR"], [24, 25, 17, 19, "USED-FOR"]]},
{"sentence": "The model will represent the language proficiency of the user and is designed to be referenced during both writing analysis and feedback production .", "ner": [["model", 1, 1, "Generic"], ["writing analysis", 18, 19, "Task"], ["feedback production", 21, 22, "Task"]], "relations": [[1, 1, 18, 19, "USED-FOR"], [1, 1, 21, 22, "USED-FOR"], [18, 19, 21, 22, "CONJUNCTION"]]},
{"sentence": "We motivate our model design by citing relevant research on second language and cognitive skill acquisition , and briefly discuss preliminary empirical evidence supporting the design .", "ner": [["model design", 3, 4, "Generic"], ["second language and cognitive skill acquisition", 10, 15, "Task"], ["design", 25, 25, "Generic"]], "relations": [[10, 15, 3, 4, "USED-FOR"]]},
{"sentence": "We conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling user proficiency at a high level of granularity and specificity .", "ner": [["design", 6, 6, "Generic"], ["language assessment / correction application", 17, 21, "Task"], ["user proficiency", 24, 25, "OtherScientificTerm"], ["granularity", 31, 31, "Metric"], ["specificity", 33, 33, "Metric"]], "relations": [[6, 6, 17, 21, "USED-FOR"], [6, 6, 24, 25, "USED-FOR"], [31, 31, 24, 25, "EVALUATE-FOR"], [31, 31, 33, 33, "CONJUNCTION"], [33, 33, 24, 25, "EVALUATE-FOR"]]},
{"sentence": "Constraint propagation is one of the key techniques in constraint programming , and a large body of work has built up around it .", "ner": [["Constraint propagation", 0, 1, "Method"], ["constraint programming", 9, 10, "Method"]], "relations": [[0, 1, 9, 10, "PART-OF"]]},
{"sentence": "Special-purpose constraint propagation algorithms frequently make implicit use of short supports -- by examining a subset of the variables , they can infer support -LRB- a justification that a variable-value pair still forms part of a solution to the constraint -RRB- for all other variables and values and save substantial work .", "ner": [["Special-purpose constraint propagation algorithms", 0, 3, "Method"]], "relations": []},
{"sentence": "Recently short supports have been used in general purpose prop-agators , and -LRB- when the constraint is amenable to short supports -RRB- speed ups of more than three orders of magnitude have been demonstrated .", "ner": [["general purpose prop-agators", 7, 9, "Method"], ["constraint", 15, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper we present SHORTSTR2 , a development of the Simple Tabular Reduction algorithm STR2 + .", "ner": [["SHORTSTR2", 5, 5, "Method"], ["Simple Tabular Reduction algorithm STR2 +", 11, 16, "Method"]], "relations": [[11, 16, 5, 5, "USED-FOR"]]},
{"sentence": "We show that SHORTSTR2 is complementary to the existing algorithms SHORTGAC and HAGGISGAC that exploit short supports , while being much simpler .", "ner": [["SHORTSTR2", 3, 3, "Method"], ["SHORTGAC", 10, 10, "Method"], ["HAGGISGAC", 12, 12, "Method"]], "relations": [[3, 3, 10, 10, "COMPARE"], [3, 3, 12, 12, "COMPARE"], [10, 10, 12, 12, "CONJUNCTION"]]},
{"sentence": "When a constraint is amenable to short supports , the short support set can be exponentially smaller than the full-length support set .", "ner": [["constraint", 2, 2, "OtherScientificTerm"], ["short supports", 6, 7, "OtherScientificTerm"], ["short support set", 10, 12, "OtherScientificTerm"], ["full-length support set", 19, 21, "OtherScientificTerm"]], "relations": [[10, 12, 19, 21, "COMPARE"]]},
{"sentence": "Therefore SHORTSTR2 can efficiently propagate many constraints that STR2 + can not even load into memory .", "ner": [["SHORTSTR2", 1, 1, "Method"], ["constraints", 6, 6, "OtherScientificTerm"], ["STR2 +", 8, 9, "Method"]], "relations": []},
{"sentence": "We also show that SHORTSTR2 can be combined with a simple algorithm to identify short supports from full-length supports , to provide a superior drop-in replacement for STR2 + .", "ner": [["SHORTSTR2", 4, 4, "Method"], ["algorithm", 11, 11, "Generic"], ["short supports", 14, 15, "OtherScientificTerm"], ["full-length supports", 17, 18, "OtherScientificTerm"], ["drop-in replacement", 24, 25, "OtherScientificTerm"], ["STR2 +", 27, 28, "Method"]], "relations": [[4, 4, 14, 15, "USED-FOR"], [4, 4, 24, 25, "USED-FOR"], [11, 11, 4, 4, "CONJUNCTION"], [11, 11, 14, 15, "USED-FOR"], [17, 18, 4, 4, "USED-FOR"], [17, 18, 4, 4, "USED-FOR"], [17, 18, 11, 11, "USED-FOR"], [17, 18, 11, 11, "USED-FOR"], [24, 25, 27, 28, "USED-FOR"]]},
{"sentence": "We propose a detection method for orthographic variants caused by transliteration in a large corpus .", "ner": [["detection method", 3, 4, "Method"], ["orthographic variants", 6, 7, "OtherScientificTerm"], ["transliteration", 10, 10, "Task"]], "relations": [[3, 4, 6, 7, "USED-FOR"]]},
{"sentence": "The method employs two similarities .", "ner": [["method", 1, 1, "Generic"], ["similarities", 4, 4, "Generic"]], "relations": [[4, 4, 1, 1, "USED-FOR"]]},
{"sentence": "One is string similarity based on edit distance .", "ner": [["string similarity", 2, 3, "OtherScientificTerm"], ["edit distance", 6, 7, "Method"]], "relations": [[6, 7, 2, 3, "USED-FOR"]]},
{"sentence": "The other is contextual similarity by a vector space model .", "ner": [["contextual similarity", 3, 4, "OtherScientificTerm"], ["vector space model", 7, 9, "Method"]], "relations": [[7, 9, 3, 4, "USED-FOR"]]},
{"sentence": "Experimental results show that the method performed a 0.889 F-measure in an open test .", "ner": [["method", 5, 5, "Generic"], ["F-measure", 9, 9, "Metric"]], "relations": [[9, 9, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "Uncertainty handling plays an important role during shape tracking .", "ner": [["Uncertainty handling", 0, 1, "Task"], ["shape tracking", 7, 8, "Task"]], "relations": [[0, 1, 7, 8, "USED-FOR"]]},
{"sentence": "We have recently shown that the fusion of measurement information with system dynamics and shape priors greatly improves the tracking performance for very noisy images such as ultrasound sequences -LSB- 22 -RSB- .", "ner": [["fusion of measurement information with system dynamics and shape priors", 6, 15, "Method"], ["tracking", 19, 19, "Task"], ["noisy images", 23, 24, "Material"], ["ultrasound sequences", 27, 28, "Material"]], "relations": [[6, 15, 19, 19, "USED-FOR"], [19, 19, 23, 24, "USED-FOR"], [27, 28, 23, 24, "HYPONYM-OF"]]},
{"sentence": "Nevertheless , this approach required user initialization of the tracking process .", "ner": [["approach", 3, 3, "Generic"], ["user initialization", 5, 6, "OtherScientificTerm"], ["tracking process", 9, 10, "Method"]], "relations": [[5, 6, 3, 3, "USED-FOR"], [5, 6, 9, 10, "USED-FOR"]]},
{"sentence": "This paper solves the automatic initial-ization problem by performing boosted shape detection as a generic measurement process and integrating it in our tracking framework .", "ner": [["automatic initial-ization problem", 4, 6, "Task"], ["boosted shape detection", 9, 11, "Method"], ["generic measurement process", 14, 16, "Method"], ["it", 19, 19, "Generic"], ["tracking framework", 22, 23, "Method"]], "relations": [[9, 11, 4, 6, "USED-FOR"], [14, 16, 9, 11, "USED-FOR"], [19, 19, 22, 23, "PART-OF"]]},
{"sentence": "We show how to propagate the local detection uncertainties of multiple shape candidates during shape alignment , fusion with the predicted shape prior , and fusion with subspace constraints .", "ner": [["local detection uncertainties", 6, 8, "OtherScientificTerm"], ["local detection uncertainties of multiple shape candidates", 6, 12, "OtherScientificTerm"], ["shape alignment", 14, 15, "OtherScientificTerm"], ["predicted shape prior", 20, 22, "OtherScientificTerm"], ["subspace constraints", 27, 28, "OtherScientificTerm"]], "relations": []},
{"sentence": "As a result , we treat all sources of information in a unified way and derive the posterior shape model as the shape with the maximum likelihood .", "ner": [["posterior shape model", 17, 19, "Method"], ["maximum likelihood", 25, 26, "OtherScientificTerm"]], "relations": [[25, 26, 17, 19, "USED-FOR"]]},
{"sentence": "Our framework is applied for the automatic tracking of endocardium in ultrasound sequences of the human heart .", "ner": [["framework", 1, 1, "Generic"], ["automatic tracking of endocardium", 6, 9, "Task"], ["endocardium", 9, 9, "OtherScientificTerm"], ["ultrasound sequences of the human heart", 11, 16, "Material"]], "relations": [[1, 1, 6, 9, "USED-FOR"], [9, 9, 11, 16, "PART-OF"]]},
{"sentence": "Reliable detection and robust tracking results are achieved when compared to existing approaches and inter-expert variations .", "ner": [["detection", 1, 1, "Task"], ["tracking", 4, 4, "Task"], ["approaches", 12, 12, "Generic"], ["inter-expert variations", 14, 15, "OtherScientificTerm"]], "relations": [[1, 1, 4, 4, "CONJUNCTION"], [12, 12, 14, 15, "CONJUNCTION"]]},
{"sentence": "We present a syntax-based constraint for word alignment , known as the cohesion constraint .", "ner": [["syntax-based constraint", 3, 4, "OtherScientificTerm"], ["word alignment", 6, 7, "Task"], ["cohesion constraint", 12, 13, "OtherScientificTerm"]], "relations": [[3, 4, 6, 7, "USED-FOR"], [12, 13, 3, 4, "HYPONYM-OF"]]},
{"sentence": "It requires disjoint English phrases to be mapped to non-overlapping intervals in the French sentence .", "ner": [["It", 0, 0, "Generic"], ["English phrases", 3, 4, "Material"], ["French sentence", 13, 14, "Material"]], "relations": [[3, 4, 0, 0, "USED-FOR"]]},
{"sentence": "We evaluate the utility of this constraint in two different algorithms .", "ner": [["constraint", 6, 6, "Generic"], ["algorithms", 10, 10, "Generic"]], "relations": [[10, 10, 6, 6, "EVALUATE-FOR"]]},
{"sentence": "The results show that it can provide a significant improvement in alignment quality .", "ner": [["it", 4, 4, "Generic"], ["alignment quality", 11, 12, "Metric"]], "relations": [[11, 12, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "This paper considers the problem of automatic assessment of local coherence .", "ner": [["automatic assessment of local coherence", 6, 10, "Task"]], "relations": []},
{"sentence": "We present a novel entity-based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text .", "ner": [["entity-based representation of discourse", 4, 7, "Method"], ["Centering Theory", 12, 13, "Method"], ["raw text", 20, 21, "Material"]], "relations": [[12, 13, 4, 7, "USED-FOR"], [20, 21, 4, 7, "USED-FOR"]]},
{"sentence": "We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function .", "ner": [["coherence assessment", 2, 3, "Task"], ["ranking learning problem", 6, 8, "Task"], ["discourse representation", 14, 15, "Method"], ["ranking function", 22, 23, "OtherScientificTerm"]], "relations": [[6, 8, 2, 3, "USED-FOR"], [14, 15, 22, 23, "USED-FOR"]]},
{"sentence": "Our experiments demonstrate that the induced model achieves significantly higher accuracy than a state-of-the-art coherence model .", "ner": [["induced model", 5, 6, "Method"], ["accuracy", 10, 10, "Metric"], ["coherence model", 14, 15, "Method"]], "relations": [[5, 6, 14, 15, "COMPARE"], [10, 10, 5, 6, "EVALUATE-FOR"], [10, 10, 14, 15, "EVALUATE-FOR"]]},
{"sentence": "This paper introduces a robust interactive method for speech understanding .", "ner": [["robust interactive method", 4, 6, "Generic"], ["speech understanding", 8, 9, "Task"]], "relations": [[4, 6, 8, 9, "USED-FOR"]]},
{"sentence": "The generalized LR parsing is enhanced in this approach .", "ner": [["generalized LR parsing", 1, 3, "Method"], ["approach", 8, 8, "Generic"]], "relations": [[8, 8, 1, 3, "USED-FOR"]]},
{"sentence": "Parsing proceeds from left to right correcting minor errors .", "ner": [["Parsing", 0, 0, "Method"]], "relations": []},
{"sentence": "When a very noisy portion is detected , the parser skips that portion using a fake non-terminal symbol .", "ner": [["parser", 9, 9, "Method"], ["non-terminal symbol", 16, 17, "OtherScientificTerm"]], "relations": [[16, 17, 9, 9, "USED-FOR"]]},
{"sentence": "The unidentified portion is resolved by re-utterance of that portion which is parsed very efficiently by using the parse record of the first utterance .", "ner": [], "relations": []},
{"sentence": "The user does not have to speak the whole sentence again .", "ner": [], "relations": []},
{"sentence": "This method is also capable of handling unknown words , which is important in practical systems .", "ner": [["method", 1, 1, "Generic"], ["unknown words", 7, 8, "OtherScientificTerm"]], "relations": [[1, 1, 7, 8, "USED-FOR"]]},
{"sentence": "Detected unknown words can be incrementally incorporated into the dictionary after the interaction with the user .", "ner": [], "relations": []},
{"sentence": "A pilot system has shown great effectiveness of this approach .", "ner": [["approach", 9, 9, "Generic"]], "relations": []},
{"sentence": "This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams -LRB- 87.2 % -96.7 % accuracy depending on classification method -RRB- .", "ner": [["medium-length speeches", 15, 16, "Material"], ["EUROPARL corpus", 19, 20, "Material"], ["frequency counts of word n-grams", 25, 29, "Method"], ["word n-grams", 28, 29, "OtherScientificTerm"], ["accuracy", 35, 35, "Metric"], ["classification method", 38, 39, "Method"]], "relations": [[15, 16, 19, 20, "PART-OF"], [35, 35, 38, 39, "EVALUATE-FOR"]]},
{"sentence": "The paper also examines in detail which positive markers are most powerful and identifies a number of linguistic aspects as well as culture - and domain-related ones .", "ner": [["positive markers", 7, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "We investigated whether automatic phonetic transcriptions -LRB- APTs -RRB- can replace manually verified phonetic transcriptions -LRB- MPTs -RRB- in a large corpus-based study on pronunciation variation .", "ner": [["automatic phonetic transcriptions -LRB- APTs -RRB-", 3, 8, "OtherScientificTerm"], ["manually verified phonetic transcriptions", 11, 14, "OtherScientificTerm"], ["pronunciation variation", 24, 25, "Task"]], "relations": [[3, 8, 11, 14, "COMPARE"], [3, 8, 24, 25, "USED-FOR"], [11, 14, 24, 25, "USED-FOR"]]},
{"sentence": "To this end , we compared the performance of both transcription types in a classification experiment aimed at establishing the direct influence of a particular situational setting on pronunciation variation .", "ner": [["classification", 14, 14, "Task"], ["pronunciation variation", 28, 29, "Task"]], "relations": []},
{"sentence": "We trained classifiers on the speech processes extracted from the alignments of an APT and an MPT with a canonical transcription .", "ner": [["classifiers", 2, 2, "Method"], ["speech processes", 5, 6, "Material"], ["alignments", 10, 10, "OtherScientificTerm"], ["APT", 13, 13, "OtherScientificTerm"], ["MPT", 16, 16, "OtherScientificTerm"], ["canonical transcription", 19, 20, "OtherScientificTerm"]], "relations": [[5, 6, 2, 2, "USED-FOR"], [10, 10, 5, 6, "USED-FOR"], [10, 10, 13, 13, "USED-FOR"], [10, 10, 16, 16, "USED-FOR"], [13, 13, 16, 16, "CONJUNCTION"], [19, 20, 10, 10, "USED-FOR"]]},
{"sentence": "We tested whether the classifiers were equally good at verifying whether unknown transcriptions represent read speech or telephone dialogues , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .", "ner": [["classifiers", 4, 4, "Material"], ["unknown transcriptions", 11, 12, "OtherScientificTerm"], ["read speech", 14, 15, "Material"], ["telephone dialogues", 17, 18, "Material"], ["speech processes", 24, 25, "Material"]], "relations": [[4, 4, 11, 12, "USED-FOR"], [11, 12, 14, 15, "USED-FOR"], [11, 12, 17, 18, "USED-FOR"], [14, 15, 17, 18, "CONJUNCTION"]]},
{"sentence": "Our results not only show that similar distinguishing speech processes were identified ; our APT-based classifier yielded better classification accuracy than the MPT-based classifier whilst using fewer classification features .", "ner": [["APT-based classifier", 14, 15, "Method"], ["classification accuracy", 18, 19, "Metric"], ["MPT-based classifier", 22, 23, "Method"], ["classification features", 27, 28, "OtherScientificTerm"]], "relations": [[14, 15, 22, 23, "COMPARE"], [18, 19, 14, 15, "EVALUATE-FOR"], [18, 19, 22, 23, "EVALUATE-FOR"], [27, 28, 14, 15, "USED-FOR"], [27, 28, 22, 23, "USED-FOR"]]},
{"sentence": "Machine reading is a relatively new field that features computer programs designed to read flowing text and extract fact assertions expressed by the narrative content .", "ner": [["Machine reading", 0, 1, "Task"], ["field", 6, 6, "Generic"], ["computer programs", 9, 10, "Method"], ["flowing text", 14, 15, "Material"], ["fact assertions", 18, 19, "OtherScientificTerm"], ["narrative content", 23, 24, "Material"]], "relations": [[9, 10, 14, 15, "USED-FOR"], [9, 10, 18, 19, "USED-FOR"], [18, 19, 23, 24, "FEATURE-OF"]]},
{"sentence": "This task involves two core technologies : natural language processing -LRB- NLP -RRB- and information extraction -LRB- IE -RRB- .", "ner": [["task", 1, 1, "Generic"], ["natural language processing -LRB- NLP -RRB-", 7, 12, "Method"], ["information extraction -LRB- IE -RRB-", 14, 18, "Task"]], "relations": [[7, 12, 1, 1, "PART-OF"], [14, 18, 1, 1, "PART-OF"]]},
{"sentence": "In this paper we describe a machine reading system that we have developed within a cognitive architecture .", "ner": [["machine reading system", 6, 8, "Task"], ["cognitive architecture", 15, 16, "Method"]], "relations": [[15, 16, 6, 8, "FEATURE-OF"]]},
{"sentence": "We show how we have integrated into the framework several levels of knowledge for a particular domain , ideas from cognitive semantics and construction grammar , plus tools from prior NLP and IE research .", "ner": [["cognitive semantics", 20, 21, "OtherScientificTerm"], ["construction grammar", 23, 24, "Method"], ["prior NLP", 29, 30, "Task"], ["IE research", 32, 33, "Task"]], "relations": [[20, 21, 23, 24, "CONJUNCTION"], [29, 30, 32, 33, "CONJUNCTION"]]},
{"sentence": "The result is a system that is capable of reading and interpreting complex and fairly idiosyncratic texts in the family history domain .", "ner": [["system", 4, 4, "Generic"], ["idiosyncratic texts", 15, 16, "Material"], ["family history domain", 19, 21, "Material"]], "relations": [[4, 4, 15, 16, "USED-FOR"], [19, 21, 15, 16, "FEATURE-OF"]]},
{"sentence": "We describe the architecture and performance of the system .", "ner": [["system", 8, 8, "Generic"]], "relations": []},
{"sentence": "After presenting the results from several evaluations that we have carried out , we summarize possible future directions .", "ner": [], "relations": []},
{"sentence": "Nonstationary chaotic behavior is not an oxymoron .", "ner": [["Nonstationary chaotic behavior", 0, 2, "OtherScientificTerm"], ["oxymoron", 6, 6, "OtherScientificTerm"]], "relations": []},
{"sentence": "We present two methods for capturing nonstationary chaos , then present a few examples including biological signals , ocean waves and traffic flow .", "ner": [["methods", 3, 3, "Generic"], ["nonstationary chaos", 6, 7, "OtherScientificTerm"], ["examples", 13, 13, "Generic"], ["biological signals", 15, 16, "Material"], ["ocean waves", 18, 19, "Material"], ["traffic flow", 21, 22, "Material"]], "relations": [[3, 3, 6, 7, "USED-FOR"], [15, 16, 13, 13, "HYPONYM-OF"], [15, 16, 18, 19, "CONJUNCTION"], [18, 19, 13, 13, "HYPONYM-OF"], [18, 19, 21, 22, "CONJUNCTION"], [21, 22, 13, 13, "HYPONYM-OF"]]},
{"sentence": "The issue is of practical interest because it is often useful to capture when nonstationary events take place and it is desirable to know over what periods a signal is stationary .", "ner": [["nonstationary events", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper presents a formal analysis for a large class of words called alternative markers , which includes other -LRB- than -RRB- , such -LRB- as -RRB- , and besides .", "ner": [["formal analysis", 4, 5, "Method"], ["alternative markers", 13, 14, "OtherScientificTerm"]], "relations": [[4, 5, 13, 14, "USED-FOR"]]},
{"sentence": "These words appear frequently enough in dialog to warrant serious attention , yet present natural language search engines perform poorly on queries containing them .", "ner": [["words", 1, 1, "Generic"], ["dialog", 6, 6, "OtherScientificTerm"], ["natural language search engines", 14, 17, "Method"], ["them", 23, 23, "Generic"]], "relations": [[1, 1, 6, 6, "PART-OF"]]},
{"sentence": "I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the search engine 's operational semantics .", "ner": [["search engine", 7, 8, "Method"], ["approximation of the formal analysis", 16, 20, "Method"], ["formal analysis", 19, 20, "Method"], ["search engine", 26, 27, "Method"], ["operational semantics", 29, 30, "OtherScientificTerm"]], "relations": [[16, 20, 7, 8, "PART-OF"], [29, 30, 26, 27, "PART-OF"]]},
{"sentence": "The value of this approach is that as the operational semantics of natural language applications improve , even larger improvements are possible .", "ner": [["approach", 4, 4, "Generic"], ["operational semantics", 9, 10, "OtherScientificTerm"], ["natural language applications", 12, 14, "Task"]], "relations": [[9, 10, 12, 14, "PART-OF"]]},
{"sentence": "In this paper , we address the problem of combining several language models -LRB- LMs -RRB- .", "ner": [["language models -LRB- LMs -RRB-", 11, 15, "Method"]], "relations": []},
{"sentence": "We find that simple interpolation methods , like log-linear and linear interpolation , improve the performance but fall short of the performance of an oracle .", "ner": [["interpolation methods", 4, 5, "Method"], ["log-linear and linear interpolation", 8, 11, "Method"]], "relations": [[8, 11, 4, 5, "HYPONYM-OF"]]},
{"sentence": "The oracle knows the reference word string and selects the word string with the best performance -LRB- typically , word or semantic error rate -RRB- from a list of word strings , where each word string has been obtained by using a different LM .", "ner": [["word or semantic error rate", 19, 23, "Metric"], ["LM", 43, 43, "Method"]], "relations": []},
{"sentence": "Actually , the oracle acts like a dynamic combiner with hard decisions using the reference .", "ner": [["dynamic combiner", 7, 8, "Method"], ["hard decisions", 10, 11, "OtherScientificTerm"]], "relations": [[10, 11, 7, 8, "FEATURE-OF"]]},
{"sentence": "We provide experimental results that clearly show the need for a dynamic language model combination to improve the performance further .", "ner": [["dynamic language model combination", 11, 14, "Method"]], "relations": []},
{"sentence": "We suggest a method that mimics the behavior of the oracle using a neural network or a decision tree .", "ner": [["method", 3, 3, "Generic"], ["neural network", 13, 14, "Method"], ["decision tree", 17, 18, "Method"]], "relations": [[13, 14, 3, 3, "USED-FOR"], [17, 18, 3, 3, "USED-FOR"], [17, 18, 13, 14, "CONJUNCTION"]]},
{"sentence": "The method amounts to tagging LMs with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .", "ner": [["method", 1, 1, "Generic"], ["LMs", 5, 5, "Method"], ["confidence measures", 7, 8, "Metric"], ["LM", 17, 17, "Method"]], "relations": [[1, 1, 5, 5, "USED-FOR"], [7, 8, 1, 1, "USED-FOR"]]},
{"sentence": "We describe a new method for the representation of NLP structures within reranking approaches .", "ner": [["method", 4, 4, "Generic"], ["NLP structures", 9, 10, "OtherScientificTerm"], ["reranking approaches", 12, 13, "Method"]], "relations": [[4, 4, 9, 10, "USED-FOR"], [12, 13, 9, 10, "FEATURE-OF"]]},
{"sentence": "We make use of a conditional log-linear model , with hidden variables representing the assignment of lexical items to word clusters or word senses .", "ner": [["conditional log-linear model", 5, 7, "Method"], ["hidden variables", 10, 11, "OtherScientificTerm"], ["word clusters", 19, 20, "OtherScientificTerm"], ["word senses", 22, 23, "OtherScientificTerm"]], "relations": [[10, 11, 5, 7, "USED-FOR"], [19, 20, 22, 23, "CONJUNCTION"]]},
{"sentence": "The model learns to automatically make these assignments based on a discriminative training criterion .", "ner": [["model", 1, 1, "Generic"], ["discriminative training criterion", 11, 13, "Metric"]], "relations": [[11, 13, 1, 1, "USED-FOR"]]},
{"sentence": "Training and decoding with the model requires summing over an exponential number of hidden-variable assignments : the required summations can be computed efficiently and exactly using dynamic programming .", "ner": [["model", 5, 5, "Generic"], ["hidden-variable assignments", 13, 14, "OtherScientificTerm"], ["summations", 18, 18, "Generic"], ["dynamic programming", 26, 27, "Method"]], "relations": [[26, 27, 18, 18, "USED-FOR"]]},
{"sentence": "As a case study , we apply the model to parse reranking .", "ner": [["model", 8, 8, "Generic"], ["parse reranking", 10, 11, "Task"]], "relations": [[8, 8, 10, 11, "USED-FOR"]]},
{"sentence": "The model gives an F-measure improvement of ~ 1.25 % beyond the base parser , and an ~ 0.25 % improvement beyond Collins -LRB- 2000 -RRB- reranker .", "ner": [["model", 1, 1, "Generic"], ["F-measure", 4, 4, "Metric"], ["base parser", 12, 13, "Method"], ["Collins -LRB- 2000 -RRB- reranker", 22, 26, "Method"]], "relations": [[1, 1, 12, 13, "COMPARE"], [4, 4, 1, 1, "EVALUATE-FOR"], [12, 13, 22, 26, "COMPARE"]]},
{"sentence": "Although our experiments are focused on parsing , the techniques described generalize naturally to NLP structures other than parse trees .", "ner": [["parsing", 6, 6, "Task"], ["techniques", 9, 9, "Generic"], ["NLP structures", 14, 15, "OtherScientificTerm"], ["parse trees", 18, 19, "OtherScientificTerm"]], "relations": [[9, 9, 6, 6, "USED-FOR"], [9, 9, 14, 15, "USED-FOR"], [9, 9, 18, 19, "USED-FOR"], [18, 19, 14, 15, "CONJUNCTION"]]},
{"sentence": "This paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data .", "ner": [["algorithm", 4, 4, "Generic"], ["learning the time-varying shape of a non-rigid 3D object", 6, 14, "Task"], ["uncalibrated 2D tracking data", 16, 19, "Material"]], "relations": [[4, 4, 6, 14, "USED-FOR"]]},
{"sentence": "We model shape motion as a rigid component -LRB- rotation and translation -RRB- combined with a non-rigid deformation .", "ner": [["shape motion", 2, 3, "OtherScientificTerm"], ["rigid component", 6, 7, "Method"], ["rotation", 9, 9, "OtherScientificTerm"], ["translation", 11, 11, "OtherScientificTerm"], ["non-rigid deformation", 16, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "Reconstruction is ill-posed if arbitrary deformations are allowed .", "ner": [["Reconstruction", 0, 0, "OtherScientificTerm"], ["arbitrary deformations", 4, 5, "OtherScientificTerm"]], "relations": []},
{"sentence": "We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution .", "ner": [["object shape", 8, 9, "OtherScientificTerm"], ["Gaussian distribution", 18, 19, "Method"]], "relations": [[18, 19, 8, 9, "USED-FOR"]]},
{"sentence": "Based on this assumption , the algorithm simultaneously estimates 3D shape and motion for each time frame , learns the parameters of the Gaussian , and robustly fills-in missing data points .", "ner": [["algorithm", 6, 6, "Generic"], ["3D shape and motion", 9, 12, "OtherScientificTerm"], ["Gaussian", 23, 23, "Method"]], "relations": [[6, 6, 9, 12, "USED-FOR"]]},
{"sentence": "We then extend the algorithm to model temporal smoothness in object shape , thus allowing it to handle severe cases of missing data .", "ner": [["algorithm", 4, 4, "Generic"], ["temporal smoothness in object shape", 7, 11, "OtherScientificTerm"], ["it", 15, 15, "Generic"], ["missing data", 21, 22, "Material"]], "relations": [[4, 4, 7, 11, "USED-FOR"], [15, 15, 21, 22, "USED-FOR"]]},
{"sentence": "Automatic summarization and information extraction are two important Internet services .", "ner": [["Automatic summarization", 0, 1, "Task"], ["information extraction", 3, 4, "Task"]], "relations": [[0, 1, 3, 4, "CONJUNCTION"]]},
{"sentence": "MUC and SUMMAC play their appropriate roles in the next generation Internet .", "ner": [["MUC", 0, 0, "Material"], ["SUMMAC", 2, 2, "Material"]], "relations": [[0, 0, 2, 2, "CONJUNCTION"]]},
{"sentence": "This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1 .", "ner": [["automatic summarization", 5, 6, "Task"], ["models", 11, 11, "Generic"], ["summary generation", 16, 17, "Task"], ["tasks", 20, 20, "Generic"], ["SUMMAC-1", 23, 23, "Material"]], "relations": [[11, 11, 16, 17, "USED-FOR"], [11, 11, 20, 20, "USED-FOR"], [20, 20, 23, 23, "PART-OF"]]},
{"sentence": "For categorization task , positive feature vectors and negative feature vectors are used cooperatively to construct generic , indicative summaries .", "ner": [["categorization task", 1, 2, "Task"], ["positive feature vectors", 4, 6, "OtherScientificTerm"], ["negative feature vectors", 8, 10, "OtherScientificTerm"], ["generic , indicative summaries", 16, 19, "OtherScientificTerm"]], "relations": [[4, 6, 1, 2, "USED-FOR"], [4, 6, 8, 10, "CONJUNCTION"], [4, 6, 16, 19, "USED-FOR"], [8, 10, 1, 2, "USED-FOR"], [8, 10, 16, 19, "USED-FOR"]]},
{"sentence": "For adhoc task , a text model based on relationship between nouns and verbs is used to filter out irrelevant discourse segment , to rank relevant sentences , and to generate the user-directed summaries .", "ner": [["adhoc task", 1, 2, "Task"], ["text model", 5, 6, "Method"], ["discourse segment", 20, 21, "OtherScientificTerm"], ["user-directed summaries", 32, 33, "OtherScientificTerm"]], "relations": [[5, 6, 1, 2, "USED-FOR"], [5, 6, 20, 21, "USED-FOR"], [5, 6, 32, 33, "USED-FOR"]]},
{"sentence": "The result shows that the NormF of the best summary and that of the fixed summary for adhoc tasks are 0.456 and 0 .", "ner": [["NormF", 5, 5, "Metric"], ["adhoc tasks", 17, 18, "Task"]], "relations": [[5, 5, 17, 18, "EVALUATE-FOR"]]},
{"sentence": "447 .", "ner": [], "relations": []},
{"sentence": "The NormF of the best summary and that of the fixed summary for categorization task are 0.4090 and 0.4023 .", "ner": [["NormF", 1, 1, "Metric"], ["categorization task", 13, 14, "Task"]], "relations": [[1, 1, 13, 14, "EVALUATE-FOR"]]},
{"sentence": "Our system outperforms the average system in categorization task but does a common job in adhoc task .", "ner": [["system", 1, 1, "Generic"], ["system", 5, 5, "Generic"], ["categorization task", 7, 8, "Task"], ["adhoc task", 15, 16, "Task"]], "relations": [[1, 1, 5, 5, "COMPARE"], [7, 8, 1, 1, "EVALUATE-FOR"], [7, 8, 5, 5, "EVALUATE-FOR"], [15, 16, 1, 1, "EVALUATE-FOR"], [15, 16, 7, 8, "EVALUATE-FOR"]]},
{"sentence": "In real-world action recognition problems , low-level features can not adequately characterize the rich spatial-temporal structures in action videos .", "ner": [["real-world action recognition problems", 1, 4, "Task"], ["low-level features", 6, 7, "OtherScientificTerm"], ["rich spatial-temporal structures", 13, 15, "OtherScientificTerm"], ["action videos", 17, 18, "Material"]], "relations": [[13, 15, 17, 18, "FEATURE-OF"]]},
{"sentence": "In this work , we encode actions based on attributes that describes actions as high-level concepts e.g. , jump forward or motion in the air .", "ner": [["high-level concepts", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "We base our analysis on two types of action attributes .", "ner": [["action attributes", 8, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "One type of action attributes is generated by humans .", "ner": [["action attributes", 3, 4, "OtherScientificTerm"]], "relations": []},
{"sentence": "The second type is data-driven attributes , which are learned from data using dictionary learning methods .", "ner": [["data-driven attributes", 4, 5, "OtherScientificTerm"], ["dictionary learning methods", 13, 15, "Method"]], "relations": [[13, 15, 4, 5, "USED-FOR"]]},
{"sentence": "Attribute-based representation may exhibit high variance due to noisy and redundant attributes .", "ner": [["Attribute-based representation", 0, 1, "Method"], ["noisy and redundant attributes", 8, 11, "OtherScientificTerm"]], "relations": []},
{"sentence": "We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set .", "ner": [["discriminative and compact attribute-based representation", 3, 7, "Method"], ["discriminative attributes", 13, 14, "OtherScientificTerm"]], "relations": [[13, 14, 3, 7, "USED-FOR"]]},
{"sentence": "Three attribute selection criteria are proposed and formulated as a submodular optimization problem .", "ner": [["attribute selection criteria", 1, 3, "Metric"], ["submodular optimization problem", 10, 12, "Task"]], "relations": [[10, 12, 1, 3, "USED-FOR"]]},
{"sentence": "A greedy optimization algorithm is presented and guaranteed to be at least -LRB- 1-1 / e -RRB- - approximation to the optimum .", "ner": [["greedy optimization algorithm", 1, 3, "Method"]], "relations": []},
{"sentence": "Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches .", "ner": [["Olympic Sports and UCF101 datasets", 4, 8, "Material"], ["attribute-based representation", 13, 14, "Method"], ["action recognition algorithms", 21, 23, "Method"], ["recognition approaches", 29, 30, "Method"]], "relations": [[4, 8, 13, 14, "EVALUATE-FOR"], [13, 14, 21, 23, "USED-FOR"], [21, 23, 29, 30, "COMPARE"]]},
{"sentence": "This paper reports a completed stage of ongoing research at the University of York .", "ner": [], "relations": []},
{"sentence": "Landsbergen 's advocacy of analytical inverses for compositional syntax rules encourages the application of Definite Clause Grammar techniques to the construction of a parser returning Montague analysis trees .", "ner": [["analytical inverses", 4, 5, "Method"], ["compositional syntax rules", 7, 9, "OtherScientificTerm"], ["Definite Clause Grammar techniques", 14, 17, "Method"], ["parser returning Montague analysis trees", 23, 27, "Task"]], "relations": [[4, 5, 7, 9, "USED-FOR"], [4, 5, 14, 17, "USED-FOR"], [14, 17, 23, 27, "USED-FOR"]]},
{"sentence": "A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing * and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae .", "ner": [["parser MDCC", 1, 2, "Method"], ["augmented Friedman - Warren algorithm", 8, 12, "Method"], ["post referencing", 14, 15, "OtherScientificTerm"], ["intenslonal logic translator LILT", 23, 26, "OtherScientificTerm"], ["derivational history", 32, 33, "OtherScientificTerm"], ["reduced IL formulae", 36, 38, "OtherScientificTerm"]], "relations": [[8, 12, 1, 2, "USED-FOR"], [14, 15, 8, 12, "FEATURE-OF"], [32, 33, 23, 26, "USED-FOR"], [36, 38, 32, 33, "FEATURE-OF"]]},
{"sentence": "Some familiarity with Montague 's PTQ and the basic DCG mechanism is assumed .", "ner": [["Montague 's PTQ", 3, 5, "Method"], ["basic DCG mechanism", 8, 10, "Method"]], "relations": [[3, 5, 8, 10, "CONJUNCTION"]]},
{"sentence": "Despite their success , convolutional neural networks are computationally expensive because they must examine all image locations .", "ner": [["convolutional neural networks", 4, 6, "Method"], ["they", 11, 11, "Generic"]], "relations": []},
{"sentence": "Stochastic attention-based models have been shown to improve computational efficiency at test time , but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates .", "ner": [["Stochastic attention-based models", 0, 2, "Method"], ["computational efficiency", 8, 9, "Metric"], ["they", 15, 15, "Generic"], ["intractable posterior inference", 22, 24, "Task"], ["stochastic gradient estimates", 30, 32, "Task"]], "relations": [[8, 9, 0, 2, "EVALUATE-FOR"], [22, 24, 30, 32, "CONJUNCTION"]]},
{"sentence": "Borrowing techniques from the literature on training deep generative models , we present the Wake-Sleep Recurrent Attention Model , a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients .", "ner": [["Borrowing techniques", 0, 1, "Method"], ["deep generative models", 7, 9, "Method"], ["Wake-Sleep Recurrent Attention Model", 14, 17, "Method"], ["method", 20, 20, "Generic"], ["stochastic attention networks", 23, 25, "Method"], ["posterior inference", 28, 29, "Task"], ["stochastic gradients", 37, 38, "OtherScientificTerm"]], "relations": [[0, 1, 7, 9, "USED-FOR"], [20, 20, 23, 25, "USED-FOR"], [23, 25, 28, 29, "USED-FOR"]]},
{"sentence": "We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation .", "ner": [["method", 4, 4, "Generic"], ["training time", 10, 11, "Metric"], ["stochastic attention networks", 13, 15, "Method"], ["image classification", 20, 21, "Task"], ["caption generation", 23, 24, "Task"]], "relations": [[10, 11, 4, 4, "EVALUATE-FOR"], [10, 11, 13, 15, "FEATURE-OF"], [20, 21, 4, 4, "EVALUATE-FOR"], [20, 21, 23, 24, "CONJUNCTION"], [23, 24, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "A new exemplar-based framework unifying image completion , texture synthesis and image inpainting is presented in this work .", "ner": [["exemplar-based framework", 2, 3, "Method"], ["image completion", 5, 6, "Task"], ["texture synthesis", 8, 9, "Task"], ["image inpainting", 11, 12, "Task"]], "relations": [[2, 3, 5, 6, "USED-FOR"], [2, 3, 8, 9, "USED-FOR"], [2, 3, 11, 12, "USED-FOR"], [5, 6, 8, 9, "CONJUNCTION"], [8, 9, 11, 12, "CONJUNCTION"]]},
{"sentence": "Contrary to existing greedy techniques , these tasks are posed in the form of a discrete global optimization problem with a well defined objective function .", "ner": [["greedy techniques", 3, 4, "Method"], ["tasks", 7, 7, "Generic"], ["discrete global optimization problem", 15, 18, "Task"], ["well defined objective function", 21, 24, "OtherScientificTerm"]], "relations": [[3, 4, 7, 7, "COMPARE"], [15, 18, 7, 7, "FEATURE-OF"], [21, 24, 15, 18, "FEATURE-OF"]]},
{"sentence": "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important extensions over standard belief propagation -LRB- BP -RRB- : '' priority-based message scheduling '' and '' dynamic label pruning '' .", "ner": [["problem", 3, 3, "Generic"], ["optimization scheme", 6, 7, "Method"], ["Priority-BP", 10, 10, "Method"], ["extensions", 19, 19, "Generic"], ["belief propagation -LRB- BP -RRB-", 22, 26, "Method"], ["priority-based message scheduling", 29, 31, "Method"], ["dynamic label pruning", 35, 37, "Method"]], "relations": [[6, 7, 3, 3, "USED-FOR"], [10, 10, 6, 7, "HYPONYM-OF"], [19, 19, 6, 7, "PART-OF"], [22, 26, 19, 19, "USED-FOR"], [29, 31, 19, 19, "HYPONYM-OF"], [29, 31, 35, 37, "CONJUNCTION"], [35, 37, 19, 19, "HYPONYM-OF"]]},
{"sentence": "These two extensions work in cooperation to deal with the intolerable computational cost of BP caused by the huge number of existing labels .", "ner": [["extensions", 2, 2, "Generic"], ["intolerable computational cost of BP", 10, 14, "OtherScientificTerm"], ["BP", 14, 14, "Method"]], "relations": [[2, 2, 10, 14, "USED-FOR"]]},
{"sentence": "Moreover , both extensions are generic and can therefore be applied to any MRF energy function as well .", "ner": [["extensions", 3, 3, "Generic"], ["MRF energy function", 13, 15, "Method"]], "relations": [[3, 3, 13, 15, "USED-FOR"]]},
{"sentence": "The effectiveness of our method is demonstrated on a wide variety of image completion examples .", "ner": [["method", 4, 4, "Generic"], ["image completion examples", 12, 14, "Material"]], "relations": [[12, 14, 4, 4, "USED-FOR"]]},
{"sentence": "In this paper , we compare the relative effects of segment order , segmentation and segment contiguity on the retrieval performance of a translation memory system .", "ner": [["segment order", 10, 11, "OtherScientificTerm"], ["segmentation", 13, 13, "OtherScientificTerm"], ["segment contiguity", 15, 16, "OtherScientificTerm"], ["retrieval", 19, 19, "Task"], ["translation memory system", 23, 25, "Method"]], "relations": [[10, 11, 13, 13, "CONJUNCTION"], [10, 11, 23, 25, "USED-FOR"], [13, 13, 15, 16, "CONJUNCTION"], [13, 13, 23, 25, "USED-FOR"], [15, 16, 23, 25, "USED-FOR"], [19, 19, 23, 25, "EVALUATE-FOR"]]},
{"sentence": "We take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character - and word-segmented data , in combination with a range of local segment contiguity models -LRB- in the form of N-grams -RRB- .", "ner": [["bag-of-words and segment order-sensitive string comparison methods", 6, 12, "Method"], ["character - and word-segmented data", 19, 23, "Material"], ["local segment contiguity models", 31, 34, "Method"], ["N-grams", 40, 40, "Method"]], "relations": [[19, 23, 6, 12, "USED-FOR"], [31, 34, 6, 12, "CONJUNCTION"], [40, 40, 31, 34, "FEATURE-OF"]]},
{"sentence": "Over two distinct datasets , we find that indexing according to simple character bigrams produces a retrieval accuracy superior to any of the tested word N-gram models .", "ner": [["indexing", 8, 8, "Task"], ["character bigrams", 12, 13, "Method"], ["retrieval accuracy", 16, 17, "Metric"], ["word N-gram models", 24, 26, "Method"]], "relations": [[12, 13, 8, 8, "USED-FOR"], [12, 13, 24, 26, "COMPARE"], [16, 17, 12, 13, "EVALUATE-FOR"], [16, 17, 24, 26, "EVALUATE-FOR"]]},
{"sentence": "Further , in their optimum configuration , bag-of-words methods are shown to be equivalent to segment order-sensitive methods in terms of retrieval accuracy , but much faster .", "ner": [["bag-of-words methods", 7, 8, "Method"], ["segment order-sensitive methods", 15, 17, "Method"], ["retrieval accuracy", 21, 22, "Metric"]], "relations": [[7, 8, 15, 17, "COMPARE"], [21, 22, 7, 8, "EVALUATE-FOR"], [21, 22, 15, 17, "EVALUATE-FOR"]]},
{"sentence": "We also provide evidence that our findings are scalable .", "ner": [], "relations": []},
{"sentence": "In this paper we show how two standard outputs from information extraction -LRB- IE -RRB- systems - named entity annotations and scenario templates - can be used to enhance access to text collections via a standard text browser .", "ner": [["outputs", 8, 8, "Generic"], ["information extraction -LRB- IE -RRB- systems", 10, 15, "Method"], ["named entity annotations", 17, 19, "OtherScientificTerm"], ["scenario templates", 21, 22, "OtherScientificTerm"], ["text collections", 31, 32, "Material"], ["text browser", 36, 37, "Method"]], "relations": [[8, 8, 31, 32, "USED-FOR"], [17, 19, 8, 8, "HYPONYM-OF"], [17, 19, 21, 22, "CONJUNCTION"], [21, 22, 8, 8, "HYPONYM-OF"], [36, 37, 31, 32, "USED-FOR"]]},
{"sentence": "We describe how this information is used in a prototype system designed to support information workers ' access to a pharmaceutical news archive as part of their industry watch function .", "ner": [["prototype system", 9, 10, "Generic"], ["pharmaceutical news archive", 20, 22, "Material"]], "relations": [[9, 10, 20, 22, "USED-FOR"]]},
{"sentence": "We also report results of a preliminary , qualitative user evaluation of the system , which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers .", "ner": [["qualitative user evaluation", 8, 10, "Metric"], ["system", 13, 13, "Generic"], ["IE-enhanced text browsers", 38, 40, "Method"]], "relations": [[8, 10, 13, 13, "EVALUATE-FOR"]]},
{"sentence": "We present a new model-based bundle adjustment algorithm to recover the 3D model of a scene/object from a sequence of images with unknown motions .", "ner": [["model-based bundle adjustment algorithm", 4, 7, "Method"], ["3D model", 11, 12, "Method"], ["images", 20, 20, "Material"], ["unknown motions", 22, 23, "OtherScientificTerm"]], "relations": [[4, 7, 11, 12, "USED-FOR"], [20, 20, 11, 12, "USED-FOR"], [22, 23, 20, 20, "PART-OF"]]},
{"sentence": "Instead of representing scene/object by a collection of isolated 3D features -LRB- usually points -RRB- , our algorithm uses a surface controlled by a small set of parameters .", "ner": [["isolated 3D features", 8, 10, "OtherScientificTerm"], ["algorithm", 17, 17, "Generic"], ["surface", 20, 20, "OtherScientificTerm"]], "relations": [[20, 20, 17, 17, "USED-FOR"]]},
{"sentence": "Compared with previous model-based approaches , our approach has the following advantages .", "ner": [["model-based approaches", 3, 4, "Generic"], ["approach", 7, 7, "Generic"]], "relations": [[3, 4, 7, 7, "COMPARE"]]},
{"sentence": "First , instead of using the model space as a regular-izer , we directly use it as our search space , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .", "ner": [["model space", 6, 7, "OtherScientificTerm"], ["regular-izer", 10, 10, "OtherScientificTerm"], ["it", 15, 15, "Generic"], ["search space", 18, 19, "OtherScientificTerm"]], "relations": [[6, 7, 10, 10, "USED-FOR"], [10, 10, 18, 19, "COMPARE"], [15, 15, 18, 19, "USED-FOR"]]},
{"sentence": "Second , our algorithm automatically associates tracked points with their correct locations on the surfaces , thereby eliminating the need for a prior 2D-to-3D association .", "ner": [["algorithm", 3, 3, "Generic"], ["surfaces", 14, 14, "OtherScientificTerm"], ["prior 2D-to-3D association", 22, 24, "OtherScientificTerm"]], "relations": []},
{"sentence": "Third , regarding face modeling , we use a very small set of face metrics -LRB- meaningful deformations -RRB- to parame-terize the face geometry , resulting in a smaller search space and a better posed system .", "ner": [["face modeling", 3, 4, "Task"], ["face metrics", 13, 14, "Metric"], ["face geometry", 22, 23, "OtherScientificTerm"], ["search space", 29, 30, "OtherScientificTerm"], ["posed system", 34, 35, "Method"]], "relations": [[13, 14, 3, 4, "USED-FOR"], [13, 14, 22, 23, "USED-FOR"], [13, 14, 29, 30, "USED-FOR"], [13, 14, 34, 35, "USED-FOR"]]},
{"sentence": "Experiments with both synthetic and real data show that this new algorithm is faster , more accurate and more stable than existing ones .", "ner": [["synthetic and real data", 3, 6, "Material"], ["algorithm", 11, 11, "Generic"], ["ones", 22, 22, "Generic"]], "relations": [[3, 6, 11, 11, "EVALUATE-FOR"], [3, 6, 22, 22, "EVALUATE-FOR"], [11, 11, 22, 22, "COMPARE"]]},
{"sentence": "This paper presents an approach to the unsupervised learning of parts of speech which uses both morphological and syntactic information .", "ner": [["approach", 4, 4, "Generic"], ["unsupervised learning of parts of speech", 7, 12, "Task"], ["morphological and syntactic information", 16, 19, "OtherScientificTerm"]], "relations": [[4, 4, 7, 12, "USED-FOR"], [16, 19, 4, 4, "USED-FOR"]]},
{"sentence": "While the model is more complex than those which have been employed for unsupervised learning of POS tags in English , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .", "ner": [["model", 2, 2, "Generic"], ["those", 7, 7, "Generic"], ["unsupervised learning of POS tags in English", 13, 19, "Task"], ["syntactic information", 24, 25, "OtherScientificTerm"], ["morphology", 38, 38, "OtherScientificTerm"]], "relations": [[2, 2, 7, 7, "COMPARE"], [7, 7, 13, 19, "USED-FOR"], [24, 25, 7, 7, "USED-FOR"]]},
{"sentence": "In many languages , morphology provides better clues to a word 's category than word order .", "ner": [["morphology", 4, 4, "OtherScientificTerm"], ["word order", 14, 15, "OtherScientificTerm"]], "relations": [[4, 4, 14, 15, "COMPARE"]]},
{"sentence": "We present the computational model for POS learning , and present results for applying it to Bulgarian , a Slavic language with relatively free word order and rich morphology .", "ner": [["computational model", 3, 4, "Generic"], ["POS learning", 6, 7, "Task"], ["it", 14, 14, "Generic"], ["Bulgarian", 16, 16, "Material"], ["Slavic language", 19, 20, "Material"], ["free word order", 23, 25, "OtherScientificTerm"], ["rich morphology", 27, 28, "OtherScientificTerm"]], "relations": [[3, 4, 6, 7, "USED-FOR"], [16, 16, 14, 14, "USED-FOR"], [16, 16, 19, 20, "HYPONYM-OF"], [23, 25, 16, 16, "FEATURE-OF"], [23, 25, 27, 28, "CONJUNCTION"], [27, 28, 16, 16, "FEATURE-OF"]]},
{"sentence": "Words in Chinese text are not naturally separated by delimiters , which poses a challenge to standard machine translation -LRB- MT -RRB- systems .", "ner": [["Chinese text", 2, 3, "Material"], ["machine translation -LRB- MT -RRB- systems", 17, 22, "Task"]], "relations": []},
{"sentence": "In MT , the widely used approach is to apply a Chinese word segmenter trained from manually annotated data , using a fixed lexicon .", "ner": [["MT", 1, 1, "Task"], ["Chinese word segmenter", 11, 13, "Method"], ["manually annotated data", 16, 18, "Material"]], "relations": [[11, 13, 1, 1, "USED-FOR"], [16, 18, 11, 13, "USED-FOR"]]},
{"sentence": "Such word segmentation is not necessarily optimal for translation .", "ner": [["word segmentation", 1, 2, "Method"], ["translation", 8, 8, "Task"]], "relations": [[1, 2, 8, 8, "USED-FOR"]]},
{"sentence": "We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT .", "ner": [["Bayesian semi-supervised Chinese word segmentation model", 3, 8, "Method"], ["monolingual and bilingual information", 12, 15, "OtherScientificTerm"], ["segmentation", 19, 19, "Task"], ["MT", 22, 22, "Task"]], "relations": [[3, 8, 19, 19, "USED-FOR"], [12, 15, 3, 8, "USED-FOR"], [19, 19, 22, 22, "USED-FOR"]]},
{"sentence": "Experiments show that our method improves a state-of-the-art MT system in a small and a large data environment .", "ner": [["method", 4, 4, "Generic"], ["MT system", 8, 9, "Method"]], "relations": [[4, 4, 8, 9, "COMPARE"]]},
{"sentence": "In this paper we compare two competing approaches to part-of-speech tagging , statistical and constraint-based disambiguation , using French as our test language .", "ner": [["approaches", 7, 7, "Generic"], ["part-of-speech tagging", 9, 10, "Task"], ["statistical and constraint-based disambiguation", 12, 15, "Task"], ["French", 18, 18, "Material"]], "relations": [[7, 7, 9, 10, "USED-FOR"], [18, 18, 7, 7, "USED-FOR"]]},
{"sentence": "We imposed a time limit on our experiment : the amount of time spent on the design of our constraint system was about the same as the time we used to train and test the easy-to-implement statistical model .", "ner": [["constraint system", 19, 20, "Generic"], ["statistical model", 36, 37, "Method"]], "relations": [[19, 20, 36, 37, "COMPARE"]]},
{"sentence": "We describe the two systems and compare the results .", "ner": [["systems", 4, 4, "Generic"]], "relations": []},
{"sentence": "The accuracy of the statistical method is reasonably good , comparable to taggers for English .", "ner": [["accuracy", 1, 1, "Metric"], ["statistical method", 4, 5, "Method"], ["taggers", 12, 12, "Method"], ["English", 14, 14, "Material"]], "relations": [[1, 1, 4, 5, "EVALUATE-FOR"], [1, 1, 12, 12, "EVALUATE-FOR"], [4, 5, 12, 12, "COMPARE"], [12, 12, 14, 14, "USED-FOR"]]},
{"sentence": "But the constraint-based tagger seems to be superior even with the limited time we allowed ourselves for rule development .", "ner": [["constraint-based tagger", 2, 3, "Method"]], "relations": []},
{"sentence": "Structured-light methods actively generate geometric correspondence data between projectors and cameras in order to facilitate robust 3D reconstruction .", "ner": [["Structured-light methods", 0, 1, "Method"], ["geometric correspondence data", 4, 6, "Material"], ["robust 3D reconstruction", 15, 17, "Task"]], "relations": [[0, 1, 4, 6, "USED-FOR"], [4, 6, 15, 17, "USED-FOR"]]},
{"sentence": "In this paper , we present Photogeometric Structured Light whereby a standard structured light method is extended to include photometric methods .", "ner": [["Photogeometric Structured Light", 6, 8, "Method"], ["structured light method", 12, 14, "Method"], ["photometric methods", 19, 20, "Method"]], "relations": [[12, 14, 6, 8, "PART-OF"], [19, 20, 6, 8, "PART-OF"]]},
{"sentence": "Photometric processing serves the double purpose of increasing the amount of recovered surface detail and of enabling the structured-light setup to be robustly self-calibrated .", "ner": [["Photometric processing", 0, 1, "Method"], ["recovered surface detail", 11, 13, "OtherScientificTerm"], ["structured-light setup", 18, 19, "OtherScientificTerm"]], "relations": [[0, 1, 11, 13, "USED-FOR"], [0, 1, 18, 19, "USED-FOR"]]},
{"sentence": "Further , our framework uses a photogeometric optimization that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate multi-view 3D model which best complies with photometric and geometric data .", "ner": [["framework", 3, 3, "Generic"], ["photogeometric optimization", 6, 7, "Method"], ["multi-view 3D model", 24, 26, "Method"], ["photometric and geometric data", 31, 34, "Material"]], "relations": [[6, 7, 3, 3, "USED-FOR"], [31, 34, 24, 26, "USED-FOR"]]},
{"sentence": "In this paper , a discrimination and robustness oriented adaptive learning procedure is proposed to deal with the task of syntactic ambiguity resolution .", "ner": [["adaptive learning procedure", 9, 11, "Method"], ["syntactic ambiguity resolution", 20, 22, "Task"]], "relations": [[9, 11, 20, 22, "USED-FOR"]]},
{"sentence": "Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .", "ner": [["insufficient training data", 5, 7, "OtherScientificTerm"], ["approximation error", 9, 10, "OtherScientificTerm"], ["language model", 14, 15, "Method"], ["statistical approaches", 18, 19, "Method"], ["ambiguities", 23, 23, "OtherScientificTerm"], ["maximum likelihood method", 29, 31, "Method"]], "relations": [[5, 7, 9, 10, "CONJUNCTION"], [18, 19, 23, 23, "USED-FOR"], [29, 31, 18, 19, "USED-FOR"]]},
{"sentence": "The proposed method remedies these problems by adjusting the parameters to maximize the accuracy rate directly .", "ner": [["method", 2, 2, "Generic"], ["problems", 5, 5, "Generic"]], "relations": []},
{"sentence": "To make the proposed algorithm robust , the possible variations between the training corpus and the real tasks are also taken into consideration by enlarging the separation margin between the correct candidate and its competing members .", "ner": [["algorithm", 4, 4, "Generic"], ["separation margin", 26, 27, "OtherScientificTerm"]], "relations": []},
{"sentence": "Significant improvement has been observed in the test .", "ner": [], "relations": []},
{"sentence": "The accuracy rate of syntactic disambiguation is raised from 46.0 % to 60.62 % by using this novel approach .", "ner": [["accuracy rate", 1, 2, "Metric"], ["syntactic disambiguation", 4, 5, "Task"], ["approach", 18, 18, "Generic"]], "relations": [[1, 2, 4, 5, "EVALUATE-FOR"], [4, 5, 18, 18, "EVALUATE-FOR"]]},
{"sentence": "This paper presents a new approach to statistical sentence generation in which alternative phrases are represented as packed sets of trees , or forests , and then ranked statistically to choose the best one .", "ner": [["approach", 5, 5, "Generic"], ["statistical sentence generation", 7, 9, "Task"], ["trees", 20, 20, "OtherScientificTerm"], ["forests", 23, 23, "OtherScientificTerm"]], "relations": [[5, 5, 7, 9, "USED-FOR"]]},
{"sentence": "This representation offers advantages in compactness and in the ability to represent syntactic information .", "ner": [["representation", 1, 1, "Generic"], ["syntactic information", 12, 13, "OtherScientificTerm"]], "relations": []},
{"sentence": "It also facilitates more efficient statistical ranking than a previous approach to statistical generation .", "ner": [["It", 0, 0, "Generic"], ["statistical ranking", 5, 6, "Method"], ["approach", 10, 10, "Generic"], ["statistical generation", 12, 13, "Task"]], "relations": [[0, 0, 5, 6, "USED-FOR"], [0, 0, 10, 10, "COMPARE"], [10, 10, 12, 13, "USED-FOR"]]},
{"sentence": "An efficient ranking algorithm is described , together with experimental results showing significant improvements over simple enumeration or a lattice-based approach .", "ner": [["ranking algorithm", 2, 3, "Method"], ["enumeration", 16, 16, "Method"], ["lattice-based approach", 19, 20, "Method"]], "relations": [[2, 3, 16, 16, "COMPARE"], [2, 3, 19, 20, "COMPARE"], [16, 16, 19, 20, "CONJUNCTION"]]},
{"sentence": "This article deals with the interpretation of conceptual operations underlying the communicative use of natural language -LRB- NL -RRB- within the Structured Inheritance Network -LRB- SI-Nets -RRB- paradigm .", "ner": [["conceptual operations", 7, 8, "Task"], ["natural language -LRB- NL -RRB-", 14, 18, "Material"], ["Structured Inheritance Network -LRB- SI-Nets -RRB- paradigm", 21, 27, "Method"]], "relations": [[14, 18, 21, 27, "USED-FOR"]]},
{"sentence": "The operations are reduced to functions of a formal language , thus changing the level of abstraction of the operations to be performed on SI-Nets .", "ner": [["operations", 1, 1, "Generic"], ["formal language", 8, 9, "OtherScientificTerm"], ["operations", 19, 19, "Generic"], ["SI-Nets", 24, 24, "Method"]], "relations": [[19, 19, 24, 24, "USED-FOR"]]},
{"sentence": "In this sense , operations on SI-Nets are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the conceptual system of NL .", "ner": [["operations", 4, 4, "Generic"], ["SI-Nets", 6, 6, "Method"], ["epistemological objects", 13, 14, "OtherScientificTerm"], ["conceptual system", 34, 35, "Method"], ["NL", 37, 37, "Material"]], "relations": [[4, 4, 6, 6, "USED-FOR"], [37, 37, 34, 35, "USED-FOR"]]},
{"sentence": "For this purpose , we have designed a version of KL-ONE which represents the epistemological level , while the new experimental language , KL-Conc , represents the conceptual level .", "ner": [["KL-ONE", 10, 10, "Method"], ["epistemological level", 14, 15, "OtherScientificTerm"], ["experimental language", 20, 21, "Generic"], ["KL-Conc", 23, 23, "Method"], ["conceptual level", 27, 28, "OtherScientificTerm"]], "relations": [[10, 10, 23, 23, "COMPARE"], [14, 15, 10, 10, "FEATURE-OF"], [27, 28, 23, 23, "FEATURE-OF"]]},
{"sentence": "KL-Conc would seem to be a more natural and intuitive way of interacting with SI-Nets .", "ner": [["KL-Conc", 0, 0, "Method"], ["SI-Nets", 14, 14, "Method"]], "relations": []},
{"sentence": "We present an algorithm for calibrated camera relative pose estimation from lines .", "ner": [["algorithm", 3, 3, "Generic"], ["calibrated camera relative pose estimation", 5, 9, "Task"]], "relations": [[3, 3, 5, 9, "USED-FOR"]]},
{"sentence": "Given three lines with two of the lines parallel and orthogonal to the third we can compute the relative rotation between two images .", "ner": [["relative rotation", 18, 19, "OtherScientificTerm"]], "relations": []},
{"sentence": "We can also compute the relative translation from two intersection points .", "ner": [["relative translation", 5, 6, "OtherScientificTerm"]], "relations": []},
{"sentence": "We also present a framework in which such lines can be detected .", "ner": [["framework", 4, 4, "Generic"]], "relations": []},
{"sentence": "We evaluate the performance of the algorithm using synthetic and real data .", "ner": [["algorithm", 6, 6, "Generic"], ["synthetic and real data", 8, 11, "Material"]], "relations": [[8, 11, 6, 6, "USED-FOR"]]},
{"sentence": "The intended use of the algorithm is with robust hypothesize-and-test frameworks such as RANSAC .", "ner": [["algorithm", 5, 5, "Generic"], ["hypothesize-and-test frameworks", 9, 10, "Method"], ["RANSAC", 13, 13, "Method"]], "relations": [[5, 5, 9, 10, "CONJUNCTION"], [13, 13, 9, 10, "HYPONYM-OF"]]},
{"sentence": "Our approach is suitable for urban and indoor environments where most lines are either parallel or orthogonal to each other .", "ner": [["approach", 1, 1, "Generic"], ["urban and indoor environments", 5, 8, "OtherScientificTerm"]], "relations": [[1, 1, 5, 8, "USED-FOR"]]},
{"sentence": "In this paper , we present a fully automated extraction system , named IntEx , to identify gene and protein interactions in biomedical text .", "ner": [["fully automated extraction system", 7, 10, "Method"], ["IntEx", 13, 13, "Method"], ["gene and protein interactions", 17, 20, "Task"], ["biomedical text", 22, 23, "Material"]], "relations": [[7, 10, 17, 20, "USED-FOR"], [13, 13, 7, 10, "HYPONYM-OF"], [22, 23, 17, 20, "USED-FOR"]]},
{"sentence": "Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles .", "ner": [["approach", 1, 1, "Generic"], ["syntactic roles", 16, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "Then , tagging biological entities with the help of biomedical and linguistic ontologies .", "ner": [["biological entities", 3, 4, "OtherScientificTerm"], ["biomedical and linguistic ontologies", 9, 12, "OtherScientificTerm"]], "relations": [[9, 12, 3, 4, "USED-FOR"]]},
{"sentence": "Finally , extracting complete interactions by analyzing the matching contents of syntactic roles and their linguistically significant combinations .", "ner": [["syntactic roles", 11, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "Our extraction system handles complex sentences and extracts multiple and nested interactions specified in a sentence .", "ner": [["extraction system", 1, 2, "Method"], ["multiple and nested interactions", 8, 11, "OtherScientificTerm"]], "relations": [[1, 2, 8, 11, "USED-FOR"]]},
{"sentence": "Experimental evaluations with two other state of the art extraction systems indicate that the IntEx system achieves better performance without the labor intensive pattern engineering requirement .", "ner": [["extraction systems", 9, 10, "Method"], ["IntEx system", 14, 15, "Method"], ["pattern engineering requirement", 23, 25, "OtherScientificTerm"]], "relations": [[14, 15, 9, 10, "COMPARE"]]},
{"sentence": "This paper introduces a method for computational analysis of move structures in abstracts of research articles .", "ner": [["method", 4, 4, "Generic"], ["computational analysis of move structures", 6, 10, "Task"], ["abstracts of research articles", 12, 15, "Material"]], "relations": [[4, 4, 6, 10, "USED-FOR"], [12, 15, 6, 10, "USED-FOR"]]},
{"sentence": "In our approach , sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions .", "ner": [["approach", 2, 2, "Generic"], ["rhetorical functions", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "The method involves automatically gathering a large number of abstracts from the Web and building a language model of abstract moves .", "ner": [["method", 1, 1, "Generic"], ["abstracts", 9, 9, "Material"], ["Web", 12, 12, "Material"], ["language model", 16, 17, "Method"], ["abstract moves", 19, 20, "Material"]], "relations": [[12, 12, 9, 9, "USED-FOR"], [19, 20, 16, 17, "USED-FOR"]]},
{"sentence": "We also present a prototype concordancer , CARE , which exploits the move-tagged abstracts for digital learning .", "ner": [["prototype concordancer", 4, 5, "Method"], ["CARE", 7, 7, "Method"], ["move-tagged abstracts", 12, 13, "Material"], ["digital learning", 15, 16, "Task"]], "relations": [[7, 7, 4, 5, "HYPONYM-OF"], [7, 7, 12, 13, "USED-FOR"], [12, 13, 15, 16, "USED-FOR"]]},
{"sentence": "This system provides a promising approach to Web-based computer-assisted academic writing .", "ner": [["system", 1, 1, "Generic"], ["approach", 5, 5, "Generic"], ["Web-based computer-assisted academic writing", 7, 10, "Task"]], "relations": [[1, 1, 5, 5, "USED-FOR"], [5, 5, 7, 10, "USED-FOR"]]},
{"sentence": "This work presents a real-time system for multiple object tracking in dynamic scenes .", "ner": [["real-time system", 4, 5, "Method"], ["multiple object tracking in dynamic scenes", 7, 12, "Task"]], "relations": [[4, 5, 7, 12, "USED-FOR"]]},
{"sentence": "A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a prior knowledge about the shape or motion of objects .", "ner": [["system", 5, 5, "Generic"], ["long-duration and complete occlusion", 12, 15, "OtherScientificTerm"], ["prior knowledge", 18, 19, "OtherScientificTerm"], ["shape", 22, 22, "OtherScientificTerm"], ["motion of objects", 24, 26, "OtherScientificTerm"]], "relations": [[5, 5, 12, 15, "USED-FOR"], [18, 19, 22, 22, "FEATURE-OF"], [18, 19, 24, 26, "FEATURE-OF"], [22, 22, 24, 26, "CONJUNCTION"]]},
{"sentence": "The system produces good segment and tracking results at a frame rate of 15-20 fps for image size of 320x240 , as demonstrated by extensive experiments performed using video sequences under different conditions indoor and outdoor with long-duration and complete occlusions in changing background .", "ner": [["system", 1, 1, "Generic"], ["tracking", 6, 6, "Task"], ["frame rate", 10, 11, "OtherScientificTerm"], ["image size", 16, 17, "OtherScientificTerm"], ["video sequences", 28, 29, "Material"], ["long-duration and complete occlusions", 37, 40, "OtherScientificTerm"], ["changing background", 42, 43, "OtherScientificTerm"]], "relations": [[6, 6, 1, 1, "EVALUATE-FOR"]]},
{"sentence": "We propose a method of organizing reading materials for vocabulary learning .", "ner": [["method", 3, 3, "Generic"], ["organizing reading materials", 5, 7, "Task"], ["vocabulary learning", 9, 10, "Task"]], "relations": [[3, 3, 5, 7, "USED-FOR"], [5, 7, 9, 10, "USED-FOR"]]},
{"sentence": "It enables us to select a concise set of reading texts -LRB- from a target corpus -RRB- that contains all the target vocabulary to be learned .", "ner": [["It", 0, 0, "Generic"], ["target vocabulary", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "We used a specialized vocabulary for an English certification test as the target vocabulary and used English Wikipedia , a free-content encyclopedia , as the target corpus .", "ner": [["English Wikipedia", 16, 17, "Material"], ["free-content encyclopedia", 20, 21, "Material"]], "relations": [[16, 17, 20, 21, "HYPONYM-OF"]]},
{"sentence": "The organized reading materials would enable learners not only to study the target vocabulary efficiently but also to gain a variety of knowledge through reading .", "ner": [], "relations": []},
{"sentence": "The reading materials are available on our web site .", "ner": [], "relations": []},
{"sentence": "A novel bootstrapping approach to Named Entity -LRB- NE -RRB- tagging using concept-based seeds and successive learners is presented .", "ner": [["bootstrapping approach", 2, 3, "Method"], ["Named Entity -LRB- NE -RRB- tagging", 5, 10, "Task"], ["concept-based seeds", 12, 13, "OtherScientificTerm"], ["successive learners", 15, 16, "Method"]], "relations": [[2, 3, 5, 10, "USED-FOR"], [12, 13, 2, 3, "USED-FOR"], [12, 13, 15, 16, "CONJUNCTION"], [15, 16, 2, 3, "USED-FOR"]]},
{"sentence": "This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted NE , e.g. he/she/man / woman for PERSON NE .", "ner": [["approach", 1, 1, "Generic"], ["NE", 19, 19, "OtherScientificTerm"], ["PERSON NE", 26, 27, "OtherScientificTerm"]], "relations": [[26, 27, 19, 19, "HYPONYM-OF"]]},
{"sentence": "The bootstrapping procedure is implemented as training two successive learners .", "ner": [["bootstrapping procedure", 1, 2, "Method"], ["successive learners", 8, 9, "Method"]], "relations": [[8, 9, 1, 2, "USED-FOR"]]},
{"sentence": "First , decision list is used to learn the parsing-based NE rules .", "ner": [["decision list", 2, 3, "OtherScientificTerm"], ["parsing-based NE rules", 9, 11, "OtherScientificTerm"]], "relations": [[2, 3, 9, 11, "USED-FOR"]]},
{"sentence": "Then , a Hidden Markov Model is trained on a corpus automatically tagged by the first learner .", "ner": [["Hidden Markov Model", 3, 5, "Method"], ["learner", 16, 16, "Generic"]], "relations": []},
{"sentence": "The resulting NE system approaches supervised NE performance for some NE types .", "ner": [["NE system", 2, 3, "Method"], ["supervised NE", 5, 6, "Task"]], "relations": [[2, 3, 5, 6, "USED-FOR"]]},
{"sentence": "We present the first known empirical test of an increasingly common speculative claim , by evaluating a representative Chinese-to-English SMT model directly on word sense disambiguation performance , using standard WSD evaluation methodology and datasets from the Senseval-3 Chinese lexical sample task .", "ner": [["Chinese-to-English SMT model", 18, 20, "Method"], ["word sense disambiguation", 23, 25, "Task"], ["WSD evaluation methodology", 30, 32, "Method"], ["Senseval-3 Chinese lexical sample task", 37, 41, "Material"]], "relations": [[23, 25, 18, 20, "EVALUATE-FOR"], [30, 32, 18, 20, "EVALUATE-FOR"], [37, 41, 18, 20, "EVALUATE-FOR"]]},
{"sentence": "Much effort has been put in designing and evaluating dedicated word sense disambiguation -LRB- WSD -RRB- models , in particular with the Senseval series of workshops .", "ner": [["dedicated word sense disambiguation -LRB- WSD -RRB- models", 9, 16, "Method"], ["Senseval series of workshops", 22, 25, "Material"]], "relations": [[22, 25, 9, 16, "EVALUATE-FOR"]]},
{"sentence": "At the same time , the recent improvements in the BLEU scores of statistical machine translation -LRB- SMT -RRB- suggests that SMT models are good at predicting the right translation of the words in source language sentences .", "ner": [["BLEU scores", 10, 11, "Metric"], ["statistical machine translation -LRB- SMT -RRB-", 13, 18, "Task"], ["SMT models", 21, 22, "Method"], ["translation", 29, 29, "Task"]], "relations": [[10, 11, 13, 18, "EVALUATE-FOR"], [21, 22, 29, 29, "USED-FOR"]]},
{"sentence": "Surprisingly however , the WSD accuracy of SMT models has never been evaluated and compared with that of the dedicated WSD models .", "ner": [["WSD accuracy", 4, 5, "Metric"], ["SMT models", 7, 8, "Method"], ["that", 16, 16, "Generic"], ["dedicated WSD models", 19, 21, "Method"]], "relations": [[4, 5, 7, 8, "EVALUATE-FOR"], [16, 16, 4, 5, "COMPARE"]]},
{"sentence": "We present controlled experiments showing the WSD accuracy of current typical SMT models to be significantly lower than that of all the dedicated WSD models considered .", "ner": [["WSD accuracy", 6, 7, "Metric"], ["SMT models", 11, 12, "Method"], ["that", 18, 18, "Generic"], ["dedicated WSD models", 22, 24, "Method"]], "relations": [[6, 7, 11, 12, "EVALUATE-FOR"], [18, 18, 6, 7, "COMPARE"]]},
{"sentence": "This tends to support the view that despite recent speculative claims to the contrary , current SMT models do have limitations in comparison with dedicated WSD models , and that SMT should benefit from the better predictions made by the WSD models .", "ner": [["SMT models", 16, 17, "Method"], ["dedicated WSD models", 24, 26, "Method"], ["SMT", 30, 30, "Method"], ["WSD models", 40, 41, "Method"]], "relations": [[16, 17, 24, 26, "COMPARE"], [40, 41, 30, 30, "USED-FOR"]]},
{"sentence": "In this paper we present a novel , customizable : IE paradigm that takes advantage of predicate-argument structures .", "ner": [["IE paradigm", 10, 11, "Method"], ["predicate-argument structures", 16, 17, "OtherScientificTerm"]], "relations": [[16, 17, 10, 11, "USED-FOR"]]},
{"sentence": "We also introduce a new way of automatically identifying predicate argument structures , which is central to our IE paradigm .", "ner": [["automatically identifying predicate argument structures", 7, 11, "Task"], ["IE paradigm", 18, 19, "Method"]], "relations": []},
{"sentence": "It is based on : -LRB- 1 -RRB- an extended set of features ; and -LRB- 2 -RRB- inductive decision tree learning .", "ner": [["It", 0, 0, "Generic"], ["features", 12, 12, "OtherScientificTerm"], ["inductive decision tree learning", 18, 21, "Method"]], "relations": [[12, 12, 0, 0, "USED-FOR"], [12, 12, 18, 21, "CONJUNCTION"], [18, 21, 0, 0, "USED-FOR"]]},
{"sentence": "The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results .", "ner": [["predicate-argument structures", 8, 9, "OtherScientificTerm"], ["IE", 13, 13, "Task"]], "relations": [[8, 9, 13, 13, "USED-FOR"]]},
{"sentence": "In this paper we present a statistical profile of the Named Entity task , a specific information extraction task for which corpora in several languages are available .", "ner": [["statistical profile", 6, 7, "Generic"], ["Named Entity task", 10, 12, "Task"], ["information extraction task", 16, 18, "Task"]], "relations": [[6, 7, 10, 12, "USED-FOR"], [10, 12, 16, 18, "HYPONYM-OF"]]},
{"sentence": "Using the results of the statistical analysis , we propose an algorithm for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .", "ner": [["statistical analysis", 5, 6, "Method"], ["algorithm", 11, 11, "Generic"], ["lower bound estimation", 13, 15, "Method"], ["Named Entity corpora", 17, 19, "Material"], ["analysis", 31, 31, "Generic"]], "relations": [[5, 6, 11, 11, "USED-FOR"], [11, 11, 13, 15, "USED-FOR"], [13, 15, 17, 19, "USED-FOR"]]},
{"sentence": "We attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely unsuper-vised induction of an SMT-style stochastic transduction grammar for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .", "ner": [["under-explored language genre of spoken language", 4, 9, "Material"], ["lyrics in music", 11, 13, "Material"], ["unsuper-vised induction", 17, 18, "Method"], ["SMT-style stochastic transduction grammar", 21, 24, "Method"], ["hip hop lyrics", 26, 28, "Material"], ["fully-automatically learned challenge-response system", 32, 35, "Method"], ["rhyming lyrics", 38, 39, "Material"]], "relations": [[11, 13, 4, 9, "HYPONYM-OF"], [17, 18, 4, 9, "USED-FOR"], [17, 18, 21, 24, "USED-FOR"], [17, 18, 32, 35, "USED-FOR"], [26, 28, 21, 24, "FEATURE-OF"], [32, 35, 38, 39, "USED-FOR"]]},
{"sentence": "Unlike previous efforts , we choose the domain of hip hop lyrics , which is particularly unstructured and noisy .", "ner": [["hip hop lyrics", 9, 11, "Material"]], "relations": []},
{"sentence": "A novel feature of our approach is that it is completely unsupervised and requires no a priori linguistic or phonetic knowledge .", "ner": [["approach", 5, 5, "Generic"], ["priori linguistic or phonetic knowledge", 16, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "In spite of the level of difficulty of the challenge , the model nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used phrase-based SMT models upon the same task .", "ner": [["model", 12, 12, "Generic"], ["human evaluators", 20, 21, "OtherScientificTerm"], ["phrase-based SMT models", 30, 32, "Method"], ["task", 36, 36, "Generic"]], "relations": [[12, 12, 30, 32, "COMPARE"], [36, 36, 12, 12, "EVALUATE-FOR"], [36, 36, 30, 32, "EVALUATE-FOR"]]},
{"sentence": "In this paper , we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue .", "ner": [["predicting segment boundaries", 10, 12, "Task"], ["spoken multiparty dialogue", 14, 16, "Material"]], "relations": [[14, 16, 10, 12, "USED-FOR"]]},
{"sentence": "We extend prior work in two ways .", "ner": [], "relations": []},
{"sentence": "We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries .", "ner": [["approaches", 3, 3, "Generic"], ["predicting top-level topic shifts", 9, 12, "Task"], ["identifying subtopic boundaries", 17, 19, "Task"]], "relations": [[3, 3, 9, 12, "USED-FOR"], [3, 3, 17, 19, "USED-FOR"], [9, 12, 17, 19, "FEATURE-OF"]]},
{"sentence": "We then explore the impact on performance of using ASR output as opposed to human transcription .", "ner": [["ASR output", 9, 10, "OtherScientificTerm"], ["human transcription", 14, 15, "OtherScientificTerm"]], "relations": [[9, 10, 14, 15, "COMPARE"]]},
{"sentence": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "ner": [["features", 5, 5, "OtherScientificTerm"], ["predicting top-level and predicting subtopic boundaries", 8, 13, "Task"], ["tasks", 17, 17, "Generic"], ["predicting subtopic boundaries", 23, 25, "Task"], ["lexical cohesion-based approach", 28, 30, "Method"], ["predicting top-level boundaries", 41, 43, "Task"], ["machine learning approach", 46, 48, "Method"], ["lexical-cohesion and conversational features", 51, 54, "OtherScientificTerm"], ["conversational cues", 62, 63, "OtherScientificTerm"], ["cue phrases", 67, 68, "OtherScientificTerm"], ["overlapping speech", 70, 71, "OtherScientificTerm"], ["indicators", 75, 75, "Generic"], ["top-level prediction task", 78, 80, "Generic"]], "relations": [[23, 25, 8, 13, "PART-OF"], [28, 30, 23, 25, "USED-FOR"], [41, 43, 8, 13, "PART-OF"], [46, 48, 41, 43, "USED-FOR"], [51, 54, 46, 48, "CONJUNCTION"], [67, 68, 62, 63, "HYPONYM-OF"], [70, 71, 62, 63, "HYPONYM-OF"], [70, 71, 67, 68, "CONJUNCTION"], [75, 75, 78, 80, "USED-FOR"]]},
{"sentence": "We also find that the transcription errors inevitable in ASR output have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks .", "ner": [["transcription errors", 5, 6, "OtherScientificTerm"], ["ASR output", 9, 10, "OtherScientificTerm"], ["models", 16, 16, "Generic"], ["lexical-cohesion and conversational features", 19, 22, "OtherScientificTerm"], ["tasks", 36, 36, "Generic"]], "relations": [[5, 6, 9, 10, "FEATURE-OF"], [16, 16, 19, 22, "CONJUNCTION"]]},
{"sentence": "We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton .", "ner": [["unsupervised technique", 4, 5, "Method"], ["morphology", 8, 8, "Task"], ["hubs", 11, 11, "OtherScientificTerm"], ["automaton", 14, 14, "OtherScientificTerm"]], "relations": [[4, 5, 8, 8, "USED-FOR"], [11, 11, 4, 5, "USED-FOR"], [11, 11, 14, 14, "PART-OF"]]},
{"sentence": "For our purposes , a hub is a node in a graph with in-degree greater than one and out-degree greater than one .", "ner": [["hub", 5, 5, "OtherScientificTerm"], ["node", 8, 8, "OtherScientificTerm"], ["graph", 11, 11, "OtherScientificTerm"]], "relations": [[5, 5, 8, 8, "HYPONYM-OF"], [8, 8, 11, 11, "PART-OF"]]},
{"sentence": "We create a word-trie , transform it into a minimal DFA , then identify hubs .", "ner": [["word-trie", 3, 3, "Method"], ["it", 6, 6, "Generic"], ["minimal DFA", 9, 10, "OtherScientificTerm"], ["hubs", 14, 14, "OtherScientificTerm"]], "relations": [[3, 3, 14, 14, "USED-FOR"], [9, 10, 14, 14, "USED-FOR"]]},
{"sentence": "Those hubs mark the boundary between root and suffix , achieving similar performance to more complex mixtures of techniques .", "ner": [["hubs", 1, 1, "OtherScientificTerm"], ["root", 6, 6, "OtherScientificTerm"], ["suffix", 8, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "In Bayesian machine learning , conjugate priors are popular , mostly due to mathematical convenience .", "ner": [["Bayesian machine learning", 1, 3, "Task"], ["conjugate priors", 5, 6, "OtherScientificTerm"]], "relations": [[5, 6, 1, 3, "PART-OF"]]},
{"sentence": "In this paper , we show that there are deeper reasons for choosing a conjugate prior .", "ner": [["conjugate prior", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "Specifically , we formulate the conjugate prior in the form of Bregman divergence and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive .", "ner": [["conjugate prior", 5, 6, "OtherScientificTerm"], ["Bregman divergence", 11, 12, "OtherScientificTerm"], ["conjugate priors", 22, 23, "OtherScientificTerm"]], "relations": [[11, 12, 5, 6, "FEATURE-OF"]]},
{"sentence": "This geometric interpretation allows one to view the hyperparameters of conjugate priors as the effective sample points , thus providing additional intuition .", "ner": [["conjugate priors", 10, 11, "OtherScientificTerm"]], "relations": []},
{"sentence": "We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning .", "ner": [["geometric understanding of conjugate priors", 3, 7, "OtherScientificTerm"], ["conjugate priors", 6, 7, "OtherScientificTerm"], ["hyperparameters", 11, 11, "Generic"], ["prior", 16, 16, "Generic"], ["generative and discriminative components", 21, 24, "Method"], ["hybrid model", 27, 28, "Method"], ["semi-supervised learning", 30, 31, "Task"]], "relations": [[3, 7, 11, 11, "USED-FOR"], [16, 16, 21, 24, "USED-FOR"], [21, 24, 27, 28, "PART-OF"], [27, 28, 30, 31, "USED-FOR"]]},
{"sentence": "This paper defines a generative probabilistic model of parse trees , which we call PCFG-LA .", "ner": [["generative probabilistic model of parse trees", 4, 9, "Method"], ["PCFG-LA", 14, 14, "Method"]], "relations": [[14, 14, 4, 9, "HYPONYM-OF"]]},
{"sentence": "This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables .", "ner": [["model", 1, 1, "Generic"], ["PCFG", 6, 6, "Method"], ["non-terminal symbols", 9, 10, "OtherScientificTerm"], ["latent variables", 14, 15, "OtherScientificTerm"]], "relations": [[6, 6, 1, 1, "USED-FOR"], [9, 10, 6, 6, "PART-OF"], [14, 15, 9, 10, "USED-FOR"]]},
{"sentence": "Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm .", "ner": [["Finegrained CFG rules", 0, 2, "OtherScientificTerm"], ["parsed corpus", 8, 9, "Material"], ["PCFG-LA model", 13, 14, "Method"], ["EM-algorithm", 17, 17, "Method"]], "relations": [[8, 9, 0, 2, "USED-FOR"], [17, 17, 13, 14, "USED-FOR"]]},
{"sentence": "Because exact parsing with a PCFG-LA is NP-hard , several approximations are described and empirically compared .", "ner": [["exact parsing", 1, 2, "Task"], ["PCFG-LA", 5, 5, "Method"]], "relations": [[5, 5, 1, 2, "USED-FOR"]]},
{"sentence": "In experiments using the Penn WSJ corpus , our automatically trained model gave a performance of 86.6 % -LRB- F1 , sentences < 40 words -RRB- , which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection .", "ner": [["Penn WSJ corpus", 4, 6, "Material"], ["model", 11, 11, "Generic"], ["F1", 19, 19, "Metric"], ["unlexicalized PCFG parser", 34, 36, "Method"], ["manual feature selection", 40, 42, "Method"]], "relations": [[4, 6, 11, 11, "EVALUATE-FOR"], [4, 6, 34, 36, "EVALUATE-FOR"], [11, 11, 34, 36, "COMPARE"], [19, 19, 11, 11, "EVALUATE-FOR"], [19, 19, 34, 36, "EVALUATE-FOR"], [40, 42, 34, 36, "USED-FOR"]]},
{"sentence": "This paper reports on two contributions to large vocabulary continuous speech recognition .", "ner": [["large vocabulary continuous speech recognition", 7, 11, "Task"]], "relations": []},
{"sentence": "First , we present a new paradigm for speaker-independent -LRB- SI -RRB- training of hidden Markov models -LRB- HMM -RRB- , which uses a large amount of speech from a few speakers instead of the traditional practice of using a little speech from many speakers .", "ner": [["speaker-independent -LRB- SI -RRB- training of hidden Markov models -LRB- HMM -RRB-", 8, 19, "Task"], ["speech", 27, 27, "Material"]], "relations": [[27, 27, 8, 19, "USED-FOR"]]},
{"sentence": "In addition , combination of the training speakers is done by averaging the statistics of independently trained models rather than the usual pooling of all the speech data from many speakers prior to training .", "ner": [["averaging the statistics of independently trained models", 11, 17, "OtherScientificTerm"], ["pooling of all the speech data", 22, 27, "OtherScientificTerm"]], "relations": [[11, 17, 22, 27, "COMPARE"]]},
{"sentence": "With only 12 training speakers for SI recognition , we achieved a 7.5 % word error rate on a standard grammar and test set from the DARPA Resource Management corpus .", "ner": [["SI recognition", 6, 7, "Task"], ["word error rate", 14, 16, "Metric"], ["DARPA Resource Management corpus", 26, 29, "Material"]], "relations": [[14, 16, 6, 7, "EVALUATE-FOR"], [26, 29, 6, 7, "EVALUATE-FOR"]]},
{"sentence": "This performance is comparable to our best condition for this test suite , using 109 training speakers .", "ner": [], "relations": []},
{"sentence": "Second , we show a significant improvement for speaker adaptation -LRB- SA -RRB- using the new SI corpus and a small amount of speech from the new -LRB- target -RRB- speaker .", "ner": [["speaker adaptation -LRB- SA -RRB-", 8, 12, "Task"], ["SI corpus", 16, 17, "Material"]], "relations": [[16, 17, 8, 12, "EVALUATE-FOR"]]},
{"sentence": "A probabilistic spectral mapping is estimated independently for each training -LRB- reference -RRB- speaker and the target speaker .", "ner": [["probabilistic spectral mapping", 1, 3, "Method"]], "relations": []},
{"sentence": "Each reference model is transformed to the space of the target speaker and combined by averaging .", "ner": [["reference model", 1, 2, "Method"]], "relations": []},
{"sentence": "Using only 40 utterances from the target speaker for adaptation , the error rate dropped to 4.1 % -- a 45 % reduction in error compared to the SI result .", "ner": [["adaptation", 9, 9, "Task"], ["error rate", 12, 13, "Metric"], ["SI", 28, 28, "Task"]], "relations": [[12, 13, 9, 9, "EVALUATE-FOR"]]},
{"sentence": "In this paper , we show how to construct a transfer dictionary automatically .", "ner": [["transfer dictionary", 10, 11, "Material"]], "relations": []},
{"sentence": "Dictionary construction , one of the most difficult tasks in developing a machine translation system , is expensive .", "ner": [["Dictionary construction", 0, 1, "Task"], ["machine translation system", 12, 14, "Task"]], "relations": [[0, 1, 12, 14, "PART-OF"]]},
{"sentence": "To avoid this problem , we investigate how we build a dictionary using existing linguistic resources .", "ner": [["dictionary", 11, 11, "Generic"], ["linguistic resources", 14, 15, "Material"]], "relations": [[14, 15, 11, 11, "USED-FOR"]]},
{"sentence": "Our algorithm can be applied to any language pairs , but for the present we focus on building a Korean-to-Japanese dictionary using English as a pivot .", "ner": [["Korean-to-Japanese dictionary", 19, 20, "Material"], ["English", 22, 22, "Material"]], "relations": [[22, 22, 19, 20, "USED-FOR"]]},
{"sentence": "We attempt three ways of automatic construction to corroborate the effect of the directionality of dictionaries .", "ner": [["automatic construction", 5, 6, "Task"], ["directionality of dictionaries", 13, 15, "OtherScientificTerm"], ["dictionaries", 15, 15, "Generic"]], "relations": [[5, 6, 13, 15, "EVALUATE-FOR"]]},
{"sentence": "First , we introduce `` one-time look up '' method using a Korean-to-English and a Japanese-to-English dictionary .", "ner": [["`` one-time look up '' method", 4, 9, "Method"], ["Korean-to-English and a Japanese-to-English dictionary", 12, 16, "Material"]], "relations": [[12, 16, 4, 9, "USED-FOR"]]},
{"sentence": "Second , we show a method using `` overlapping constraint '' with a Korean-to-English dictionary and an English-to-Japanese dictionary .", "ner": [["method", 5, 5, "Generic"], ["`` overlapping constraint ''", 7, 10, "OtherScientificTerm"], ["Korean-to-English dictionary", 13, 14, "Material"], ["English-to-Japanese dictionary", 17, 18, "Material"]], "relations": [[7, 10, 5, 5, "USED-FOR"], [13, 14, 5, 5, "USED-FOR"], [13, 14, 17, 18, "CONJUNCTION"], [17, 18, 5, 5, "USED-FOR"]]},
{"sentence": "Third , we consider another alternative method rarely used for building a dictionary : an English-to-Korean dictionary and English-to-Japanese dictionary .", "ner": [["method", 6, 6, "Generic"], ["dictionary", 12, 12, "Generic"], ["English-to-Korean dictionary", 15, 16, "Material"], ["English-to-Japanese dictionary", 18, 19, "Material"]], "relations": [[6, 6, 12, 12, "USED-FOR"], [15, 16, 12, 12, "HYPONYM-OF"], [15, 16, 18, 19, "CONJUNCTION"], [18, 19, 12, 12, "HYPONYM-OF"]]},
{"sentence": "We found that the first method is the most effective and the best result can be obtained from combining the three methods .", "ner": [["method", 5, 5, "Generic"]], "relations": []},
{"sentence": "An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented .", "ner": [["CFG filtering techniques", 4, 6, "Method"], ["LTAG", 8, 8, "Method"], ["HPSG", 10, 10, "Method"]], "relations": [[4, 6, 8, 8, "USED-FOR"], [4, 6, 10, 10, "USED-FOR"], [8, 8, 10, 10, "COMPARE"]]},
{"sentence": "We demonstrate that an approximation of HPSG produces a more effective CFG filter than that of LTAG .", "ner": [["approximation of HPSG", 4, 6, "Method"], ["HPSG", 6, 6, "Method"], ["CFG filter", 11, 12, "Method"], ["that", 14, 14, "Generic"], ["LTAG", 16, 16, "Method"]], "relations": [[4, 6, 11, 12, "USED-FOR"], [11, 12, 14, 14, "COMPARE"], [14, 14, 16, 16, "USED-FOR"]]},
{"sentence": "We also investigate the reason for that difference .", "ner": [], "relations": []},
{"sentence": "Syntax-based statistical machine translation -LRB- MT -RRB- aims at applying statistical models to structured data .", "ner": [["Syntax-based statistical machine translation -LRB- MT -RRB-", 0, 6, "Task"], ["statistical models", 10, 11, "Method"], ["structured data", 13, 14, "Material"]], "relations": [[10, 11, 0, 6, "USED-FOR"], [13, 14, 10, 11, "USED-FOR"]]},
{"sentence": "In this paper , we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar .", "ner": [["syntax-based statistical machine translation system", 7, 11, "Task"], ["probabilistic synchronous dependency insertion grammar", 15, 19, "Method"]], "relations": [[15, 19, 7, 11, "USED-FOR"]]},
{"sentence": "Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees .", "ner": [["Synchronous dependency insertion grammars", 0, 3, "Method"], ["synchronous grammars", 8, 9, "Method"], ["dependency trees", 12, 13, "OtherScientificTerm"]], "relations": [[0, 3, 8, 9, "HYPONYM-OF"], [12, 13, 0, 3, "FEATURE-OF"]]},
{"sentence": "We first introduce our approach to inducing such a grammar from parallel corpora .", "ner": [["approach", 4, 4, "Generic"], ["grammar", 9, 9, "Method"], ["parallel corpora", 11, 12, "Material"]], "relations": [[4, 4, 9, 9, "USED-FOR"], [11, 12, 9, 9, "USED-FOR"]]},
{"sentence": "Second , we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree-to-tree transducer .", "ner": [["graphical model", 5, 6, "Method"], ["machine translation task", 9, 11, "Task"], ["stochastic tree-to-tree transducer", 20, 22, "Method"]], "relations": [[5, 6, 9, 11, "USED-FOR"], [20, 22, 5, 6, "USED-FOR"]]},
{"sentence": "We introduce a polynomial time decoding algorithm for the model .", "ner": [["polynomial time decoding algorithm", 3, 6, "Method"], ["model", 9, 9, "Generic"]], "relations": [[3, 6, 9, 9, "USED-FOR"]]},
{"sentence": "We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software .", "ner": [["MT system", 6, 7, "Method"], ["NIST and Bleu automatic MT evaluation software", 10, 16, "Metric"]], "relations": [[10, 16, 6, 7, "USED-FOR"]]},
{"sentence": "The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality .", "ner": [["system", 5, 5, "Generic"], ["baseline system", 8, 9, "Generic"], ["IBM models", 13, 14, "Method"], ["translation speed and quality", 17, 20, "Metric"]], "relations": [[5, 5, 8, 9, "COMPARE"], [13, 14, 8, 9, "USED-FOR"], [17, 20, 5, 5, "EVALUATE-FOR"], [17, 20, 8, 9, "EVALUATE-FOR"]]},
{"sentence": "We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences .", "ner": [["framework", 3, 3, "Generic"], ["concepts", 9, 9, "OtherScientificTerm"], ["distributional measures of word co-occurrences", 11, 15, "Method"]], "relations": [[11, 15, 3, 3, "USED-FOR"]]},
{"sentence": "We use the categories in a published thesaurus as coarse-grained concepts , allowing all possible distance values to be stored in a concept-concept matrix roughly .01 % the size of that created by existing measures .", "ner": [["coarse-grained concepts", 9, 10, "OtherScientificTerm"], ["concept-concept matrix", 22, 23, "OtherScientificTerm"]], "relations": []},
{"sentence": "We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of -LRB- 1 -RRB- ranking word pairs in order of semantic distance , and -LRB- 2 -RRB- correcting real-word spelling errors .", "ner": [["concept-distance measures", 6, 7, "Method"], ["distributional word-distance measures", 10, 12, "Method"], ["tasks", 15, 15, "Generic"], ["ranking word pairs in order of semantic distance", 20, 27, "Task"], ["correcting real-word spelling errors", 33, 36, "Task"]], "relations": [[6, 7, 15, 15, "USED-FOR"], [10, 12, 6, 7, "COMPARE"], [10, 12, 15, 15, "USED-FOR"], [20, 27, 15, 15, "HYPONYM-OF"], [33, 36, 15, 15, "HYPONYM-OF"], [33, 36, 20, 27, "CONJUNCTION"]]},
{"sentence": "In the latter task , of all the WordNet-based measures , only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures .", "ner": [["task", 3, 3, "Generic"], ["WordNet-based measures", 8, 9, "Method"], ["distributional concept-distance measures", 21, 23, "Method"]], "relations": [[3, 3, 8, 9, "EVALUATE-FOR"], [3, 3, 21, 23, "EVALUATE-FOR"], [21, 23, 8, 9, "COMPARE"]]},
{"sentence": "We focus on the problem of building large repositories of lexical conceptual structure -LRB- LCS -RRB- representations for verbs in multiple languages .", "ner": [["lexical conceptual structure -LRB- LCS -RRB- representations", 10, 16, "Method"]], "relations": []},
{"sentence": "One of the main results of this work is the definition of a relation between broad semantic classes and LCS meaning components .", "ner": [["broad semantic classes", 15, 17, "OtherScientificTerm"], ["LCS meaning components", 19, 21, "Method"]], "relations": [[15, 17, 19, 21, "CONJUNCTION"]]},
{"sentence": "Our acquisition program - LEXICALL - takes , as input , the result of previous work on verb classification and thematic grid tagging , and outputs LCS representations for different languages .", "ner": [["acquisition program - LEXICALL -", 1, 5, "Method"], ["verb classification", 17, 18, "Task"], ["thematic grid tagging", 20, 22, "Task"], ["LCS representations", 26, 27, "Method"]], "relations": [[1, 5, 26, 27, "USED-FOR"], [17, 18, 1, 5, "USED-FOR"], [17, 18, 20, 22, "CONJUNCTION"], [20, 22, 1, 5, "USED-FOR"]]},
{"sentence": "These representations have been ported into English , Arabic and Spanish lexicons , each containing approximately 9000 verbs .", "ner": [["representations", 1, 1, "Generic"], ["English , Arabic and Spanish lexicons", 6, 11, "OtherScientificTerm"]], "relations": [[1, 1, 6, 11, "USED-FOR"]]},
{"sentence": "We are currently using these lexicons in an operational foreign language tutoring and machine translation .", "ner": [["lexicons", 5, 5, "Generic"], ["operational foreign language tutoring", 8, 11, "Task"], ["machine translation", 13, 14, "Task"]], "relations": [[5, 5, 8, 11, "USED-FOR"], [5, 5, 13, 14, "USED-FOR"], [8, 11, 13, 14, "CONJUNCTION"]]},
{"sentence": "The theoretical study of the range concatenation grammar -LSB- RCG -RSB- formalism has revealed many attractive properties which may be used in NLP .", "ner": [["range concatenation grammar -LSB- RCG -RSB- formalism", 5, 11, "Method"], ["NLP", 22, 22, "Task"]], "relations": [[5, 11, 22, 22, "USED-FOR"]]},
{"sentence": "In particular , range concatenation languages -LSB- RCL -RSB- can be parsed in polynomial time and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity .", "ner": [["range concatenation languages -LSB- RCL -RSB-", 3, 8, "OtherScientificTerm"], ["polynomial time", 13, 14, "OtherScientificTerm"], ["grammatical formalisms", 18, 19, "Method"], ["RCGs", 25, 25, "Method"], ["worst-case parsing time complexity", 29, 32, "Metric"]], "relations": [[13, 14, 3, 8, "FEATURE-OF"], [29, 32, 18, 19, "EVALUATE-FOR"]]},
{"sentence": "For example , after translation into an equivalent RCG , any tree adjoining grammar can be parsed in O -LRB- n6 -RRB- time .", "ner": [["RCG", 8, 8, "Method"], ["tree adjoining grammar", 11, 13, "Method"], ["O -LRB- n6 -RRB- time", 18, 22, "OtherScientificTerm"]], "relations": [[18, 22, 11, 13, "FEATURE-OF"]]},
{"sentence": "In this paper , we study a parsing technique whose purpose is to improve the practical efficiency of RCL parsers .", "ner": [["parsing technique", 7, 8, "Method"], ["RCL parsers", 18, 19, "Method"]], "relations": [[7, 8, 18, 19, "USED-FOR"]]},
{"sentence": "The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .", "ner": [["non-deterministic parsing", 1, 2, "Method"], ["main parser", 6, 7, "Method"], ["language L", 10, 11, "OtherScientificTerm"], ["shared derivation forest", 20, 22, "OtherScientificTerm"], ["RCL parser", 27, 28, "Method"]], "relations": [[6, 7, 10, 11, "USED-FOR"], [27, 28, 20, 22, "USED-FOR"]]},
{"sentence": "The results of a practical evaluation of this method on a wide coverage English grammar are given .", "ner": [["method", 8, 8, "Generic"], ["wide coverage English grammar", 11, 14, "Method"]], "relations": [[11, 14, 8, 8, "EVALUATE-FOR"]]},
{"sentence": "In this paper we introduce Ant-Q , a family of algorithms which present many similarities with Q-learning -LRB- Watkins , 1989 -RRB- , and which we apply to the solution of symmetric and asym-metric instances of the traveling salesman problem -LRB- TSP -RRB- .", "ner": [["Ant-Q", 5, 5, "Method"], ["Q-learning", 16, 16, "Method"], ["symmetric and asym-metric instances of the traveling salesman problem -LRB- TSP -RRB-", 31, 42, "Task"]], "relations": [[5, 5, 31, 42, "USED-FOR"]]},
{"sentence": "Ant-Q algorithms were inspired by work on the ant system -LRB- AS -RRB- , a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in -LRB- Dorigo , 1992 ; Dorigo , Maniezzo and Colorni , 1996 -RRB- .", "ner": [["Ant-Q algorithms", 0, 1, "Method"], ["ant system -LRB- AS -RRB-", 8, 12, "Method"], ["distributed algorithm", 15, 16, "Method"], ["combinatorial optimization", 18, 19, "Task"]], "relations": [[8, 12, 0, 1, "USED-FOR"], [8, 12, 15, 16, "HYPONYM-OF"], [15, 16, 18, 19, "USED-FOR"]]},
{"sentence": "We show that AS is a particular instance of the Ant-Q family , and that there are instances of this family which perform better than AS .", "ner": [["AS", 3, 3, "Method"], ["Ant-Q family", 10, 11, "Method"], ["instances", 17, 17, "Generic"], ["family", 20, 20, "Generic"], ["AS", 25, 25, "OtherScientificTerm"]], "relations": [[3, 3, 10, 11, "HYPONYM-OF"], [17, 17, 20, 20, "PART-OF"], [17, 17, 25, 25, "COMPARE"]]},
{"sentence": "We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP 's are competitive with those obtained by other heuristic approaches based on neural networks or local search .", "ner": [["Ant-Q", 6, 6, "Method"], ["Ant-Q", 15, 15, "Method"], ["symmetric TSP", 17, 18, "Task"], ["heuristic approaches", 27, 28, "Method"], ["neural networks", 31, 32, "Method"], ["local search", 34, 35, "Method"]], "relations": [[15, 15, 17, 18, "USED-FOR"], [15, 15, 27, 28, "COMPARE"], [31, 32, 27, 28, "USED-FOR"], [31, 32, 34, 35, "CONJUNCTION"], [34, 35, 27, 28, "USED-FOR"]]},
{"sentence": "Finally , we apply Ant-Q to some difficult asymmetric TSP 's obtaining very good results : Ant-Q was able to find solutions of a quality which usually can be found only by very specialized algorithms .", "ner": [["Ant-Q", 4, 4, "Method"], ["asymmetric TSP", 8, 9, "Task"], ["Ant-Q", 16, 16, "Method"]], "relations": [[4, 4, 8, 9, "USED-FOR"]]},
{"sentence": "In this paper , we develop a geometric framework for linear or nonlinear discriminant subspace learning and classification .", "ner": [["geometric framework", 7, 8, "Method"], ["linear or nonlinear discriminant subspace learning and classification", 10, 17, "Task"]], "relations": [[7, 8, 10, 17, "USED-FOR"]]},
{"sentence": "In our framework , the structures of classes are conceptualized as a semi-Riemannian manifold which is considered as a submanifold embedded in an ambient semi-Riemannian space .", "ner": [["framework", 2, 2, "Generic"], ["structures of classes", 5, 7, "OtherScientificTerm"], ["semi-Riemannian manifold", 12, 13, "OtherScientificTerm"], ["submanifold", 19, 19, "OtherScientificTerm"], ["ambient semi-Riemannian space", 23, 25, "OtherScientificTerm"]], "relations": [[12, 13, 5, 7, "USED-FOR"], [19, 19, 23, 25, "PART-OF"]]},
{"sentence": "The class structures of original samples can be characterized and deformed by local metrics of the semi-Riemannian space .", "ner": [["class structures", 1, 2, "OtherScientificTerm"], ["local metrics of the semi-Riemannian space", 12, 17, "Metric"]], "relations": [[12, 17, 1, 2, "USED-FOR"]]},
{"sentence": "Semi-Riemannian metrics are uniquely determined by the smoothing of discrete functions and the nullity of the semi-Riemannian space .", "ner": [["Semi-Riemannian metrics", 0, 1, "Metric"], ["smoothing of discrete functions", 7, 10, "Method"], ["nullity of the semi-Riemannian space", 13, 17, "OtherScientificTerm"]], "relations": [[7, 10, 0, 1, "USED-FOR"], [7, 10, 13, 17, "CONJUNCTION"], [13, 17, 0, 1, "USED-FOR"]]},
{"sentence": "Based on the geometrization of class structures , optimizing class structures in the feature space is equivalent to maximizing the quadratic quantities of metric tensors in the semi-Riemannian space .", "ner": [["geometrization of class structures", 3, 6, "Method"], ["class structures", 9, 10, "OtherScientificTerm"], ["feature space", 13, 14, "OtherScientificTerm"], ["quadratic quantities of metric tensors", 20, 24, "OtherScientificTerm"], ["semi-Riemannian space", 27, 28, "OtherScientificTerm"]], "relations": [[13, 14, 9, 10, "FEATURE-OF"], [27, 28, 20, 24, "FEATURE-OF"]]},
{"sentence": "Thus supervised discriminant subspace learning reduces to unsupervised semi-Riemannian mani-fold learning .", "ner": [["supervised discriminant subspace learning", 1, 4, "Method"], ["unsupervised semi-Riemannian mani-fold learning", 7, 10, "Method"]], "relations": []},
{"sentence": "Based on the proposed framework , a novel algorithm , dubbed as Semi-Riemannian Discriminant Analysis -LRB- SRDA -RRB- , is presented for subspace-based classification .", "ner": [["framework", 4, 4, "Generic"], ["algorithm", 8, 8, "Generic"], ["Semi-Riemannian Discriminant Analysis -LRB- SRDA -RRB-", 12, 17, "Method"], ["subspace-based classification", 22, 23, "Task"]], "relations": [[4, 4, 8, 8, "USED-FOR"], [8, 8, 22, 23, "USED-FOR"]]},
{"sentence": "The performance of SRDA is tested on face recognition -LRB- singular case -RRB- and handwritten capital letter classification -LRB- nonsingular case -RRB- against existing algorithms .", "ner": [["SRDA", 3, 3, "Method"], ["face recognition -LRB- singular case", 7, 11, "Task"], ["handwritten capital letter classification -LRB- nonsingular case -RRB-", 14, 21, "Task"], ["algorithms", 24, 24, "Generic"]], "relations": [[3, 3, 24, 24, "COMPARE"], [7, 11, 3, 3, "EVALUATE-FOR"], [7, 11, 14, 21, "CONJUNCTION"], [7, 11, 24, 24, "EVALUATE-FOR"], [14, 21, 3, 3, "EVALUATE-FOR"], [14, 21, 24, 24, "EVALUATE-FOR"]]},
{"sentence": "The experimental results show that SRDA works well on recognition and classification , implying that semi-Riemannian geometry is a promising new tool for pattern recognition and machine learning .", "ner": [["SRDA", 5, 5, "Generic"], ["recognition", 9, 9, "Task"], ["classification", 11, 11, "Task"], ["semi-Riemannian geometry", 15, 16, "Method"], ["pattern recognition", 23, 24, "Task"], ["machine learning", 26, 27, "Task"]], "relations": [[5, 5, 9, 9, "USED-FOR"], [5, 5, 11, 11, "USED-FOR"], [9, 9, 11, 11, "CONJUNCTION"], [15, 16, 23, 24, "USED-FOR"], [15, 16, 26, 27, "USED-FOR"], [23, 24, 26, 27, "CONJUNCTION"]]},
{"sentence": "A deterministic parser is under development which represents a departure from traditional deterministic parsers in that it combines both symbolic and connectionist components .", "ner": [["deterministic parser", 1, 2, "Method"], ["deterministic parsers", 12, 13, "Method"], ["it", 16, 16, "Generic"], ["symbolic and connectionist components", 19, 22, "Method"]], "relations": [[1, 2, 12, 13, "COMPARE"], [19, 22, 16, 16, "PART-OF"]]},
{"sentence": "The connectionist component is trained either from patterns derived from the rules of a deterministic grammar .", "ner": [["connectionist component", 1, 2, "Method"], ["patterns", 7, 7, "OtherScientificTerm"], ["rules of a deterministic grammar", 11, 15, "Method"]], "relations": [[7, 7, 1, 2, "USED-FOR"], [11, 15, 7, 7, "USED-FOR"]]},
{"sentence": "The development and evolution of such a hybrid architecture has lead to a parser which is superior to any known deterministic parser .", "ner": [["hybrid architecture", 7, 8, "Method"], ["parser", 13, 13, "Method"], ["deterministic parser", 20, 21, "Method"]], "relations": [[7, 8, 13, 13, "USED-FOR"], [13, 13, 20, 21, "COMPARE"]]},
{"sentence": "Experiments are described and powerful training techniques are demonstrated that permit decision-making by the connectionist component in the parsing process .", "ner": [["training techniques", 5, 6, "Method"], ["decision-making", 11, 11, "Task"], ["connectionist component", 14, 15, "Method"], ["parsing process", 18, 19, "Method"]], "relations": [[5, 6, 11, 11, "USED-FOR"], [14, 15, 11, 11, "USED-FOR"], [14, 15, 18, 19, "PART-OF"]]},
{"sentence": "This approach has permitted some simplifications to the rules of other deterministic parsers , including the elimination of rule packets and priorities .", "ner": [["approach", 1, 1, "Generic"], ["rules", 8, 8, "OtherScientificTerm"], ["deterministic parsers", 11, 12, "Method"]], "relations": []},
{"sentence": "Furthermore , parsing is performed more robustly and with more tolerance for error .", "ner": [["parsing", 2, 2, "Task"]], "relations": []},
{"sentence": "Data are presented which show how a connectionist -LRB- neural -RRB- network trained with linguistic rules can parse both expected -LRB- grammatical -RRB- sentences as well as some novel -LRB- ungrammatical or lexically ambiguous -RRB- sentences .", "ner": [["connectionist -LRB- neural -RRB- network", 7, 11, "Method"], ["linguistic rules", 14, 15, "OtherScientificTerm"], ["expected -LRB- grammatical -RRB- sentences", 19, 23, "Material"], ["-LRB- ungrammatical or lexically ambiguous -RRB- sentences", 29, 35, "Material"]], "relations": [[7, 11, 19, 23, "USED-FOR"], [7, 11, 29, 35, "USED-FOR"], [14, 15, 7, 11, "USED-FOR"], [19, 23, 29, 35, "CONJUNCTION"]]},
{"sentence": "Robust natural language interpretation requires strong semantic domain models , fail-soft recovery heuristics , and very flexible control structures .", "ner": [["natural language interpretation", 1, 3, "Task"], ["semantic domain models", 6, 8, "Method"], ["fail-soft recovery heuristics", 10, 12, "Method"], ["control structures", 17, 18, "OtherScientificTerm"]], "relations": [[6, 8, 1, 3, "USED-FOR"], [6, 8, 10, 12, "CONJUNCTION"], [10, 12, 1, 3, "USED-FOR"], [10, 12, 17, 18, "CONJUNCTION"], [17, 18, 1, 3, "USED-FOR"]]},
{"sentence": "Although single-strategy parsers have met with a measure of success , a multi-strategy approach is shown to provide a much higher degree of flexibility , redundancy , and ability to bring task-specific domain knowledge -LRB- in addition to general linguistic knowledge -RRB- to bear on both grammatical and ungrammatical input .", "ner": [["single-strategy parsers", 1, 2, "Method"], ["multi-strategy approach", 12, 13, "Method"], ["task-specific domain knowledge", 31, 33, "OtherScientificTerm"], ["general linguistic knowledge", 38, 40, "OtherScientificTerm"], ["grammatical and ungrammatical input", 46, 49, "OtherScientificTerm"]], "relations": [[1, 2, 12, 13, "COMPARE"], [31, 33, 38, 40, "CONJUNCTION"]]},
{"sentence": "A parsing algorithm is presented that integrates several different parsing strategies , with case-frame instantiation dominating .", "ner": [["parsing algorithm", 1, 2, "Method"], ["parsing strategies", 9, 10, "Method"], ["case-frame instantiation", 13, 14, "OtherScientificTerm"]], "relations": [[9, 10, 1, 2, "PART-OF"], [13, 14, 9, 10, "HYPONYM-OF"]]},
{"sentence": "Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and ungrammatical structures , as well as less exotic , grammatically correct input .", "ner": [["parsing strategies", 3, 4, "Method"], ["conjunctions", 22, 22, "OtherScientificTerm"], ["fragmentary input", 24, 25, "OtherScientificTerm"], ["ungrammatical structures", 28, 29, "OtherScientificTerm"], ["exotic , grammatically correct input", 35, 39, "OtherScientificTerm"]], "relations": [[3, 4, 22, 22, "USED-FOR"], [3, 4, 24, 25, "USED-FOR"], [3, 4, 28, 29, "USED-FOR"], [3, 4, 35, 39, "USED-FOR"], [22, 22, 24, 25, "CONJUNCTION"], [24, 25, 28, 29, "CONJUNCTION"], [28, 29, 35, 39, "CONJUNCTION"]]},
{"sentence": "Several specific heuristics for handling ungrammatical input are presented within this multi-strategy framework .", "ner": [["specific heuristics", 1, 2, "Method"], ["ungrammatical input", 5, 6, "OtherScientificTerm"], ["multi-strategy framework", 11, 12, "Method"]], "relations": [[1, 2, 5, 6, "USED-FOR"], [1, 2, 11, 12, "PART-OF"]]},
{"sentence": "Recently , Stacked Auto-Encoders -LRB- SAE -RRB- have been successfully used for learning imbalanced datasets .", "ner": [["Stacked Auto-Encoders -LRB- SAE -RRB-", 2, 6, "Method"], ["learning imbalanced datasets", 12, 14, "Task"]], "relations": [[2, 6, 12, 14, "USED-FOR"]]},
{"sentence": "In this paper , for the first time , we propose to use a Neural Network classifier furnished by an SAE structure for detecting the errors made by a strong Automatic Speech Recognition -LRB- ASR -RRB- system .", "ner": [["Neural Network classifier", 14, 16, "Method"], ["SAE structure", 20, 21, "Method"], ["Automatic Speech Recognition -LRB- ASR -RRB- system", 30, 36, "Method"]], "relations": [[14, 16, 30, 36, "USED-FOR"], [20, 21, 14, 16, "USED-FOR"]]},
{"sentence": "Error detection on an automatic transcription provided by a '' strong '' ASR system , i.e. exhibiting a small word error rate , is difficult due to the limited number of '' positive '' examples -LRB- i.e. words erroneously recognized -RRB- available for training a binary classi-fier .", "ner": [["Error detection", 0, 1, "Task"], ["automatic transcription", 4, 5, "Material"], ["ASR system", 12, 13, "Method"], ["word error rate", 19, 21, "Metric"], ["binary classi-fier", 45, 46, "Method"]], "relations": [[0, 1, 4, 5, "USED-FOR"]]},
{"sentence": "In this paper we investigate and compare different types of classifiers for automatically detecting ASR errors , including the one based on a stacked auto-encoder architecture .", "ner": [["classifiers", 10, 10, "Method"], ["automatically detecting ASR errors", 12, 15, "Task"], ["one", 19, 19, "Generic"], ["stacked auto-encoder architecture", 23, 25, "Method"]], "relations": [[10, 10, 12, 15, "USED-FOR"], [19, 19, 10, 10, "HYPONYM-OF"], [23, 25, 19, 19, "USED-FOR"]]},
{"sentence": "We show the effectiveness of the latter by measuring and comparing performance on the automatic transcriptions of an English corpus collected from TED talks .", "ner": [["automatic transcriptions", 14, 15, "Material"], ["English corpus", 18, 19, "Material"], ["TED talks", 22, 23, "Material"]], "relations": [[18, 19, 14, 15, "FEATURE-OF"], [22, 23, 18, 19, "USED-FOR"]]},
{"sentence": "Performance of each investigated classifier is evaluated both via receiving operating curve and via a measure , called mean absolute error , related to the quality in predicting the corresponding word error rate .", "ner": [["classifier", 4, 4, "Method"], ["receiving operating curve", 9, 11, "Metric"], ["measure", 15, 15, "Generic"], ["mean absolute error", 18, 20, "Metric"], ["word error rate", 30, 32, "Metric"]], "relations": [[9, 11, 4, 4, "EVALUATE-FOR"], [9, 11, 15, 15, "CONJUNCTION"], [15, 15, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "The results demonstrates that the classifier based on SAE detects the ASR errors better than the other classification methods .", "ner": [["classifier", 5, 5, "Method"], ["SAE", 8, 8, "Method"], ["ASR errors", 11, 12, "OtherScientificTerm"], ["classification methods", 17, 18, "Method"]], "relations": [[5, 5, 11, 12, "USED-FOR"], [5, 5, 17, 18, "COMPARE"], [8, 8, 5, 5, "USED-FOR"], [17, 18, 11, 12, "USED-FOR"]]},
{"sentence": "Within the EU Network of Excellence PASCAL , a challenge was organized to design a statistical machine learning algorithm that segments words into the smallest meaning-bearing units of language , morphemes .", "ner": [["statistical machine learning algorithm", 15, 18, "Method"], ["smallest meaning-bearing units of language", 24, 28, "OtherScientificTerm"], ["morphemes", 30, 30, "OtherScientificTerm"]], "relations": [[15, 18, 24, 28, "USED-FOR"], [30, 30, 24, 28, "HYPONYM-OF"]]},
{"sentence": "Ideally , these are basic vocabulary units suitable for different tasks , such as speech and text understanding , machine translation , information retrieval , and statistical language modeling .", "ner": [["these", 2, 2, "Generic"], ["tasks", 10, 10, "Generic"], ["speech and text understanding", 14, 17, "Task"], ["machine translation", 19, 20, "Task"], ["information retrieval", 22, 23, "Task"], ["statistical language modeling", 26, 28, "Task"]], "relations": [[2, 2, 10, 10, "USED-FOR"], [14, 17, 10, 10, "HYPONYM-OF"], [14, 17, 19, 20, "CONJUNCTION"], [19, 20, 10, 10, "HYPONYM-OF"], [19, 20, 22, 23, "CONJUNCTION"], [22, 23, 10, 10, "HYPONYM-OF"], [22, 23, 26, 28, "CONJUNCTION"], [26, 28, 10, 10, "HYPONYM-OF"]]},
{"sentence": "Twelve research groups participated in the challenge and had submitted segmentation results obtained by their algorithms .", "ner": [], "relations": []},
{"sentence": "In this paper , we evaluate the application of these segmen-tation algorithms to large vocabulary speech recognition using statistical n-gram language models based on the proposed word segments instead of entire words .", "ner": [["segmen-tation algorithms", 10, 11, "Method"], ["large vocabulary speech recognition", 13, 16, "Task"], ["statistical n-gram language models", 18, 21, "Method"]], "relations": [[10, 11, 13, 16, "USED-FOR"], [18, 21, 10, 11, "EVALUATE-FOR"]]},
{"sentence": "Experiments were done for two ag-glutinative and morphologically rich languages : Finnish and Turk-ish .", "ner": [["ag-glutinative and morphologically rich languages", 5, 9, "Material"], ["Finnish", 11, 11, "Material"], ["Turk-ish", 13, 13, "Material"]], "relations": [[11, 11, 5, 9, "HYPONYM-OF"], [11, 11, 13, 13, "CONJUNCTION"], [13, 13, 5, 9, "HYPONYM-OF"]]},
{"sentence": "We also investigate combining various segmentations to improve the performance of the recognizer .", "ner": [], "relations": []},
{"sentence": "This paper describes a recently collected spoken language corpus for the ATIS -LRB- Air Travel Information System -RRB- domain .", "ner": [["spoken language corpus", 6, 8, "Material"], ["ATIS -LRB- Air Travel Information System -RRB- domain", 11, 18, "Task"]], "relations": [[6, 8, 11, 18, "USED-FOR"]]},
{"sentence": "This data collection effort has been co-ordinated by MADCOW -LRB- Multi-site ATIS Data COllection Working group -RRB- .", "ner": [["data collection", 1, 2, "Generic"]], "relations": []},
{"sentence": "We summarize the motivation for this effort , the goals , the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of spontaneous speech from five sites for use in a multi-site common evaluation of speech , natural language and spoken language .", "ner": [["multi-site data collection paradigm", 15, 18, "Method"], ["collection", 28, 28, "Generic"], ["spontaneous speech", 35, 36, "Material"], ["multi-site common evaluation of speech , natural language and spoken language", 44, 54, "Task"]], "relations": [[35, 36, 44, 54, "EVALUATE-FOR"]]},
{"sentence": "This paper proposes the Hierarchical Directed Acyclic Graph -LRB- HDAG -RRB- Kernel for structured natural language data .", "ner": [["Hierarchical Directed Acyclic Graph -LRB- HDAG -RRB- Kernel", 4, 11, "Method"], ["structured natural language data", 13, 16, "Material"]], "relations": [[4, 11, 13, 16, "USED-FOR"]]},
{"sentence": "The HDAG Kernel directly accepts several levels of both chunks and their relations , and then efficiently computes the weighed sum of the number of common attribute sequences of the HDAGs .", "ner": [["HDAG Kernel", 1, 2, "Method"], ["HDAGs", 30, 30, "Method"]], "relations": []},
{"sentence": "We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function .", "ner": [["method", 4, 4, "Generic"], ["question classification and sentence alignment tasks", 6, 11, "Task"], ["similarity measure", 18, 19, "Metric"], ["kernel function", 22, 23, "Method"]], "relations": [[4, 4, 6, 11, "USED-FOR"], [18, 19, 4, 4, "EVALUATE-FOR"], [18, 19, 22, 23, "CONJUNCTION"], [22, 23, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "The results of the experiments demonstrate that the HDAG Kernel is superior to other kernel functions and baseline methods .", "ner": [["HDAG Kernel", 8, 9, "Method"], ["kernel functions", 14, 15, "Method"], ["baseline methods", 17, 18, "Generic"]], "relations": [[8, 9, 14, 15, "COMPARE"], [8, 9, 17, 18, "COMPARE"], [14, 15, 17, 18, "CONJUNCTION"]]},
{"sentence": "We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies .", "ner": [["CoNLL 2008 shared task", 9, 12, "Task"], ["generative history-based latent variable model", 16, 20, "Method"], ["synchronous dependency parser", 29, 31, "Task"], ["syntactic and semantic dependencies", 34, 37, "OtherScientificTerm"]], "relations": [[16, 20, 9, 12, "USED-FOR"], [16, 20, 29, 31, "USED-FOR"], [29, 31, 34, 37, "USED-FOR"]]},
{"sentence": "The submitted model yields 79.1 % macro-average F1 performance , for the joint task , 86.9 % syntactic dependencies LAS and 71.0 % semantic dependencies F1 .", "ner": [["model", 2, 2, "Generic"], ["macro-average F1 performance", 6, 8, "Metric"], ["task", 13, 13, "Generic"], ["syntactic dependencies LAS", 17, 19, "Metric"], ["semantic dependencies F1", 23, 25, "Metric"]], "relations": [[6, 8, 2, 2, "EVALUATE-FOR"], [6, 8, 13, 13, "EVALUATE-FOR"], [17, 19, 13, 13, "EVALUATE-FOR"], [17, 19, 23, 25, "CONJUNCTION"], [23, 25, 13, 13, "EVALUATE-FOR"]]},
{"sentence": "A larger model trained after the deadline achieves 80.5 % macro-average F1 , 87.6 % syntactic dependencies LAS , and 73.1 % semantic dependencies F1 .", "ner": [["model", 2, 2, "Generic"], ["macro-average F1", 10, 11, "Metric"], ["syntactic dependencies LAS", 15, 17, "Metric"], ["semantic dependencies F1", 22, 24, "Metric"]], "relations": [[10, 11, 2, 2, "EVALUATE-FOR"], [10, 11, 15, 17, "CONJUNCTION"], [15, 17, 2, 2, "EVALUATE-FOR"], [15, 17, 22, 24, "CONJUNCTION"], [22, 24, 2, 2, "EVALUATE-FOR"]]},
{"sentence": "This paper describes a new , large scale discourse-level annotation project - the Penn Discourse TreeBank -LRB- PDTB -RRB- .", "ner": [["large scale discourse-level annotation", 6, 9, "Task"], ["Penn Discourse TreeBank -LRB- PDTB -RRB-", 13, 18, "Material"]], "relations": []},
{"sentence": "We present an approach to annotating a level of discourse structure that is based on identifying discourse connectives and their arguments .", "ner": [["approach", 3, 3, "Generic"], ["discourse structure", 9, 10, "OtherScientificTerm"], ["discourse connectives", 16, 17, "OtherScientificTerm"]], "relations": [[3, 3, 9, 10, "USED-FOR"], [16, 17, 3, 3, "USED-FOR"]]},
{"sentence": "The PDTB is being built directly on top of the Penn TreeBank and Propbank , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .", "ner": [["PDTB", 1, 1, "Material"], ["Penn TreeBank", 10, 11, "Material"], ["Propbank", 13, 13, "Material"], ["extraction of useful syntactic and semantic features", 18, 24, "Task"], ["practical algorithms", 36, 37, "Method"]], "relations": [[1, 1, 18, 24, "USED-FOR"], [1, 1, 36, 37, "EVALUATE-FOR"], [10, 11, 1, 1, "USED-FOR"], [10, 11, 13, 13, "CONJUNCTION"], [13, 13, 1, 1, "USED-FOR"]]},
{"sentence": "We provide a detailed preliminary analysis of inter-annotator agreement - both the level of agreement and the types of inter-annotator variation .", "ner": [["inter-annotator agreement", 7, 8, "Metric"], ["level of agreement", 12, 14, "Metric"], ["inter-annotator variation", 19, 20, "OtherScientificTerm"]], "relations": [[12, 14, 7, 8, "FEATURE-OF"], [12, 14, 19, 20, "CONJUNCTION"], [19, 20, 7, 8, "FEATURE-OF"]]},
{"sentence": "Currently , N-gram models are the most common and widely used models for statistical language modeling .", "ner": [["N-gram models", 2, 3, "Method"], ["statistical language modeling", 13, 15, "Task"]], "relations": [[2, 3, 13, 15, "USED-FOR"]]},
{"sentence": "In this paper , we investigated an alternative way to build language models , i.e. , using artificial neural networks to learn the language model .", "ner": [["language models", 11, 12, "Method"], ["artificial neural networks", 17, 19, "Method"], ["language model", 23, 24, "Method"]], "relations": [[17, 19, 23, 24, "USED-FOR"]]},
{"sentence": "Our experiment result shows that the neural network can learn a language model that has performance even better than standard statistical methods .", "ner": [["neural network", 6, 7, "Method"], ["language model", 11, 12, "Method"], ["statistical methods", 20, 21, "Method"]], "relations": [[6, 7, 11, 12, "USED-FOR"], [6, 7, 20, 21, "COMPARE"]]},
{"sentence": "Automated facial expression recognition has received increased attention over the past two decades .", "ner": [["Automated facial expression recognition", 0, 3, "Task"]], "relations": []},
{"sentence": "Existing works in the field usually do not encode either the temporal evolution or the intensity of the observed facial displays .", "ner": [["Existing works", 0, 1, "Generic"], ["temporal evolution", 11, 12, "OtherScientificTerm"], ["intensity of the observed facial displays", 15, 20, "OtherScientificTerm"]], "relations": [[15, 20, 11, 12, "CONJUNCTION"]]},
{"sentence": "They also fail to jointly model multidimensional -LRB- multi-class -RRB- continuous facial behaviour data ; binary classifiers-one for each target basic-emotion class-are used instead .", "ner": [["They", 0, 0, "Generic"], ["multidimensional -LRB- multi-class -RRB- continuous facial behaviour data", 6, 13, "Material"]], "relations": []},
{"sentence": "In this paper , intrinsic topology of multidimensional continuous facial affect data is first modeled by an ordinal man-ifold .", "ner": [["intrinsic topology of multidimensional continuous facial", 4, 9, "OtherScientificTerm"], ["ordinal man-ifold", 17, 18, "OtherScientificTerm"]], "relations": [[17, 18, 4, 9, "USED-FOR"]]},
{"sentence": "This topology is then incorporated into the Hidden Conditional Ordinal Random Field -LRB- H-CORF -RRB- framework for dynamic ordinal regression by constraining H-CORF parameters to lie on the ordinal manifold .", "ner": [["topology", 1, 1, "Generic"], ["Hidden Conditional Ordinal Random Field -LRB- H-CORF -RRB- framework", 7, 15, "Method"], ["dynamic ordinal regression", 17, 19, "Task"], ["H-CORF parameters", 22, 23, "OtherScientificTerm"], ["ordinal manifold", 28, 29, "OtherScientificTerm"]], "relations": [[1, 1, 7, 15, "PART-OF"], [7, 15, 17, 19, "USED-FOR"]]},
{"sentence": "The resulting model attains simultaneous dynamic recognition and intensity estimation of facial expressions of multiple emotions .", "ner": [["model", 2, 2, "Generic"], ["simultaneous dynamic recognition", 4, 6, "Task"], ["intensity estimation of facial expressions", 8, 12, "Task"]], "relations": [[2, 2, 4, 6, "USED-FOR"], [2, 2, 8, 12, "USED-FOR"], [4, 6, 8, 12, "CONJUNCTION"]]},
{"sentence": "To the best of our knowledge , the proposed method is the first one to achieve this on both deliberate as well as spontaneous facial affect data .", "ner": [["the proposed method", 7, 9, "Method"], ["spontaneous facial affect data", 23, 26, "Material"]], "relations": [[23, 26, 7, 9, "EVALUATE-FOR"]]},
{"sentence": "Recent advances in linear classification have shown that for applications such as document classification , the training can be extremely efficient .", "ner": [["linear classification", 3, 4, "Task"], ["applications", 9, 9, "Generic"], ["document classification", 12, 13, "Task"]], "relations": [[12, 13, 9, 9, "HYPONYM-OF"]]},
{"sentence": "However , most of the existing training methods are designed by assuming that data can be stored in the computer memory .", "ner": [["training methods", 6, 7, "Method"], ["computer memory", 19, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "These methods can not be easily applied to data larger than the memory capacity due to the random access to the disk .", "ner": [["methods", 1, 1, "Generic"], ["data", 8, 8, "Generic"], ["memory capacity", 12, 13, "OtherScientificTerm"], ["random access", 17, 18, "OtherScientificTerm"], ["disk", 21, 21, "OtherScientificTerm"]], "relations": [[8, 8, 12, 13, "COMPARE"]]},
{"sentence": "We propose and analyze a block minimization framework for data larger than the memory size .", "ner": [["block minimization framework", 5, 7, "Method"], ["data", 9, 9, "Generic"], ["memory size", 13, 14, "OtherScientificTerm"]], "relations": [[5, 7, 9, 9, "USED-FOR"], [9, 9, 13, 14, "COMPARE"]]},
{"sentence": "At each step a block of data is loaded from the disk and handled by certain learning methods .", "ner": [["disk", 11, 11, "OtherScientificTerm"], ["learning methods", 16, 17, "Method"]], "relations": []},
{"sentence": "We investigate two implementations of the proposed framework for primal and dual SVMs , respectively .", "ner": [["framework", 7, 7, "Generic"], ["primal and dual SVMs", 9, 12, "Method"]], "relations": [[7, 7, 9, 12, "USED-FOR"]]},
{"sentence": "As data can not fit in memory , many design considerations are very different from those for traditional algorithms .", "ner": [], "relations": []},
{"sentence": "Experiments using data sets 20 times larger than the memory demonstrate the effectiveness of the proposed method .", "ner": [["method", 16, 16, "Generic"]], "relations": []},
{"sentence": "Instances of a word drawn from different domains may have different sense priors -LRB- the proportions of the different senses of a word -RRB- .", "ner": [["sense priors", 11, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "This in turn affects the accuracy of word sense disambiguation -LRB- WSD -RRB- systems trained and applied on different domains .", "ner": [["accuracy", 5, 5, "Metric"], ["word sense disambiguation -LRB- WSD -RRB- systems", 7, 13, "Method"]], "relations": [[5, 5, 7, 13, "EVALUATE-FOR"]]},
{"sentence": "This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .", "ner": [["method", 4, 4, "Generic"], ["sense priors of words", 8, 11, "OtherScientificTerm"], ["new domain", 15, 16, "Material"], ["well calibrated probabilities", 24, 26, "OtherScientificTerm"], ["estimations", 30, 30, "Generic"]], "relations": [[4, 4, 8, 11, "USED-FOR"], [15, 16, 8, 11, "FEATURE-OF"], [24, 26, 30, 30, "USED-FOR"]]},
{"sentence": "By using well calibrated probabilities , we are able to estimate the sense priors effectively to achieve significant improvements in WSD accuracy .", "ner": [["well calibrated probabilities", 2, 4, "OtherScientificTerm"], ["sense priors", 12, 13, "OtherScientificTerm"], ["WSD accuracy", 20, 21, "Metric"]], "relations": [[2, 4, 12, 13, "USED-FOR"]]},
{"sentence": "In this paper we deal with a recently developed large Czech MWE database containing at the moment 160 000 MWEs -LRB- treated as lexical units -RRB- .", "ner": [["large Czech MWE database", 9, 12, "Material"], ["MWEs", 19, 19, "OtherScientificTerm"], ["lexical units", 23, 24, "OtherScientificTerm"]], "relations": []},
{"sentence": "It was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .", "ner": [["It", 0, 0, "Generic"], ["encyclopedias", 8, 8, "Material"], ["dictionaries", 10, 10, "Material"], ["public databases of proper names and toponyms", 12, 18, "Material"], ["collocations", 20, 20, "Generic"], ["Czech WordNet", 23, 24, "Material"], ["lists of botanical and zoological terms", 26, 31, "Material"]], "relations": [[8, 8, 0, 0, "USED-FOR"], [8, 8, 10, 10, "CONJUNCTION"], [10, 10, 0, 0, "USED-FOR"], [12, 18, 0, 0, "USED-FOR"], [20, 20, 0, 0, "USED-FOR"], [26, 31, 0, 0, "USED-FOR"], [26, 31, 20, 20, "CONJUNCTION"]]},
{"sentence": "We describe the structure of the database and give basic types of MWEs according to domains they belong to .", "ner": [["database", 6, 6, "Generic"], ["MWEs", 12, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "We compare the built MWEs database with the corpus data from Czech National Corpus -LRB- approx .", "ner": [["MWEs database", 4, 5, "Material"], ["Czech National Corpus", 11, 13, "Material"]], "relations": [[11, 13, 4, 5, "USED-FOR"]]},
{"sentence": "100 mil .", "ner": [], "relations": []},
{"sentence": "tokens -RRB- and present results of this comparison in the paper .", "ner": [], "relations": []},
{"sentence": "These MWEs have not been obtained from the corpus since their frequencies in it are rather low .", "ner": [["MWEs", 1, 1, "OtherScientificTerm"], ["corpus", 8, 8, "Generic"]], "relations": []},
{"sentence": "To obtain a more complete list of MWEs we propose and use a technique exploiting the Word Sketch Engine , which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs .", "ner": [["MWEs", 7, 7, "OtherScientificTerm"], ["technique", 13, 13, "Generic"], ["Word Sketch Engine", 16, 18, "Task"], ["statistical parameters", 26, 27, "OtherScientificTerm"], ["MWEs", 32, 32, "OtherScientificTerm"], ["MWEs", 45, 45, "OtherScientificTerm"]], "relations": [[13, 13, 16, 18, "USED-FOR"], [26, 27, 16, 18, "FEATURE-OF"]]},
{"sentence": "We also discuss exploitation of the database for working out a more adequate tagging and lemmatization .", "ner": [["database", 6, 6, "Generic"], ["tagging", 13, 13, "Task"], ["lemmatization", 15, 15, "Task"]], "relations": [[6, 6, 13, 13, "USED-FOR"], [6, 6, 15, 15, "USED-FOR"], [13, 13, 15, 15, "CONJUNCTION"]]},
{"sentence": "The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units , i. e. to make tagging and lemmatization more adequate .", "ner": [["MWEs", 9, 9, "OtherScientificTerm"], ["them", 15, 15, "Generic"], ["lexical units", 18, 19, "OtherScientificTerm"], ["tagging", 25, 25, "Task"], ["lemmatization", 27, 27, "Task"]], "relations": [[9, 9, 25, 25, "USED-FOR"], [9, 9, 27, 27, "USED-FOR"], [25, 25, 27, 27, "CONJUNCTION"]]},
{"sentence": "We describe the ongoing construction of a large , semantically annotated corpus resource as reliable basis for the large-scale acquisition of word-semantic information , e.g. the construction of domain-independent lexica .", "ner": [["semantically annotated corpus", 9, 11, "Material"], ["large-scale acquisition of word-semantic information", 18, 22, "Task"], ["construction of domain-independent lexica", 26, 29, "Task"]], "relations": [[9, 11, 18, 22, "USED-FOR"], [26, 29, 18, 22, "HYPONYM-OF"]]},
{"sentence": "The backbone of the annotation are semantic roles in the frame semantics paradigm .", "ner": [["annotation", 4, 4, "OtherScientificTerm"], ["semantic roles", 6, 7, "OtherScientificTerm"], ["frame semantics paradigm", 10, 12, "Method"]], "relations": [[6, 7, 10, 12, "PART-OF"]]},
{"sentence": "We report experiences and evaluate the annotated data from the first project stage .", "ner": [["annotated data", 6, 7, "Material"]], "relations": []},
{"sentence": "On this basis , we discuss the problems of vagueness and ambiguity in semantic annotation .", "ner": [["vagueness", 9, 9, "OtherScientificTerm"], ["ambiguity", 11, 11, "OtherScientificTerm"], ["semantic annotation", 13, 14, "Method"]], "relations": [[9, 9, 11, 11, "CONJUNCTION"], [9, 9, 13, 14, "FEATURE-OF"], [11, 11, 13, 14, "FEATURE-OF"]]},
{"sentence": "Statistical machine translation -LRB- SMT -RRB- is currently one of the hot spots in natural language processing .", "ner": [["Statistical machine translation -LRB- SMT -RRB-", 0, 5, "Method"], ["natural language processing", 14, 16, "Task"]], "relations": [[0, 5, 14, 16, "HYPONYM-OF"]]},
{"sentence": "Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that SMT gives competitive results to rule-based translation systems , requiring significantly less development time .", "ner": [["SMT", 21, 21, "Method"], ["rule-based translation systems", 26, 28, "Method"]], "relations": [[21, 21, 26, 28, "COMPARE"]]},
{"sentence": "This is particularly important when building translation systems for new language pairs or new domains .", "ner": [["translation systems", 6, 7, "Task"], ["new language pairs", 9, 11, "OtherScientificTerm"], ["new domains", 13, 14, "OtherScientificTerm"]], "relations": [[6, 7, 9, 11, "USED-FOR"], [6, 7, 13, 14, "USED-FOR"], [9, 11, 13, 14, "CONJUNCTION"]]},
{"sentence": "This workshop is intended to give an introduction to statistical machine translation with a focus on practical considerations .", "ner": [["statistical machine translation", 9, 11, "Method"]], "relations": []},
{"sentence": "Participants should be able , after attending this workshop , to set out building an SMT system themselves and achieving good baseline results in a short time .", "ner": [["SMT system", 15, 16, "Method"]], "relations": []},
{"sentence": "The tutorial will cover the basics of SMT : Theory will be put into practice .", "ner": [["SMT", 7, 7, "Method"]], "relations": []},
{"sentence": "STTK , a statistical machine translation tool kit , will be introduced and used to build a working translation system .", "ner": [["STTK", 0, 0, "Method"], ["statistical machine translation tool kit", 3, 7, "Method"], ["translation system", 18, 19, "Task"]], "relations": [[0, 0, 3, 7, "HYPONYM-OF"], [0, 0, 18, 19, "USED-FOR"]]},
{"sentence": "STTK has been developed by the presenter and co-workers over a number of years and is currently used as the basis of CMU 's SMT system .", "ner": [["STTK", 0, 0, "Method"], ["SMT system", 24, 25, "Method"]], "relations": [[0, 0, 24, 25, "USED-FOR"]]},
{"sentence": "It has also successfully been coupled with rule-based and example based machine translation modules to build a multi engine machine translation system .", "ner": [["It", 0, 0, "Generic"], ["rule-based and example based machine translation modules", 7, 13, "Method"], ["multi engine machine translation system", 17, 21, "Task"]], "relations": [[0, 0, 7, 13, "CONJUNCTION"], [0, 0, 17, 21, "USED-FOR"], [7, 13, 17, 21, "USED-FOR"]]},
{"sentence": "The source code of the tool kit will be made available .", "ner": [["tool kit", 5, 6, "Generic"]], "relations": []},
{"sentence": "This paper presents an unsupervised learning approach to building a non-English -LRB- Arabic -RRB- stemmer .", "ner": [["unsupervised learning approach", 4, 6, "Method"], ["non-English -LRB- Arabic -RRB- stemmer", 10, 14, "Task"]], "relations": [[4, 6, 10, 14, "USED-FOR"]]},
{"sentence": "The stemming model is based on statistical machine translation and it uses an English stemmer and a small -LRB- 10K sentences -RRB- parallel corpus as its sole training resources .", "ner": [["stemming model", 1, 2, "Method"], ["statistical machine translation", 6, 8, "Method"], ["it", 10, 10, "Generic"], ["English stemmer", 13, 14, "Method"], ["parallel corpus", 22, 23, "Material"]], "relations": [[6, 8, 1, 2, "USED-FOR"], [13, 14, 10, 10, "USED-FOR"], [22, 23, 10, 10, "USED-FOR"]]},
{"sentence": "No parallel text is needed after the training phase .", "ner": [["parallel text", 1, 2, "Material"]], "relations": []},
{"sentence": "Monolingual , unannotated text can be used to further improve the stemmer by allowing it to adapt to a desired domain or genre .", "ner": [["Monolingual , unannotated text", 0, 3, "Material"], ["stemmer", 11, 11, "Method"], ["it", 14, 14, "Generic"]], "relations": [[0, 3, 11, 11, "USED-FOR"]]},
{"sentence": "Examples and results will be given for Arabic , but the approach is applicable to any language that needs affix removal .", "ner": [["Arabic", 7, 7, "Material"], ["approach", 11, 11, "Generic"], ["affix removal", 19, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component .", "ner": [["resource-frugal approach", 1, 2, "Method"], ["agreement", 7, 7, "Metric"], ["Arabic stemmer", 16, 17, "Method"], ["rules", 20, 20, "OtherScientificTerm"], ["affix lists", 22, 23, "Material"], ["human annotated text", 26, 28, "Material"], ["unsupervised component", 34, 35, "Method"]], "relations": [[1, 2, 16, 17, "COMPARE"], [7, 7, 1, 2, "EVALUATE-FOR"], [7, 7, 16, 17, "EVALUATE-FOR"], [20, 20, 16, 17, "USED-FOR"], [20, 20, 22, 23, "CONJUNCTION"], [22, 23, 16, 17, "USED-FOR"], [22, 23, 26, 28, "CONJUNCTION"], [26, 28, 16, 17, "USED-FOR"], [26, 28, 34, 35, "CONJUNCTION"], [34, 35, 16, 17, "USED-FOR"]]},
{"sentence": "Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38 % in average precision over unstemmed text , and 96 % of the performance of the proprietary stemmer above .", "ner": [["Task-based evaluation", 0, 1, "Method"], ["Arabic information retrieval", 3, 5, "Task"], ["average precision", 13, 14, "Metric"], ["unstemmed text", 16, 17, "Material"], ["stemmer", 28, 28, "Method"]], "relations": [[3, 5, 0, 1, "USED-FOR"], [13, 14, 0, 1, "EVALUATE-FOR"], [13, 14, 16, 17, "EVALUATE-FOR"]]},
{"sentence": "The paper assesses the capability of an HMM-based TTS system to produce German speech .", "ner": [["HMM-based TTS system", 7, 9, "Method"], ["German speech", 12, 13, "Material"]], "relations": [[7, 9, 12, 13, "USED-FOR"]]},
{"sentence": "The results are discussed in qualitative terms , and compared over three different choices of context features .", "ner": [["context features", 15, 16, "OtherScientificTerm"]], "relations": []},
{"sentence": "In addition , the system is adapted to a small set of football announcements , in an exploratory attempt to synthe-sise expressive speech .", "ner": [["system", 4, 4, "Generic"], ["football announcements", 12, 13, "Material"], ["expressive speech", 21, 22, "Material"]], "relations": [[4, 4, 12, 13, "USED-FOR"], [4, 4, 21, 22, "USED-FOR"]]},
{"sentence": "We conclude that the HMMs are able to produce highly intelligible neutral German speech , with a stable quality , and that the expressivity is partially captured in spite of the small size of the football dataset .", "ner": [["HMMs", 4, 4, "Method"], ["intelligible neutral German speech", 10, 13, "Material"], ["football dataset", 35, 36, "Material"]], "relations": [[4, 4, 10, 13, "USED-FOR"]]},
{"sentence": "We give an analysis of ellipsis resolution in terms of a straightforward discourse copying algorithm that correctly predicts a wide range of phenomena .", "ner": [["analysis of ellipsis resolution", 3, 6, "Task"], ["discourse copying algorithm", 12, 14, "Method"]], "relations": []},
{"sentence": "The treatment does not suffer from problems inherent in identity-of-relations analyses .", "ner": [["treatment", 1, 1, "Generic"], ["identity-of-relations analyses", 9, 10, "Task"]], "relations": []},
{"sentence": "Furthermore , in contrast to the approach of Dalrymple et al. -LSB- 1991 -RSB- , the treatment directly encodes the intuitive distinction between full NPs and the referential elements that corefer with them through what we term role linking .", "ner": [["treatment", 16, 16, "Generic"], ["full NPs", 23, 24, "OtherScientificTerm"], ["referential elements", 27, 28, "OtherScientificTerm"], ["role linking", 37, 38, "Task"]], "relations": [[23, 24, 27, 28, "CONJUNCTION"]]},
{"sentence": "The correct predictions for several problematic examples of ellipsis naturally result .", "ner": [["ellipsis", 8, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "Finally , the analysis extends directly to other discourse copying phenomena .", "ner": [["analysis", 3, 3, "Generic"], ["discourse copying phenomena", 8, 10, "Task"]], "relations": [[3, 3, 8, 10, "USED-FOR"]]},
{"sentence": "How to obtain hierarchical relations -LRB- e.g. superordinate - hyponym relation , synonym relation -RRB- is one of the most important problems for thesaurus construction .", "ner": [["hierarchical relations", 3, 4, "OtherScientificTerm"], ["superordinate - hyponym relation", 7, 10, "OtherScientificTerm"], ["synonym relation", 12, 13, "OtherScientificTerm"], ["thesaurus construction", 23, 24, "Task"]], "relations": [[3, 4, 23, 24, "PART-OF"], [7, 10, 3, 4, "HYPONYM-OF"], [7, 10, 12, 13, "CONJUNCTION"], [12, 13, 3, 4, "HYPONYM-OF"]]},
{"sentence": "A pilot system for extracting these relations automatically from an ordinary Japanese language dictionary -LRB- Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form -RRB- is given .", "ner": [["relations", 6, 6, "Generic"], ["Japanese language dictionary", 11, 13, "Material"]], "relations": [[11, 13, 6, 6, "USED-FOR"]]},
{"sentence": "The features of the definition sentences in the dictionary , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .", "ner": [["features", 1, 1, "OtherScientificTerm"], ["definition sentences", 4, 5, "OtherScientificTerm"], ["dictionary", 8, 8, "Generic"], ["hierarchical relations", 15, 16, "OtherScientificTerm"]], "relations": [[4, 5, 1, 1, "USED-FOR"], [4, 5, 8, 8, "PART-OF"]]},
{"sentence": "Face images of non-frontal views under poor illumination with low resolution reduce dramatically face recognition accuracy .", "ner": [["Face images of non-frontal views", 0, 4, "Task"], ["face recognition accuracy", 13, 15, "Metric"]], "relations": []},
{"sentence": "This is evident most compellingly by the very low recognition rate of all existing face recognition systems when applied to live CCTV camera input .", "ner": [["recognition rate", 9, 10, "Metric"], ["face recognition systems", 14, 16, "Method"], ["live CCTV camera input", 20, 23, "Material"]], "relations": [[9, 10, 14, 16, "EVALUATE-FOR"], [20, 23, 14, 16, "USED-FOR"]]},
{"sentence": "In this paper , we present a Bayesian framework to perform multi-modal -LRB- such as variations in viewpoint and illumination -RRB- face image super-resolution for recognition in tensor space .", "ner": [["Bayesian framework", 7, 8, "Method"], ["viewpoint", 17, 17, "OtherScientificTerm"], ["illumination", 19, 19, "OtherScientificTerm"], ["face image super-resolution", 21, 23, "Task"], ["recognition", 25, 25, "Task"], ["tensor space", 27, 28, "OtherScientificTerm"]], "relations": [[7, 8, 21, 23, "USED-FOR"], [17, 17, 19, 19, "CONJUNCTION"], [21, 23, 25, 25, "USED-FOR"], [27, 28, 25, 25, "FEATURE-OF"]]},
{"sentence": "Given a single modal low-resolution face image , we benefit from the multiple factor interactions of training tensor , and super-resolve its high-resolution reconstructions across different modalities for face recognition .", "ner": [["single modal low-resolution face image", 2, 6, "Material"], ["multiple factor interactions of training tensor", 12, 17, "OtherScientificTerm"], ["high-resolution reconstructions", 22, 23, "Task"], ["modalities", 26, 26, "OtherScientificTerm"], ["face recognition", 28, 29, "Task"]], "relations": [[2, 6, 22, 23, "USED-FOR"], [12, 17, 22, 23, "USED-FOR"], [22, 23, 28, 29, "USED-FOR"], [26, 26, 22, 23, "FEATURE-OF"]]},
{"sentence": "Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and recognition by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .", "ner": [["pixel-domain super-resolution and recognition", 3, 6, "Task"], ["super-resolution", 19, 19, "Task"], ["recognition", 21, 21, "Task"], ["maximum likelihood identity parameter vector", 26, 30, "OtherScientificTerm"], ["high-resolution tensor space", 32, 34, "OtherScientificTerm"], ["recognition", 36, 36, "Task"]], "relations": [[19, 19, 3, 6, "HYPONYM-OF"], [19, 19, 21, 21, "CONJUNCTION"], [21, 21, 3, 6, "HYPONYM-OF"], [26, 30, 19, 19, "USED-FOR"], [26, 30, 21, 21, "USED-FOR"], [26, 30, 36, 36, "USED-FOR"], [32, 34, 26, 30, "FEATURE-OF"]]},
{"sentence": "We show results from multi-modal super-resolution and face recognition experiments across different imaging modalities , using low-resolution images as testing inputs and demonstrate improved recognition rates over standard tensorface and eigenface representations .", "ner": [["multi-modal super-resolution and face recognition", 4, 8, "Task"], ["imaging modalities", 12, 13, "OtherScientificTerm"], ["low-resolution images", 16, 17, "Material"], ["recognition rates", 24, 25, "Metric"], ["tensorface and eigenface representations", 28, 31, "Method"]], "relations": [[16, 17, 4, 8, "USED-FOR"], [24, 25, 4, 8, "EVALUATE-FOR"], [24, 25, 28, 31, "EVALUATE-FOR"]]},
{"sentence": "In this paper , we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models .", "ner": [["phrase-based unigram model", 7, 9, "Method"], ["statistical machine translation", 11, 13, "Task"], ["model parameters", 21, 22, "OtherScientificTerm"], ["phrase-based models", 25, 26, "Method"]], "relations": [[7, 9, 11, 13, "USED-FOR"], [7, 9, 25, 26, "COMPARE"], [21, 22, 7, 9, "USED-FOR"]]},
{"sentence": "The units of translation are blocks - pairs of phrases .", "ner": [["blocks", 5, 5, "OtherScientificTerm"]], "relations": []},
{"sentence": "During decoding , we use a block unigram model and a word-based trigram language model .", "ner": [["decoding", 1, 1, "Task"], ["block unigram model", 6, 8, "Method"], ["word-based trigram language model", 11, 14, "Method"]], "relations": [[6, 8, 1, 1, "USED-FOR"], [11, 14, 1, 1, "USED-FOR"], [11, 14, 6, 8, "CONJUNCTION"]]},
{"sentence": "During training , the blocks are learned from source interval projections using an underlying word alignment .", "ner": [["training", 1, 1, "Task"], ["blocks", 4, 4, "OtherScientificTerm"], ["source interval projections", 8, 10, "Method"], ["word alignment", 14, 15, "Method"]], "relations": [[8, 10, 4, 4, "USED-FOR"], [14, 15, 8, 10, "USED-FOR"]]},
{"sentence": "We show experimental results on block selection criteria based on unigram counts and phrase length .", "ner": [["block selection criteria", 5, 7, "Metric"], ["unigram counts", 10, 11, "OtherScientificTerm"], ["phrase length", 13, 14, "OtherScientificTerm"]], "relations": [[10, 11, 5, 7, "USED-FOR"], [10, 11, 13, 14, "CONJUNCTION"], [13, 14, 5, 7, "USED-FOR"]]},
{"sentence": "This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed -LRB- e.g. face detection or database retrieval -RRB- .", "ner": [["approach", 5, 5, "Generic"], ["fast detection", 8, 9, "Task"], ["distribution", 14, 14, "OtherScientificTerm"], ["positive and negative examples", 16, 19, "Material"], ["face detection", 25, 26, "Task"], ["database retrieval", 28, 29, "Task"]], "relations": [[5, 5, 8, 9, "USED-FOR"], [25, 26, 28, 29, "CONJUNCTION"]]},
{"sentence": "In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .", "ner": [["cascade of simple classifiers", 4, 7, "Method"], ["classifiers", 7, 7, "Method"], ["detection rates", 13, 14, "Metric"], ["modest false positive rates", 16, 19, "Metric"], ["detector", 24, 24, "Generic"], ["features", 28, 28, "OtherScientificTerm"], ["detection rates", 32, 33, "Metric"], ["false positive rates", 37, 39, "Metric"], ["fast performance", 42, 43, "Metric"]], "relations": [[4, 7, 24, 24, "USED-FOR"], [13, 14, 7, 7, "EVALUATE-FOR"], [13, 14, 16, 19, "CONJUNCTION"], [16, 19, 7, 7, "EVALUATE-FOR"], [28, 28, 24, 24, "FEATURE-OF"]]},
{"sentence": "Achieving extremely high detection rates , rather than low error , is not a task typically addressed by machine learning algorithms .", "ner": [["detection rates", 3, 4, "Metric"], ["low error", 8, 9, "Metric"], ["machine learning algorithms", 18, 20, "Method"]], "relations": [[3, 4, 8, 9, "COMPARE"]]},
{"sentence": "We propose a new variant of AdaBoost as a mechanism for training the simple classifiers used in the cascade .", "ner": [["AdaBoost", 6, 6, "Method"], ["classifiers", 14, 14, "OtherScientificTerm"], ["cascade", 18, 18, "Generic"]], "relations": [[6, 6, 14, 14, "USED-FOR"], [14, 14, 18, 18, "USED-FOR"]]},
{"sentence": "Experimental results in the domain of face detection show the training algorithm yields significant improvements in performance over conventional AdaBoost .", "ner": [["face detection", 6, 7, "Task"], ["training algorithm", 10, 11, "Method"], ["AdaBoost", 19, 19, "Method"]], "relations": [[10, 11, 6, 7, "USED-FOR"], [19, 19, 6, 7, "USED-FOR"], [19, 19, 10, 11, "COMPARE"]]},
{"sentence": "The final face detection system can process 15 frames per second , achieves over 90 % detection , and a false positive rate of 1 in a 1,000,000 .", "ner": [["face detection system", 2, 4, "Method"], ["detection", 16, 16, "Metric"], ["false positive rate", 20, 22, "Metric"]], "relations": [[16, 16, 20, 22, "CONJUNCTION"]]},
{"sentence": "This paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities .", "ner": [["method", 4, 4, "Generic"], ["joint embed-dings of images and text", 7, 12, "OtherScientificTerm"], ["two-branch neural network", 15, 17, "Method"], ["multiple layers of linear projections", 19, 23, "OtherScientificTerm"], ["nonlinearities", 26, 26, "OtherScientificTerm"]], "relations": [[4, 4, 7, 12, "USED-FOR"], [15, 17, 4, 4, "USED-FOR"], [19, 23, 15, 17, "PART-OF"], [19, 23, 26, 26, "CONJUNCTION"], [26, 26, 15, 17, "PART-OF"]]},
{"sentence": "The network is trained using a large-margin objective that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature .", "ner": [["network", 1, 1, "Generic"], ["large-margin objective", 6, 7, "OtherScientificTerm"], ["cross-view ranking constraints", 10, 12, "OtherScientificTerm"], ["within-view neighborhood structure preservation constraints", 14, 18, "OtherScientificTerm"], ["metric learning literature", 21, 23, "OtherScientificTerm"]], "relations": [[6, 7, 1, 1, "USED-FOR"], [10, 12, 6, 7, "FEATURE-OF"], [10, 12, 14, 18, "CONJUNCTION"], [14, 18, 6, 7, "FEATURE-OF"]]},
{"sentence": "Extensive experiments show that our approach gains significant improvements in accuracy for image-to-text and text-to-image retrieval .", "ner": [["approach", 5, 5, "Generic"], ["accuracy", 10, 10, "Metric"], ["image-to-text and text-to-image retrieval", 12, 15, "Task"]], "relations": [[10, 10, 5, 5, "EVALUATE-FOR"], [12, 15, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of phrase lo-calization on the Flickr30K Entities dataset .", "ner": [["method", 1, 1, "Generic"], ["Flickr30K and MSCOCO image-sentence datasets", 8, 12, "Material"], ["phrase lo-calization", 21, 22, "Task"], ["Flickr30K Entities dataset", 25, 27, "Material"]], "relations": [[8, 12, 1, 1, "EVALUATE-FOR"], [21, 22, 1, 1, "EVALUATE-FOR"], [25, 27, 21, 22, "USED-FOR"]]},
{"sentence": "State-of-the-art Question Answering -LRB- QA -RRB- systems are very sensitive to variations in the phrasing of an information need .", "ner": [["Question Answering -LRB- QA -RRB- systems", 1, 6, "Method"]], "relations": []},
{"sentence": "Finding the preferred language for such a need is a valuable task .", "ner": [], "relations": []},
{"sentence": "We investigate that claim by adopting a simple MT-based paraphrasing technique and evaluating QA system performance on paraphrased questions .", "ner": [["MT-based paraphrasing technique", 8, 10, "Method"], ["QA system", 13, 14, "Method"], ["paraphrased questions", 17, 18, "Material"]], "relations": [[8, 10, 13, 14, "USED-FOR"], [17, 18, 13, 14, "EVALUATE-FOR"]]},
{"sentence": "We found a potential increase of 35 % in MRR with respect to the original question .", "ner": [["MRR", 9, 9, "Metric"]], "relations": []},
{"sentence": "The TAP-XL Automated Analyst 's Assistant is an application designed to help an English - speaking analyst write a topical report , culling information from a large inflow of multilingual , multimedia data .", "ner": [["TAP-XL Automated Analyst 's Assistant", 1, 5, "Task"], ["multilingual , multimedia data", 29, 32, "Material"]], "relations": [[29, 32, 1, 5, "USED-FOR"]]},
{"sentence": "It gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other languages by leveraging human language technology .", "ner": [["It", 0, 0, "Generic"], ["human language technology", 27, 29, "Method"]], "relations": [[27, 29, 0, 0, "USED-FOR"]]},
{"sentence": "This paper discusses the application of Unification Categorial Grammar -LRB- UCG -RRB- to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen .", "ner": [["Unification Categorial Grammar -LRB- UCG -RRB-", 6, 11, "Method"], ["Isomorphic Grammars", 16, 17, "Method"], ["Machine Translation", 19, 20, "Task"]], "relations": [[6, 11, 16, 17, "USED-FOR"], [16, 17, 19, 20, "USED-FOR"]]},
{"sentence": "The Isomorphic Grammars approach to MT involves developing the grammars of the Source and Target languages in parallel , in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .", "ner": [["Isomorphic Grammars approach", 1, 3, "Method"], ["MT", 5, 5, "Task"], ["translation relation", 32, 33, "OtherScientificTerm"], ["isomorphic derivations", 35, 36, "OtherScientificTerm"]], "relations": [[1, 3, 5, 5, "USED-FOR"]]},
{"sentence": "The principle advantage of this approach is that knowledge concerning translation equivalence of expressions may be directly exploited , obviating the need for answers to semantic questions that we do not yet have .", "ner": [], "relations": []},
{"sentence": "Semantic and other information may still be incorporated , but as constraints on the translation relation , not as levels of textual representation .", "ner": [["translation relation", 14, 15, "OtherScientificTerm"], ["textual representation", 21, 22, "Method"]], "relations": []},
{"sentence": "After introducing this approach to MT system design , and the basics of monolingual UCG , we will show how the two can be integrated , and present an example from an implemented bi-directional English-Spanish fragment .", "ner": [["approach", 3, 3, "Generic"], ["MT system design", 5, 7, "Task"], ["monolingual UCG", 13, 14, "Task"], ["two", 21, 21, "Generic"], ["bi-directional English-Spanish fragment", 33, 35, "OtherScientificTerm"]], "relations": [[3, 3, 5, 7, "USED-FOR"], [3, 3, 13, 14, "USED-FOR"], [5, 7, 13, 14, "CONJUNCTION"], [5, 7, 21, 21, "HYPONYM-OF"], [13, 14, 21, 21, "HYPONYM-OF"]]},
{"sentence": "Finally we will present some outstanding problems with the approach .", "ner": [["approach", 9, 9, "Generic"]], "relations": []},
{"sentence": "In the security domain a key problem is identifying rare behaviours of interest .", "ner": [["security domain", 2, 3, "Task"], ["identifying rare behaviours of interest", 8, 12, "Task"], ["rare behaviours of interest", 9, 12, "OtherScientificTerm"]], "relations": [[8, 12, 2, 3, "PART-OF"]]},
{"sentence": "Training examples for these behaviours may or may not exist , and if they do exist there will be few examples , quite probably one .", "ner": [["Training examples", 0, 1, "Material"], ["behaviours", 4, 4, "Generic"], ["they", 13, 13, "Generic"]], "relations": [[0, 1, 4, 4, "USED-FOR"]]},
{"sentence": "We present a novel weakly supervised algorithm that can detect behaviours that either have never before been seen or for which there are few examples .", "ner": [["weakly supervised algorithm", 4, 6, "Method"], ["behaviours", 10, 10, "OtherScientificTerm"]], "relations": [[4, 6, 10, 10, "USED-FOR"]]},
{"sentence": "Global context is modelled , allowing the detection of abnormal behaviours that in isolation appear normal .", "ner": [["Global context", 0, 1, "OtherScientificTerm"], ["detection of abnormal behaviours", 7, 10, "Task"]], "relations": [[0, 1, 7, 10, "USED-FOR"]]},
{"sentence": "Pragmatic aspects are considered , such that no parameter tuning is required and real time performance is achieved .", "ner": [["Pragmatic aspects", 0, 1, "OtherScientificTerm"], ["parameter tuning", 8, 9, "Method"]], "relations": []},
{"sentence": "We have developed a computational model of the process of describing the layout of an apartment or house , a much-studied discourse task first characterized linguistically by Linde -LRB- 1974 -RRB- .", "ner": [["computational model", 4, 5, "Generic"], ["discourse task", 21, 22, "Task"]], "relations": [[4, 5, 21, 22, "USED-FOR"]]},
{"sentence": "The model is embodied in a program , APT , that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .", "ner": [["model", 1, 1, "Generic"], ["program", 6, 6, "Generic"], ["APT", 8, 8, "Method"], ["organizational and discourse strategies", 20, 23, "Method"]], "relations": [[1, 1, 6, 6, "PART-OF"], [20, 23, 8, 8, "USED-FOR"]]},
{"sentence": "This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification .", "ner": [["approach", 5, 5, "Generic"], ["n-gram models", 7, 8, "Method"], ["error-correction rules", 10, 11, "Method"], ["Thai key prediction", 13, 15, "Task"], ["Thai-English language identification", 17, 19, "Task"]], "relations": [[5, 5, 13, 15, "USED-FOR"], [5, 5, 17, 19, "USED-FOR"], [7, 8, 5, 5, "USED-FOR"], [7, 8, 10, 11, "CONJUNCTION"], [10, 11, 5, 5, "USED-FOR"], [13, 15, 17, 19, "CONJUNCTION"]]},
{"sentence": "The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules .", "ner": [["rule-reduction algorithm", 4, 5, "Method"], ["mutual information", 7, 8, "OtherScientificTerm"], ["error-correction rules", 12, 13, "OtherScientificTerm"]], "relations": [[7, 8, 4, 5, "USED-FOR"], [7, 8, 12, 13, "USED-FOR"]]},
{"sentence": "Our algorithm reported more than 99 % accuracy in both language identification and key prediction .", "ner": [["algorithm", 1, 1, "Generic"], ["accuracy", 7, 7, "Metric"], ["language identification", 10, 11, "Task"], ["key prediction", 13, 14, "Task"]], "relations": [[1, 1, 10, 11, "USED-FOR"], [1, 1, 13, 14, "USED-FOR"], [7, 7, 1, 1, "EVALUATE-FOR"]]},
{"sentence": "This paper concerns the discourse understanding process in spoken dialogue systems .", "ner": [["discourse understanding process", 4, 6, "Task"], ["spoken dialogue systems", 8, 10, "Method"]], "relations": [[4, 6, 8, 10, "USED-FOR"]]},
{"sentence": "This process enables the system to understand user utterances based on the context of a dialogue .", "ner": [["system", 4, 4, "Generic"], ["user utterances", 7, 8, "Material"]], "relations": [[4, 4, 7, 8, "USED-FOR"]]},
{"sentence": "Since multiple candidates for the understanding result can be obtained for a user utterance due to the ambiguity of speech understanding , it is not appropriate to decide on a single understanding result after each user utterance .", "ner": [["user utterance", 12, 13, "Material"], ["ambiguity of speech understanding", 17, 20, "OtherScientificTerm"], ["user utterance", 35, 36, "OtherScientificTerm"]], "relations": []},
{"sentence": "By holding multiple candidates for understanding results and resolving the ambiguity as the dialogue progresses , the discourse understanding accuracy can be improved .", "ner": [["ambiguity", 10, 10, "Generic"], ["discourse understanding accuracy", 17, 19, "Metric"]], "relations": []},
{"sentence": "This paper proposes a method for resolving this ambiguity based on statistical information obtained from dialogue corpora .", "ner": [["method", 4, 4, "Generic"], ["ambiguity", 8, 8, "Generic"], ["statistical information", 11, 12, "OtherScientificTerm"], ["dialogue corpora", 15, 16, "Material"]], "relations": [[4, 4, 8, 8, "USED-FOR"], [11, 12, 4, 4, "USED-FOR"], [15, 16, 11, 12, "USED-FOR"]]},
{"sentence": "Unlike conventional methods that use hand-crafted rules , the proposed method enables easy design of the discourse understanding process .", "ner": [["methods", 2, 2, "Generic"], ["hand-crafted rules", 5, 6, "OtherScientificTerm"], ["method", 10, 10, "Generic"], ["discourse understanding process", 16, 18, "Task"]], "relations": [[5, 6, 2, 2, "USED-FOR"]]},
{"sentence": "Experiment results have shown that a system that exploits the proposed method performs sufficiently and that holding multiple candidates for understanding results is effective .", "ner": [["system", 6, 6, "Generic"], ["method", 11, 11, "Generic"]], "relations": [[6, 6, 11, 11, "USED-FOR"]]},
{"sentence": "We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time .", "ner": [["question-focused sentence retrieval", 5, 7, "Task"], ["news articles", 10, 11, "Material"], ["multi-event stories", 13, 14, "Material"]], "relations": [[10, 11, 5, 7, "USED-FOR"], [13, 14, 10, 11, "FEATURE-OF"]]},
{"sentence": "Annotators generated a list of questions central to understanding each story in our corpus .", "ner": [], "relations": []},
{"sentence": "Because of the dynamic nature of the stories , many questions are time-sensitive -LRB- e.g. `` How many victims have been found ? '' -RRB-", "ner": [], "relations": []},
{"sentence": ".", "ner": [], "relations": []},
{"sentence": "Judges found sentences providing an answer to each question .", "ner": [], "relations": []},
{"sentence": "To address the sentence retrieval problem , we apply a stochastic , graph-based method for comparing the relative importance of the textual units , which was previously used successfully for generic summarization .", "ner": [["sentence retrieval problem", 3, 5, "Task"], ["stochastic , graph-based method", 10, 13, "Method"], ["generic summarization", 30, 31, "Task"]], "relations": [[10, 13, 3, 5, "USED-FOR"], [10, 13, 30, 31, "USED-FOR"]]},
{"sentence": "Currently , we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline , which compares the similarity of each sentence to the input question via IDF-weighted word overlap .", "ner": [["method", 9, 9, "Generic"], ["it", 13, 13, "Generic"], ["baseline", 18, 18, "Generic"], ["IDF-weighted word overlap", 32, 34, "OtherScientificTerm"]], "relations": [[18, 18, 13, 13, "COMPARE"]]},
{"sentence": "In our experiments , the method achieves a TRDR score that is significantly higher than that of the baseline .", "ner": [["method", 5, 5, "Generic"], ["TRDR score", 8, 9, "Metric"], ["baseline", 18, 18, "Generic"]], "relations": [[5, 5, 18, 18, "COMPARE"], [8, 9, 5, 5, "EVALUATE-FOR"], [8, 9, 18, 18, "EVALUATE-FOR"]]},
{"sentence": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .", "ner": [["sentence analysis", 4, 5, "Task"], ["defeasible reasoning", 10, 11, "Method"], ["treatment", 17, 17, "Generic"], ["Japanese sentence analyses", 19, 21, "Task"], ["argumentation system", 24, 25, "Method"], ["formalization of defeasible reasoning", 32, 35, "Method"], ["defeasible reasoning", 34, 35, "Method"], ["arguments", 39, 39, "OtherScientificTerm"], ["defeat rules", 41, 42, "OtherScientificTerm"], ["defeasibility", 45, 45, "OtherScientificTerm"]], "relations": [[10, 11, 4, 5, "USED-FOR"], [17, 17, 19, 21, "USED-FOR"], [24, 25, 19, 21, "USED-FOR"], [39, 39, 32, 35, "PART-OF"], [39, 39, 41, 42, "CONJUNCTION"], [41, 42, 32, 35, "PART-OF"], [45, 45, 39, 39, "FEATURE-OF"], [45, 45, 41, 42, "FEATURE-OF"]]},
{"sentence": "This paper presents a research on the Czech talking head system .", "ner": [["Czech talking head system", 7, 10, "Task"]], "relations": []},
{"sentence": "It gives an overview of methods used for visual speech animation , parameterization of a human face and a tongue , necessary data sources and a synthesis method .", "ner": [["methods", 5, 5, "Generic"], ["visual speech animation", 8, 10, "Task"], ["synthesis method", 26, 27, "Method"]], "relations": [[5, 5, 8, 10, "USED-FOR"]]},
{"sentence": "A 3D animation model is used for a pseudo-muscular animation schema to create such animation of visual speech which is usable for a lipreading .", "ner": [["3D animation model", 1, 3, "Method"], ["pseudo-muscular animation schema", 8, 10, "Method"], ["animation of visual speech", 14, 17, "Task"], ["lipreading", 23, 23, "Task"]], "relations": [[1, 3, 8, 10, "USED-FOR"], [8, 10, 14, 17, "USED-FOR"], [14, 17, 23, 23, "USED-FOR"]]},
{"sentence": "An extension of animation schema is presented to reach more precise deformations mainly in a lip area .", "ner": [["animation schema", 3, 4, "Method"]], "relations": []},
{"sentence": "Furthermore , a problem of forming articulatory trajectories is formulated to solve labial coarticulation effects .", "ner": [["forming articulatory trajectories", 5, 7, "Task"], ["labial coarticulation effects", 12, 14, "OtherScientificTerm"]], "relations": [[5, 7, 12, 14, "USED-FOR"]]},
{"sentence": "It is used for the synthesis method based on a selection of articulatory targets and interpolation technique .", "ner": [["It", 0, 0, "Generic"], ["synthesis method", 5, 6, "Method"], ["selection of articulatory targets", 10, 13, "OtherScientificTerm"], ["interpolation technique", 15, 16, "Method"]], "relations": [[0, 0, 5, 6, "USED-FOR"], [10, 13, 0, 0, "USED-FOR"], [10, 13, 15, 16, "CONJUNCTION"], [15, 16, 0, 0, "USED-FOR"]]},
{"sentence": "It is often assumed that when natural language processing meets the real world , the ideal of aiming for complete and correct interpretations has to be abandoned .", "ner": [["natural language processing", 6, 8, "Task"]], "relations": []},
{"sentence": "However , our experience with TACITUS ; especially in the MUC-3 evaluation , has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with methods for achieving robustness .", "ner": [["TACITUS", 5, 5, "Method"], ["MUC-3 evaluation", 10, 11, "Metric"], ["techniques", 17, 17, "Generic"], ["syntactic and pragmatic analysis", 19, 22, "Task"], ["methods", 27, 27, "Generic"], ["robustness", 30, 30, "Metric"]], "relations": [[17, 17, 19, 22, "USED-FOR"], [30, 30, 27, 27, "EVALUATE-FOR"]]},
{"sentence": "We describe three techniques for making syntactic analysis more robust -- an agenda-based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing .", "ner": [["three techniques", 2, 3, "Generic"], ["syntactic analysis", 6, 7, "Task"], ["agenda-based scheduling parser", 12, 14, "Method"], ["recovery technique", 17, 18, "Method"], ["failed parses", 20, 21, "OtherScientificTerm"], ["technique", 26, 26, "Generic"], ["terminal substring parsing", 28, 30, "Method"]], "relations": [[2, 3, 6, 7, "USED-FOR"], [12, 14, 2, 3, "HYPONYM-OF"], [12, 14, 17, 18, "CONJUNCTION"], [17, 18, 2, 3, "HYPONYM-OF"], [17, 18, 6, 7, "USED-FOR"], [17, 18, 20, 21, "USED-FOR"], [17, 18, 26, 26, "CONJUNCTION"], [26, 26, 2, 3, "HYPONYM-OF"]]},
{"sentence": "For pragmatics processing , we describe how the method of abductive inference is inherently robust , in that an interpretation is always possible , so that in the absence of the required world knowledge , performance degrades gracefully .", "ner": [["pragmatics processing", 1, 2, "Task"], ["abductive inference", 10, 11, "Method"], ["world knowledge", 32, 33, "OtherScientificTerm"]], "relations": [[10, 11, 1, 2, "USED-FOR"]]},
{"sentence": "Each of these techniques have been evaluated and the results of the evaluations are presented .", "ner": [["these techniques", 2, 3, "Generic"]], "relations": []},
{"sentence": "This paper proposes a Hidden Markov Model -LRB- HMM -RRB- and an HMM-based chunk tagger , from which a named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system is built to recognize and classify names , times and numerical quantities .", "ner": [["Hidden Markov Model -LRB- HMM -RRB-", 4, 9, "Method"], ["HMM-based chunk tagger", 12, 14, "Method"], ["named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system", 19, 28, "Method"], ["names", 35, 35, "OtherScientificTerm"], ["times and numerical quantities", 37, 40, "OtherScientificTerm"]], "relations": [[4, 9, 12, 14, "CONJUNCTION"], [4, 9, 19, 28, "USED-FOR"], [12, 14, 19, 28, "USED-FOR"], [19, 28, 35, 35, "USED-FOR"], [19, 28, 37, 40, "USED-FOR"], [35, 35, 37, 40, "CONJUNCTION"]]},
{"sentence": "Through the HMM , our system is able to apply and integrate four types of internal and external evidences : 1 -RRB- simple deterministic internal feature of the words , such as capitalization and digitalization ; 2 -RRB- internal semantic feature of important triggers ; 3 -RRB- internal gazetteer feature ; 4 -RRB- external macro context feature .", "ner": [["HMM", 2, 2, "Method"], ["deterministic internal feature of the words", 23, 28, "OtherScientificTerm"], ["capitalization", 32, 32, "OtherScientificTerm"], ["digitalization", 34, 34, "OtherScientificTerm"], ["internal gazetteer feature", 47, 49, "OtherScientificTerm"], ["external macro context feature", 53, 56, "OtherScientificTerm"]], "relations": [[32, 32, 23, 28, "HYPONYM-OF"], [32, 32, 34, 34, "CONJUNCTION"], [34, 34, 23, 28, "HYPONYM-OF"]]},
{"sentence": "In this way , the NER problem can be resolved effectively .", "ner": [["NER problem", 5, 6, "Task"]], "relations": []},
{"sentence": "Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6 % and 94.1 % respectively .", "ner": [["system", 3, 3, "Generic"], ["MUC-6 and MUC-7 English NE tasks", 5, 10, "Material"], ["F-measures", 12, 12, "Metric"]], "relations": [[5, 10, 3, 3, "EVALUATE-FOR"], [12, 12, 3, 3, "EVALUATE-FOR"]]},
{"sentence": "It shows that the performance is significantly better than reported by any other machine-learning system .", "ner": [["machine-learning system", 13, 14, "Method"]], "relations": []},
{"sentence": "Moreover , the performance is even consistently better than those based on handcrafted rules .", "ner": [["handcrafted rules", 12, 13, "OtherScientificTerm"]], "relations": []},
{"sentence": "Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications .", "ner": [["themes", 1, 1, "Generic"], ["speech and text image processing", 5, 9, "Task"], ["recognition technology", 21, 22, "Method"], ["document-oriented applications", 24, 25, "Task"]], "relations": [[1, 1, 5, 9, "PART-OF"], [21, 22, 24, 25, "USED-FOR"]]},
{"sentence": "One is the development of systems that provide functionality similar to that of text processors but operate directly on audio and scanned image data .", "ner": [["One", 0, 0, "Generic"], ["systems", 5, 5, "Generic"], ["text processors", 13, 14, "Method"], ["audio and scanned image data", 19, 23, "Material"]], "relations": [[5, 5, 13, 14, "CONJUNCTION"], [19, 23, 5, 5, "USED-FOR"]]},
{"sentence": "A second , related theme is the use of speech and text-image recognition to retrieve arbitrary , user-specified information from documents with signal content .", "ner": [["theme", 4, 4, "Generic"], ["speech and text-image recognition", 9, 12, "Method"], ["documents with signal content", 20, 23, "OtherScientificTerm"]], "relations": [[9, 12, 4, 4, "USED-FOR"], [20, 23, 9, 12, "USED-FOR"]]},
{"sentence": "This paper discusses three research initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a wordspotter for voice editing and indexing -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "ner": [["research", 4, 4, "Generic"], ["themes", 11, 11, "Generic"], ["text-image editor", 14, 15, "Method"], ["wordspotter", 21, 21, "Method"], ["voice editing and indexing", 23, 26, "Task"], ["decoding framework", 33, 34, "Method"], ["scanned-document content retrieval", 36, 38, "Task"]], "relations": [[14, 15, 4, 4, "HYPONYM-OF"], [14, 15, 21, 21, "CONJUNCTION"], [21, 21, 4, 4, "HYPONYM-OF"], [21, 21, 23, 26, "CONJUNCTION"], [23, 26, 33, 34, "CONJUNCTION"], [33, 34, 4, 4, "HYPONYM-OF"], [33, 34, 36, 38, "USED-FOR"]]},
{"sentence": "The discussion focuses on key concepts embodied in the research that enable novel signal-based document processing functionality .", "ner": [["signal-based document processing functionality", 13, 16, "Task"]], "relations": []},
{"sentence": "The problem of predicting image or video interestingness from their low-level feature representations has received increasing interest .", "ner": [["predicting image or video interestingness", 3, 7, "Task"], ["low-level feature representations", 10, 12, "Method"]], "relations": [[10, 12, 3, 7, "USED-FOR"]]},
{"sentence": "As a highly subjective visual attribute , annotating the interesting-ness value of training data for learning a prediction model is challenging .", "ner": [["subjective visual attribute", 3, 5, "OtherScientificTerm"], ["interesting-ness value", 9, 10, "OtherScientificTerm"], ["prediction model", 17, 18, "Method"]], "relations": []},
{"sentence": "To make the annotation less subjective and more reliable , recent studies employ crowdsourcing tools to collect pairwise comparisons -- relying on majority voting to prune the annotation outliers/errors .", "ner": [["crowdsourcing tools", 13, 14, "Method"], ["pairwise comparisons", 17, 18, "OtherScientificTerm"], ["majority voting", 22, 23, "Method"], ["annotation outliers/errors", 27, 28, "OtherScientificTerm"]], "relations": [[13, 14, 17, 18, "USED-FOR"], [22, 23, 13, 14, "USED-FOR"], [22, 23, 27, 28, "USED-FOR"]]},
{"sentence": "In this paper , we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .", "ner": [["way", 9, 9, "Generic"], ["annotation outliers", 12, 13, "OtherScientificTerm"], ["interestingness prediction task", 17, 19, "Task"], ["unified robust learning", 22, 24, "Method"], ["rank problem", 26, 27, "Task"], ["outlier detection", 32, 33, "Task"], ["interestingness prediction tasks", 35, 37, "Task"]], "relations": [[9, 9, 12, 13, "USED-FOR"], [9, 9, 32, 33, "USED-FOR"], [9, 9, 35, 37, "USED-FOR"], [17, 19, 12, 13, "USED-FOR"], [22, 24, 17, 19, "USED-FOR"], [22, 24, 26, 27, "USED-FOR"], [32, 33, 35, 37, "CONJUNCTION"]]},
{"sentence": "Extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new approach significantly outperforms state-of-the-art alternatives .", "ner": [["image and video interestingness benchmark datasets", 4, 9, "Material"], ["approach", 14, 14, "Generic"], ["state-of-the-art alternatives", 17, 18, "Generic"]], "relations": [[4, 9, 14, 14, "EVALUATE-FOR"], [14, 14, 17, 18, "COMPARE"]]},
{"sentence": "Many description logics -LRB- DLs -RRB- combine knowledge representation on an abstract , logical level with an interface to `` concrete '' domains such as numbers and strings .", "ner": [["description logics -LRB- DLs -RRB-", 1, 5, "Method"], ["knowledge representation", 7, 8, "Method"]], "relations": [[7, 8, 1, 5, "CONJUNCTION"]]},
{"sentence": "We propose to extend such DLs with key constraints that allow the expression of statements like `` US citizens are uniquely identified by their social security number '' .", "ner": [["DLs", 5, 5, "Method"]], "relations": []},
{"sentence": "Based on this idea , we introduce a number of natural description logics and present -LRB- un -RRB- decidability results and tight NEx-PTlME complexity bounds .", "ner": [["natural description logics", 10, 12, "Method"], ["tight NEx-PTlME complexity bounds", 21, 24, "OtherScientificTerm"]], "relations": []},
{"sentence": "We describe an implementation of data-driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system .", "ner": [["data-driven selection", 5, 6, "Task"], ["emphatic facial displays", 8, 10, "Material"], ["embodied conversational agent", 13, 15, "Task"], ["dialogue system", 18, 19, "Task"]], "relations": [[5, 6, 13, 15, "USED-FOR"], [8, 10, 5, 6, "USED-FOR"], [13, 15, 18, 19, "PART-OF"]]},
{"sentence": "A corpus of sentences in the domain of the target dialogue system was recorded , and the facial displays used by the speaker were annotated .", "ner": [["corpus of sentences", 1, 3, "Material"], ["dialogue system", 10, 11, "Method"], ["facial displays", 17, 18, "Material"]], "relations": []},
{"sentence": "The data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of context or choosing displays differently within a context .", "ner": [["data", 1, 1, "Generic"], ["models", 11, 11, "Generic"], ["facial displays", 14, 15, "Material"], ["model", 18, 18, "Generic"]], "relations": [[1, 1, 11, 11, "USED-FOR"], [11, 11, 14, 15, "USED-FOR"]]},
{"sentence": "The models were evaluated in two ways : by cross-validation against the corpus , and by asking users to rate the output .", "ner": [["models", 1, 1, "Generic"], ["cross-validation", 9, 9, "Method"], ["corpus", 12, 12, "Generic"]], "relations": [[9, 9, 1, 1, "EVALUATE-FOR"]]},
{"sentence": "The predictions of the cross-validation study differed from the actual user ratings .", "ner": [["cross-validation", 4, 4, "Method"]], "relations": []},
{"sentence": "While the cross-validation gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation .", "ner": [["cross-validation", 2, 2, "Method"]], "relations": []},
{"sentence": "This preference was especially strong among the female subjects .", "ner": [], "relations": []},
{"sentence": "When classifying high-dimensional sequence data , traditional methods -LRB- e.g. , HMMs , CRFs -RRB- may require large amounts of training data to avoid overfitting .", "ner": [["classifying high-dimensional sequence data", 1, 4, "Task"], ["HMMs", 11, 11, "Method"], ["CRFs", 13, 13, "Method"], ["overfitting", 24, 24, "OtherScientificTerm"]], "relations": [[11, 11, 1, 4, "USED-FOR"], [11, 11, 13, 13, "CONJUNCTION"], [13, 13, 1, 4, "USED-FOR"]]},
{"sentence": "In such cases dimensionality reduction can be employed to find a low-dimensional representation on which classification can be done more efficiently .", "ner": [["dimensionality reduction", 3, 4, "Method"], ["low-dimensional representation", 11, 12, "Method"], ["classification", 15, 15, "Task"]], "relations": [[3, 4, 11, 12, "USED-FOR"], [11, 12, 15, 15, "USED-FOR"]]},
{"sentence": "Existing methods for supervised dimensionality reduction often presume that the data is densely sampled so that a neighborhood graph structure can be formed , or that the data arises from a known distribution .", "ner": [["Existing methods", 0, 1, "Generic"], ["supervised dimensionality reduction", 3, 5, "Task"], ["neighborhood graph structure", 17, 19, "OtherScientificTerm"], ["known distribution", 31, 32, "OtherScientificTerm"]], "relations": [[0, 1, 3, 5, "USED-FOR"]]},
{"sentence": "Sufficient dimension reduction techniques aim to find a low dimensional representation such that the remaining degrees of freedom become conditionally independent of the output values .", "ner": [["Sufficient dimension reduction techniques", 0, 3, "Method"], ["low dimensional representation", 8, 10, "Method"]], "relations": [[0, 3, 8, 10, "USED-FOR"]]},
{"sentence": "In this paper we develop a novel sequence kernel dimension reduction approach -LRB- S-KDR -RRB- .", "ner": [["sequence kernel dimension reduction approach -LRB- S-KDR -RRB-", 7, 14, "Method"]], "relations": []},
{"sentence": "Our approach does not make strong assumptions on the distribution of the input data .", "ner": [["approach", 1, 1, "Generic"]], "relations": []},
{"sentence": "Spatial , temporal and periodic information is combined in a principled manner , and an optimal manifold is learned for the end-task .", "ner": [["Spatial , temporal and periodic information", 0, 5, "OtherScientificTerm"], ["manifold", 16, 16, "OtherScientificTerm"], ["end-task", 21, 21, "Generic"]], "relations": [[16, 16, 21, 21, "USED-FOR"]]},
{"sentence": "We demonstrate the effectiveness of our approach on several tasks involving the discrimination of human gesture and motion categories , as well as on a database of dynamic textures .", "ner": [["approach", 6, 6, "Generic"], ["discrimination of human gesture and motion categories", 12, 18, "Task"], ["database of dynamic textures", 25, 28, "Material"]], "relations": [[12, 18, 6, 6, "EVALUATE-FOR"], [25, 28, 6, 6, "EVALUATE-FOR"]]},
{"sentence": "We present an efficient algorithm for chart-based phrase structure parsing of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .", "ner": [["algorithm", 4, 4, "Generic"], ["chart-based phrase structure parsing", 6, 9, "Task"], ["natural language", 11, 12, "Material"]], "relations": [[4, 4, 6, 9, "USED-FOR"], [11, 12, 6, 9, "USED-FOR"]]},
{"sentence": "The parser gains algorithmic efficiency through a reduction of its search space .", "ner": [["parser", 1, 1, "Method"], ["search space", 10, 11, "OtherScientificTerm"]], "relations": []},
{"sentence": "As each new edge is added to the chart , the algorithm checks only the topmost of the edges adjacent to it , rather than all such edges as in conventional treatments .", "ner": [["edge", 3, 3, "OtherScientificTerm"], ["chart", 8, 8, "OtherScientificTerm"], ["edges", 18, 18, "OtherScientificTerm"], ["edges", 27, 27, "OtherScientificTerm"]], "relations": []},
{"sentence": "The resulting spanning edges are insured to be the correct ones by carefully controlling the order in which edges are introduced so that every final constituent covers the longest possible span .", "ner": [["spanning edges", 2, 3, "OtherScientificTerm"], ["edges", 18, 18, "OtherScientificTerm"]], "relations": []},
{"sentence": "This is facilitated through the use of phrase boundary heuristics based on the placement of function words , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .", "ner": [["phrase boundary heuristics", 7, 9, "Method"], ["function words", 15, 16, "OtherScientificTerm"], ["heuristic rules", 20, 21, "Method"], ["unknown words", 35, 36, "OtherScientificTerm"]], "relations": [[15, 16, 7, 9, "USED-FOR"]]},
{"sentence": "A further reduction in the search space is achieved by using semantic rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .", "ner": [["reduction in the search space", 2, 6, "OtherScientificTerm"], ["semantic", 11, 11, "OtherScientificTerm"], ["syntactic categories", 14, 15, "OtherScientificTerm"], ["terminal and non-terminal edges", 18, 21, "OtherScientificTerm"], ["edges", 34, 34, "OtherScientificTerm"], ["edges", 38, 38, "OtherScientificTerm"]], "relations": [[11, 11, 2, 6, "USED-FOR"], [11, 11, 14, 15, "COMPARE"], [11, 11, 18, 21, "FEATURE-OF"], [14, 15, 18, 21, "FEATURE-OF"]]},
{"sentence": "Automatic estimation of word significance oriented for speech-based Information Retrieval -LRB- IR -RRB- is addressed .", "ner": [["Automatic estimation of word significance", 0, 4, "Task"], ["word significance", 3, 4, "OtherScientificTerm"], ["speech-based Information Retrieval -LRB- IR -RRB-", 7, 12, "Task"]], "relations": [[0, 4, 7, 12, "USED-FOR"]]},
{"sentence": "Since the significance of words differs in IR , automatic speech recognition -LRB- ASR -RRB- performance has been evaluated based on weighted word error rate -LRB- WWER -RRB- , which gives a weight on errors from the viewpoint of IR , instead of word error rate -LRB- WER -RRB- , which treats all words uniformly .", "ner": [["significance", 2, 2, "OtherScientificTerm"], ["IR", 7, 7, "Task"], ["automatic speech recognition -LRB- ASR -RRB-", 9, 14, "Task"], ["weighted word error rate -LRB- WWER -RRB-", 21, 27, "Metric"], ["IR", 39, 39, "Task"], ["word error rate -LRB- WER -RRB-", 43, 48, "Metric"]], "relations": [[21, 27, 9, 14, "EVALUATE-FOR"], [43, 48, 21, 27, "COMPARE"]]},
{"sentence": "A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown , and the reduction of errors on both ASR and IR has been reported .", "ner": [["decoding strategy", 1, 2, "Method"], ["WWER", 5, 5, "Metric"], ["Minimum Bayes-Risk framework", 9, 11, "Method"], ["ASR", 23, 23, "Task"], ["IR", 25, 25, "Task"]], "relations": [[1, 2, 5, 5, "USED-FOR"], [9, 11, 1, 2, "USED-FOR"], [23, 23, 25, 25, "CONJUNCTION"]]},
{"sentence": "In this paper , we propose an automatic estimation method for word significance -LRB- weights -RRB- based on its influence on IR .", "ner": [["automatic estimation method", 7, 9, "Method"], ["word significance -LRB- weights -RRB-", 11, 15, "OtherScientificTerm"], ["IR", 21, 21, "Task"]], "relations": [[7, 9, 11, 15, "EVALUATE-FOR"]]},
{"sentence": "Specifically , weights are estimated so that evaluation measures of ASR and IR are equivalent .", "ner": [["evaluation measures", 7, 8, "Metric"], ["ASR", 10, 10, "Task"], ["IR", 12, 12, "Task"]], "relations": [[7, 8, 10, 10, "EVALUATE-FOR"], [7, 8, 12, 12, "EVALUATE-FOR"], [10, 10, 12, 12, "CONJUNCTION"]]},
{"sentence": "We apply the proposed method to a speech-based information retrieval system , which is a typical IR system , and show that the method works well .", "ner": [["method", 4, 4, "Generic"], ["speech-based information retrieval system", 7, 10, "Task"], ["IR system", 16, 17, "Task"], ["method", 23, 23, "Generic"]], "relations": [[4, 4, 7, 10, "USED-FOR"], [7, 10, 16, 17, "HYPONYM-OF"]]},
{"sentence": "Methods developed for spelling correction for languages like English -LRB- see the review by Kukich -LRB- Kukich , 1992 -RRB- -RRB- are not readily applicable to agglutinative languages .", "ner": [["Methods", 0, 0, "Generic"], ["spelling correction", 3, 4, "Task"], ["languages", 6, 6, "OtherScientificTerm"], ["English", 8, 8, "OtherScientificTerm"], ["agglutinative languages", 26, 27, "OtherScientificTerm"]], "relations": [[0, 0, 3, 4, "USED-FOR"], [3, 4, 6, 6, "USED-FOR"], [8, 8, 6, 6, "HYPONYM-OF"]]},
{"sentence": "This poster presents an approach to spelling correction in agglutinative languages that is based on two-level morphology and a dynamic-programming based search algorithm .", "ner": [["approach", 4, 4, "Generic"], ["spelling correction", 6, 7, "Task"], ["agglutinative languages", 9, 10, "OtherScientificTerm"], ["two-level morphology", 15, 16, "OtherScientificTerm"], ["dynamic-programming based search algorithm", 19, 22, "Method"]], "relations": [[4, 4, 6, 7, "USED-FOR"], [9, 10, 6, 7, "USED-FOR"], [15, 16, 6, 7, "USED-FOR"], [15, 16, 19, 22, "CONJUNCTION"], [19, 22, 6, 7, "USED-FOR"]]},
{"sentence": "After an overview of our approach , we present results from experiments with spelling correction in Turkish .", "ner": [["approach", 5, 5, "Generic"], ["spelling correction", 13, 14, "Task"], ["Turkish", 16, 16, "OtherScientificTerm"]], "relations": [[16, 16, 13, 14, "USED-FOR"]]},
{"sentence": "In this paper , we present a novel training method for a localized phrase-based prediction model for statistical machine translation -LRB- SMT -RRB- .", "ner": [["training method", 8, 9, "Method"], ["localized phrase-based prediction model", 12, 15, "Method"], ["statistical machine translation -LRB- SMT -RRB-", 17, 22, "Task"]], "relations": [[8, 9, 12, 15, "USED-FOR"], [12, 15, 17, 22, "USED-FOR"]]},
{"sentence": "The model predicts blocks with orientation to handle local phrase re-ordering .", "ner": [["model", 1, 1, "Generic"], ["local phrase re-ordering", 8, 10, "Task"]], "relations": [[1, 1, 8, 10, "USED-FOR"]]},
{"sentence": "We use a maximum likelihood criterion to train a log-linear block bigram model which uses real-valued features -LRB- e.g. a language model score -RRB- as well as binary features based on the block identities themselves , e.g. block bigram features .", "ner": [["maximum likelihood criterion", 3, 5, "OtherScientificTerm"], ["log-linear block bigram model", 9, 12, "Method"], ["real-valued features", 15, 16, "OtherScientificTerm"], ["language model score", 20, 22, "OtherScientificTerm"], ["binary features", 27, 28, "OtherScientificTerm"]], "relations": [[3, 5, 9, 12, "USED-FOR"], [15, 16, 9, 12, "USED-FOR"], [15, 16, 27, 28, "CONJUNCTION"], [20, 22, 15, 16, "HYPONYM-OF"], [27, 28, 9, 12, "USED-FOR"]]},
{"sentence": "Our training algorithm can easily handle millions of features .", "ner": [["training algorithm", 1, 2, "Method"], ["features", 8, 8, "OtherScientificTerm"]], "relations": [[8, 8, 1, 2, "USED-FOR"]]},
{"sentence": "The best system obtains a 18.6 % improvement over the baseline on a standard Arabic-English translation task .", "ner": [["system", 2, 2, "Generic"], ["baseline", 10, 10, "Generic"], ["Arabic-English translation task", 14, 16, "Task"]], "relations": [[2, 2, 10, 10, "COMPARE"], [14, 16, 2, 2, "EVALUATE-FOR"], [14, 16, 10, 10, "EVALUATE-FOR"]]},
{"sentence": "In this paper we describe a novel data structure for phrase-based statistical machine translation which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations .", "ner": [["data structure", 7, 8, "OtherScientificTerm"], ["phrase-based statistical machine translation", 10, 13, "Task"], ["retrieval of arbitrarily long phrases", 18, 22, "Task"], ["memory", 27, 27, "OtherScientificTerm"], ["decoder", 33, 33, "Method"]], "relations": [[7, 8, 10, 13, "USED-FOR"], [7, 8, 18, 22, "USED-FOR"]]},
{"sentence": "We detail the computational complexity and average retrieval times for looking up phrase translations in our suffix array-based data structure .", "ner": [["computational complexity", 3, 4, "Metric"], ["average retrieval times", 6, 8, "Metric"], ["phrase translations", 12, 13, "OtherScientificTerm"], ["suffix array-based data structure", 16, 19, "OtherScientificTerm"]], "relations": [[3, 4, 6, 8, "CONJUNCTION"], [12, 13, 16, 19, "PART-OF"]]},
{"sentence": "We show how sampling can be used to reduce the retrieval time by orders of magnitude with no loss in translation quality .", "ner": [["sampling", 3, 3, "Method"], ["retrieval time", 10, 11, "Metric"], ["translation quality", 20, 21, "Metric"]], "relations": [[10, 11, 3, 3, "EVALUATE-FOR"], [20, 21, 3, 3, "EVALUATE-FOR"]]},
{"sentence": "The major objective of this program is to develop and demonstrate robust , high performance continuous speech recognition -LRB- CSR -RRB- techniques focussed on application in Spoken Language Systems -LRB- SLS -RRB- which will enhance the effectiveness of military and civilian computer-based systems .", "ner": [["continuous speech recognition -LRB- CSR -RRB- techniques", 15, 21, "Method"], ["Spoken Language Systems -LRB- SLS -RRB-", 26, 31, "Task"], ["military and civilian computer-based systems", 38, 42, "Task"]], "relations": [[15, 21, 26, 31, "USED-FOR"], [26, 31, 38, 42, "USED-FOR"]]},
{"sentence": "A key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of spoken language technology into military and civilian systems , with particular focus on application of robust CSR to mobile military command and control .", "ner": [["speech recognition and understanding systems", 12, 16, "Task"], ["spoken language technology", 25, 27, "Method"], ["military and civilian systems", 29, 32, "Task"], ["CSR", 41, 41, "Method"], ["mobile military command and control", 43, 47, "Task"]], "relations": [[25, 27, 29, 32, "USED-FOR"], [41, 41, 43, 47, "USED-FOR"]]},
{"sentence": "The research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "ner": [["acoustic modelling", 7, 8, "Method"], ["rapid search", 10, 11, "Method"], ["recognition-time adaptation techniques", 14, 16, "Method"], ["large-vocabulary CSR", 19, 20, "Method"], ["techniques", 26, 26, "Generic"], ["ARPA large-vocabulary CSR corpora", 30, 33, "Material"], ["military application tasks", 36, 38, "Task"]], "relations": [[7, 8, 10, 11, "CONJUNCTION"], [7, 8, 19, 20, "USED-FOR"], [7, 8, 26, 26, "HYPONYM-OF"], [7, 8, 36, 38, "USED-FOR"], [10, 11, 14, 16, "CONJUNCTION"], [10, 11, 19, 20, "USED-FOR"], [10, 11, 26, 26, "HYPONYM-OF"], [10, 11, 36, 38, "USED-FOR"], [14, 16, 19, 20, "USED-FOR"], [14, 16, 26, 26, "HYPONYM-OF"], [14, 16, 36, 38, "USED-FOR"], [19, 20, 30, 33, "USED-FOR"], [26, 26, 36, 38, "USED-FOR"], [30, 33, 26, 26, "EVALUATE-FOR"]]},
{"sentence": "This paper examines what kind of similarity between words can be represented by what kind of word vectors in the vector space model .", "ner": [["similarity between words", 6, 8, "OtherScientificTerm"], ["word vectors", 16, 17, "OtherScientificTerm"], ["vector space model", 20, 22, "Method"]], "relations": [[16, 17, 6, 8, "USED-FOR"], [20, 22, 16, 17, "USED-FOR"]]},
{"sentence": "Through two experiments , three methods for constructing word vectors , i.e. , LSA-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , taxonomic similarity and associative similarity .", "ner": [["methods", 5, 5, "Generic"], ["constructing word vectors", 7, 9, "Task"], ["LSA-based , cooccurrence-based and dictionary-based methods", 13, 18, "Method"], ["similarity", 32, 32, "Generic"], ["taxonomic similarity", 36, 37, "OtherScientificTerm"], ["associative similarity", 39, 40, "OtherScientificTerm"]], "relations": [[5, 5, 7, 9, "USED-FOR"], [13, 18, 32, 32, "USED-FOR"], [36, 37, 32, 32, "HYPONYM-OF"], [36, 37, 39, 40, "CONJUNCTION"], [39, 40, 32, 32, "HYPONYM-OF"]]},
{"sentence": "The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .", "ner": [["dictionary-based word vectors", 8, 10, "OtherScientificTerm"], ["taxonomic similarity", 13, 14, "OtherScientificTerm"], ["LSA-based and the cooccurrence-based word vectors", 18, 23, "OtherScientificTerm"], ["associative similarity", 26, 27, "OtherScientificTerm"]], "relations": [[8, 10, 13, 14, "USED-FOR"], [18, 23, 26, 27, "USED-FOR"]]},
{"sentence": "This paper presents a maximum entropy word alignment algorithm for Arabic-English based on supervised training data .", "ner": [["maximum entropy word alignment algorithm", 4, 8, "Method"], ["Arabic-English", 10, 10, "Material"], ["supervised training data", 13, 15, "Material"]], "relations": [[10, 10, 4, 8, "USED-FOR"], [13, 15, 4, 8, "USED-FOR"]]},
{"sentence": "We demonstrate that it is feasible to create training material for problems in machine translation and that a mixture of supervised and unsupervised methods yields superior performance .", "ner": [["training material", 8, 9, "Material"], ["machine translation", 13, 14, "Task"], ["supervised and unsupervised methods", 20, 23, "Method"]], "relations": [[8, 9, 13, 14, "USED-FOR"]]},
{"sentence": "The probabilistic model used in the alignment directly models the link decisions .", "ner": [["probabilistic model", 1, 2, "Method"], ["alignment", 6, 6, "Task"], ["link decisions", 10, 11, "Task"]], "relations": [[1, 2, 6, 6, "USED-FOR"], [1, 2, 10, 11, "USED-FOR"]]},
{"sentence": "Significant improvement over traditional word alignment techniques is shown as well as improvement on several machine translation tests .", "ner": [["word alignment techniques", 4, 6, "Method"], ["machine translation tests", 15, 17, "OtherScientificTerm"]], "relations": [[4, 6, 15, 17, "USED-FOR"]]},
{"sentence": "Performance of the algorithm is contrasted with human annotation performance .", "ner": [["algorithm", 3, 3, "Generic"], ["human annotation", 7, 8, "OtherScientificTerm"]], "relations": [[3, 3, 7, 8, "COMPARE"]]},
{"sentence": "In this paper , we propose a novel Cooperative Model for natural language understanding in a dialogue system .", "ner": [["Cooperative Model", 8, 9, "Method"], ["natural language understanding", 11, 13, "Task"], ["dialogue system", 16, 17, "Task"]], "relations": [[8, 9, 11, 13, "USED-FOR"], [11, 13, 16, 17, "USED-FOR"]]},
{"sentence": "We build this based on both Finite State Model -LRB- FSM -RRB- and Statistical Learning Model -LRB- SLM -RRB- .", "ner": [["this", 2, 2, "Generic"], ["Finite State Model -LRB- FSM -RRB-", 6, 11, "Method"], ["Statistical Learning Model -LRB- SLM -RRB-", 13, 18, "Method"]], "relations": [[6, 11, 2, 2, "USED-FOR"], [6, 11, 13, 18, "CONJUNCTION"], [13, 18, 2, 2, "USED-FOR"]]},
{"sentence": "FSM provides two strategies for language understanding and have a high accuracy but little robustness and flexibility .", "ner": [["FSM", 0, 0, "Method"], ["language understanding", 5, 6, "Task"]], "relations": [[0, 0, 5, 6, "USED-FOR"]]},
{"sentence": "Statistical approach is much more robust but less accurate .", "ner": [["Statistical approach", 0, 1, "Method"]], "relations": []},
{"sentence": "Cooperative Model incorporates all the three strategies together and thus can suppress all the shortcomings of different strategies and has all the advantages of the three strategies .", "ner": [["Cooperative Model", 0, 1, "Method"]], "relations": []},
{"sentence": "In the Chinese language , a verb may have its dependents on its left , right or on both sides .", "ner": [["Chinese language", 2, 3, "Material"]], "relations": []},
{"sentence": "The ambiguity resolution of right-side dependencies is essential for dependency parsing of sentences with two or more verbs .", "ner": [["ambiguity resolution of right-side dependencies", 1, 5, "Task"], ["dependency parsing", 9, 10, "Task"]], "relations": [[1, 5, 9, 10, "USED-FOR"]]},
{"sentence": "Previous works on shift-reduce dependency parsers may not guarantee the connectivity of a dependency tree due to their weakness at resolving the right-side dependencies .", "ner": [["shift-reduce dependency parsers", 3, 5, "Method"], ["connectivity", 10, 10, "Metric"], ["dependency tree", 13, 14, "OtherScientificTerm"], ["right-side dependencies", 22, 23, "OtherScientificTerm"]], "relations": [[10, 10, 13, 14, "EVALUATE-FOR"]]},
{"sentence": "This paper proposes a two-phase shift-reduce dependency parser based on SVM learning .", "ner": [["two-phase shift-reduce dependency parser", 4, 7, "Method"], ["SVM learning", 10, 11, "Method"]], "relations": [[10, 11, 4, 7, "USED-FOR"]]},
{"sentence": "The left-side dependents and right-side nominal dependents are detected in Phase I , and right-side verbal dependents are decided in Phase II .", "ner": [["left-side dependents", 1, 2, "OtherScientificTerm"], ["right-side nominal dependents", 4, 6, "OtherScientificTerm"], ["right-side verbal dependents", 14, 16, "OtherScientificTerm"]], "relations": [[1, 2, 4, 6, "CONJUNCTION"], [14, 16, 4, 6, "CONJUNCTION"]]},
{"sentence": "In experimental evaluation , our proposed method outperforms previous shift-reduce dependency parsers for the Chine language , showing improvement of dependency accuracy by 10.08 % .", "ner": [["method", 6, 6, "Generic"], ["shift-reduce dependency parsers", 9, 11, "Method"], ["Chine language", 14, 15, "Material"], ["dependency accuracy", 20, 21, "Metric"]], "relations": [[6, 6, 9, 11, "COMPARE"], [14, 15, 6, 6, "EVALUATE-FOR"], [14, 15, 9, 11, "EVALUATE-FOR"], [20, 21, 6, 6, "EVALUATE-FOR"], [20, 21, 9, 11, "EVALUATE-FOR"]]},
{"sentence": "In order to meet the needs of a publication of papers in English , many systems to run off texts have been developed .", "ner": [], "relations": []},
{"sentence": "In this paper , we report a system FROFF which can make a fair copy of not only texts but also graphs and tables indispensable to our papers .", "ner": [["system", 7, 7, "Generic"], ["FROFF", 8, 8, "Method"]], "relations": []},
{"sentence": "Its selection of fonts , specification of character size are dynamically changeable , and the typing location can be also changed in lateral or longitudinal directions .", "ner": [["typing location", 15, 16, "OtherScientificTerm"]], "relations": []},
{"sentence": "Each character has its own width and a line length is counted by the sum of each character .", "ner": [], "relations": []},
{"sentence": "By using commands or rules which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained .", "ner": [["commands", 2, 2, "OtherScientificTerm"], ["rules", 4, 4, "OtherScientificTerm"], ["mathematical expressions", 17, 18, "OtherScientificTerm"]], "relations": [[2, 2, 4, 4, "CONJUNCTION"], [2, 2, 17, 18, "USED-FOR"], [4, 4, 17, 18, "USED-FOR"]]},
{"sentence": "This paper presents an evaluation method employing a latent variable model for paraphrases with their contexts .", "ner": [["evaluation method", 4, 5, "Generic"], ["latent variable model", 8, 10, "Method"], ["paraphrases", 12, 12, "OtherScientificTerm"]], "relations": [[4, 5, 12, 12, "EVALUATE-FOR"], [8, 10, 4, 5, "USED-FOR"]]},
{"sentence": "We assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred .", "ner": [["latent variable", 12, 13, "OtherScientificTerm"], ["model", 16, 16, "Generic"]], "relations": []},
{"sentence": "A paraphrase is evaluated for whether its sentences are used in the same context .", "ner": [["paraphrase", 1, 1, "OtherScientificTerm"]], "relations": []},
{"sentence": "Experimental results showed that the proposed method achieves almost 60 % accuracy and that there is not a large performance difference between the two models .", "ner": [["method", 6, 6, "Generic"]], "relations": []},
{"sentence": "The results also revealed an upper bound of accuracy of 77 % with the method when using only topic information .", "ner": [["accuracy", 8, 8, "Metric"], ["method", 14, 14, "Generic"], ["topic information", 18, 19, "OtherScientificTerm"]], "relations": [[8, 8, 14, 14, "EVALUATE-FOR"], [18, 19, 14, 14, "USED-FOR"]]},
{"sentence": "We describe the methods and hardware that we are using to produce a real-time demonstration of an integrated Spoken Language System .", "ner": [["methods", 3, 3, "Generic"], ["hardware", 5, 5, "Generic"], ["integrated Spoken Language System", 17, 20, "Method"]], "relations": [[3, 3, 5, 5, "CONJUNCTION"], [3, 3, 17, 20, "USED-FOR"], [5, 5, 17, 20, "USED-FOR"]]},
{"sentence": "We describe algorithms that greatly reduce the computation needed to compute the N-Best sentence hypotheses .", "ner": [["algorithms", 2, 2, "Generic"], ["N-Best sentence hypotheses", 12, 14, "OtherScientificTerm"]], "relations": [[2, 2, 12, 14, "USED-FOR"]]},
{"sentence": "To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar .", "ner": [["grammar coverage problems", 2, 4, "OtherScientificTerm"], ["fully-connected first-order statistical class grammar", 8, 12, "Method"]], "relations": [[8, 12, 2, 4, "USED-FOR"]]},
{"sentence": "The speech-search algorithm is implemented on a board with a single Intel i860 chip , which provides a factor of 5 speed-up over a SUN 4 for straight C code .", "ner": [["speech-search algorithm", 1, 2, "Method"], ["board", 7, 7, "OtherScientificTerm"], ["Intel i860 chip", 11, 13, "OtherScientificTerm"], ["SUN 4", 24, 25, "OtherScientificTerm"], ["straight C code", 27, 29, "OtherScientificTerm"]], "relations": [[7, 7, 1, 2, "USED-FOR"], [11, 13, 7, 7, "PART-OF"], [11, 13, 24, 25, "COMPARE"], [11, 13, 27, 29, "USED-FOR"], [24, 25, 27, 29, "USED-FOR"]]},
{"sentence": "The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and application back end .", "ner": [["board", 1, 1, "OtherScientificTerm"], ["VME bus", 6, 7, "OtherScientificTerm"], ["SUN4", 10, 10, "OtherScientificTerm"], ["system", 15, 15, "Generic"], ["natural language system", 19, 21, "Method"], ["application back end", 23, 25, "OtherScientificTerm"]], "relations": [[1, 1, 15, 15, "USED-FOR"], [6, 7, 10, 10, "PART-OF"], [19, 21, 1, 1, "PART-OF"], [19, 21, 23, 25, "CONJUNCTION"], [23, 25, 1, 1, "PART-OF"]]},
{"sentence": "We address the problem of estimating location information of an image using principles from automated representation learning .", "ner": [["estimating location information", 5, 7, "Task"], ["image", 10, 10, "Material"], ["automated representation learning", 14, 16, "Method"]], "relations": [[10, 10, 5, 7, "USED-FOR"], [14, 16, 5, 7, "USED-FOR"]]},
{"sentence": "We pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations , by initializing it with a geometric prior corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of parallel transport on manifolds .", "ner": [["hierarchical sparse coding approach", 3, 6, "Method"], ["features", 9, 9, "OtherScientificTerm"], ["it", 19, 19, "Generic"], ["geometric prior", 22, 23, "OtherScientificTerm"], ["image appearance space", 28, 30, "OtherScientificTerm"], ["location grouping space", 34, 36, "OtherScientificTerm"], ["parallel transport on manifolds", 41, 44, "OtherScientificTerm"]], "relations": [[22, 23, 19, 19, "USED-FOR"], [41, 44, 22, 23, "USED-FOR"]]},
{"sentence": "We then extend this approach to account for the availability of heterogeneous data modalities such as geo-tags and videos pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .", "ner": [["approach", 4, 4, "Generic"], ["heterogeneous data modalities", 11, 13, "OtherScientificTerm"], ["geo-tags", 16, 16, "OtherScientificTerm"], ["videos", 18, 18, "OtherScientificTerm"], ["transferring knowledge", 32, 33, "Task"], ["grouping of data", 41, 43, "Task"]], "relations": [[4, 4, 11, 13, "USED-FOR"], [16, 16, 11, 13, "HYPONYM-OF"], [16, 16, 18, 18, "CONJUNCTION"], [18, 18, 11, 13, "HYPONYM-OF"], [32, 33, 41, 43, "USED-FOR"]]},
{"sentence": "We evaluate our approach on several standard datasets such as im2gps , San Francisco and MediaEval2010 , and obtain state-of-the-art results .", "ner": [["approach", 3, 3, "Method"], ["datasets", 7, 7, "Generic"], ["im2gps", 10, 10, "Material"], ["San Francisco", 12, 13, "Material"], ["MediaEval2010", 15, 15, "Material"]], "relations": [[7, 7, 3, 3, "EVALUATE-FOR"], [10, 10, 7, 7, "HYPONYM-OF"], [10, 10, 12, 13, "CONJUNCTION"], [12, 13, 7, 7, "HYPONYM-OF"], [12, 13, 15, 15, "CONJUNCTION"], [15, 15, 7, 7, "HYPONYM-OF"]]},
{"sentence": "Conventional HMMs have weak duration constraints .", "ner": [["HMMs", 1, 1, "Method"], ["weak duration constraints", 3, 5, "OtherScientificTerm"]], "relations": [[3, 5, 1, 1, "FEATURE-OF"]]},
{"sentence": "In noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce word matches with unrealistic durations .", "ner": [["corrupted speech signals", 7, 9, "Material"], ["models", 11, 11, "Generic"], ["clean speech", 14, 15, "Material"], ["decoder", 19, 19, "Method"], ["word matches", 22, 23, "OtherScientificTerm"], ["unrealistic durations", 25, 26, "OtherScientificTerm"]], "relations": [[14, 15, 11, 11, "USED-FOR"], [19, 19, 22, 23, "USED-FOR"], [25, 26, 22, 23, "FEATURE-OF"]]},
{"sentence": "This paper presents a simple way to incorporate word duration constraints by unrolling HMMs to form a lattice where word duration probabilities can be applied directly to state transitions .", "ner": [["word duration constraints", 8, 10, "OtherScientificTerm"], ["unrolling HMMs", 12, 13, "Method"], ["lattice", 17, 17, "OtherScientificTerm"], ["word duration probabilities", 19, 21, "OtherScientificTerm"], ["state transitions", 27, 28, "OtherScientificTerm"]], "relations": [[12, 13, 8, 10, "USED-FOR"], [12, 13, 17, 17, "USED-FOR"], [19, 21, 27, 28, "USED-FOR"]]},
{"sentence": "The expanded HMMs are compatible with conventional Viterbi decoding .", "ner": [["HMMs", 2, 2, "Method"], ["Viterbi decoding", 7, 8, "Method"]], "relations": [[7, 8, 2, 2, "CONJUNCTION"]]},
{"sentence": "Experiments on connected-digit recognition show that when using explicit duration constraints the decoder generates word matches with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .", "ner": [["connected-digit recognition", 2, 3, "Task"], ["duration constraints", 9, 10, "OtherScientificTerm"], ["decoder", 12, 12, "Method"], ["word matches", 14, 15, "OtherScientificTerm"], ["word error rates", 22, 24, "Metric"], ["noise conditions", 33, 34, "OtherScientificTerm"]], "relations": [[2, 3, 12, 12, "USED-FOR"], [9, 10, 12, 12, "USED-FOR"], [12, 12, 14, 15, "USED-FOR"]]},
{"sentence": "Soames 1979 provides some counterexamples to the theory of natural language presuppositions that is presented in Gazdar 1979 .", "ner": [["natural language presuppositions", 9, 11, "Task"]], "relations": []},
{"sentence": "Soames 1982 provides a theory which explains these counterexamples .", "ner": [], "relations": []},
{"sentence": "Mercer 1987 rejects the solution found in Soames 1982 leaving these counterexamples unexplained .", "ner": [], "relations": []},
{"sentence": "By reappraising these insightful counterexamples , the inferential theory for natural language presuppositions described in Mercer 1987 , 1988 gives a simple and straightforward explanation for the presuppositional nature of these sentences .", "ner": [["natural language presuppositions", 10, 12, "Task"], ["presuppositional nature", 27, 28, "OtherScientificTerm"]], "relations": []},
{"sentence": "One of the claimed benefits of Tree Adjoining Grammars is that they have an extended domain of locality -LRB- EDOL -RRB- .", "ner": [["Tree Adjoining Grammars", 6, 8, "Method"], ["they", 11, 11, "Generic"], ["extended domain of locality -LRB- EDOL -RRB-", 14, 20, "OtherScientificTerm"]], "relations": [[6, 8, 14, 20, "FEATURE-OF"]]},
{"sentence": "We consider how this can be exploited to limit the need for feature structure unification during parsing .", "ner": [["feature structure unification", 12, 14, "Method"], ["parsing", 16, 16, "Task"]], "relations": [[12, 14, 16, 16, "USED-FOR"]]},
{"sentence": "We compare two wide-coverage lexicalized grammars of English , LEXSYS and XTAG , finding that the two grammars exploit EDOL in different ways .", "ner": [["lexicalized grammars of English", 4, 7, "Method"], ["LEXSYS", 9, 9, "Method"], ["XTAG", 11, 11, "Method"], ["grammars", 17, 17, "Method"], ["EDOL", 19, 19, "OtherScientificTerm"]], "relations": [[9, 9, 4, 7, "HYPONYM-OF"], [9, 9, 11, 11, "COMPARE"], [11, 11, 4, 7, "HYPONYM-OF"], [17, 17, 19, 19, "USED-FOR"]]},
{"sentence": "Identity uncertainty is a pervasive problem in real-world data analysis .", "ner": [["Identity uncertainty", 0, 1, "Task"], ["real-world data analysis", 7, 9, "Task"]], "relations": [[0, 1, 7, 9, "HYPONYM-OF"]]},
{"sentence": "It arises whenever objects are not labeled with unique identifiers or when those identifiers may not be perceived perfectly .", "ner": [["identifiers", 9, 9, "Generic"], ["identifiers", 13, 13, "Generic"]], "relations": []},
{"sentence": "In such cases , two observations may or may not correspond to the same object .", "ner": [], "relations": []},
{"sentence": "In this paper , we consider the problem in the context of citation matching -- the problem of deciding which citations correspond to the same publication .", "ner": [["problem", 7, 7, "Generic"], ["citation matching", 12, 13, "Task"], ["citations", 20, 20, "OtherScientificTerm"], ["publication", 25, 25, "OtherScientificTerm"]], "relations": []},
{"sentence": "Our approach is based on the use of a relational probability model to define a generative model for the domain , including models of author and title corruption and a probabilistic citation grammar .", "ner": [["approach", 1, 1, "Generic"], ["relational probability model", 9, 11, "Method"], ["generative model", 15, 16, "Method"], ["domain", 19, 19, "Generic"], ["models of author and title corruption", 22, 27, "Method"], ["probabilistic citation grammar", 30, 32, "Method"]], "relations": [[9, 11, 1, 1, "USED-FOR"], [9, 11, 15, 16, "USED-FOR"], [15, 16, 19, 19, "USED-FOR"], [22, 27, 9, 11, "PART-OF"], [22, 27, 30, 32, "CONJUNCTION"], [30, 32, 9, 11, "PART-OF"]]},
{"sentence": "Identity uncertainty is handled by extending standard models to incorporate probabilities over the possible mappings between terms in the language and objects in the domain .", "ner": [["Identity uncertainty", 0, 1, "Task"], ["models", 7, 7, "Generic"], ["mappings", 14, 14, "OtherScientificTerm"], ["domain", 24, 24, "OtherScientificTerm"]], "relations": [[7, 7, 0, 1, "USED-FOR"]]},
{"sentence": "Inference is based on Markov chain Monte Carlo , augmented with specific methods for generating efficient proposals when the domain contains many objects .", "ner": [["Inference", 0, 0, "Task"], ["Markov chain Monte Carlo", 4, 7, "Method"], ["methods", 12, 12, "Generic"]], "relations": [[4, 7, 0, 0, "USED-FOR"], [12, 12, 0, 0, "USED-FOR"]]},
{"sentence": "Results on several citation data sets show that the method outperforms current algorithms for citation matching .", "ner": [["citation data sets", 3, 5, "Material"], ["method", 9, 9, "Generic"], ["current algorithms", 11, 12, "Method"], ["citation matching", 14, 15, "Task"]], "relations": [[3, 5, 9, 9, "EVALUATE-FOR"], [9, 9, 11, 12, "COMPARE"], [9, 9, 14, 15, "USED-FOR"], [11, 12, 14, 15, "USED-FOR"]]},
{"sentence": "The declarative , relational nature of the model also means that our algorithm can determine object characteristics such as author names by combining multiple citations of multiple papers .", "ner": [["model", 7, 7, "Generic"], ["algorithm", 12, 12, "Generic"], ["object characteristics", 15, 16, "OtherScientificTerm"], ["author names", 19, 20, "OtherScientificTerm"], ["citations", 24, 24, "OtherScientificTerm"]], "relations": [[12, 12, 15, 16, "USED-FOR"], [19, 20, 15, 16, "HYPONYM-OF"]]},
{"sentence": "The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization .", "ner": [["supervised learning", 9, 10, "Method"], ["unsupervised learning", 12, 13, "Method"], ["human biases in summarization", 17, 20, "Task"]], "relations": [[9, 10, 17, 20, "USED-FOR"], [12, 13, 9, 10, "CONJUNCTION"], [12, 13, 17, 20, "USED-FOR"]]},
{"sentence": "In particular , we explore the use of probabilistic decision tree within the clustering framework to account for the variation as well as regularity in human created summaries .", "ner": [["probabilistic decision tree", 8, 10, "OtherScientificTerm"], ["clustering framework", 13, 14, "Method"], ["human created summaries", 25, 27, "Material"]], "relations": [[13, 14, 8, 10, "FEATURE-OF"]]},
{"sentence": "The corpus of human created extracts is created from a newspaper corpus and used as a test set .", "ner": [["corpus of human created extracts", 1, 5, "Material"], ["newspaper corpus", 10, 11, "Material"]], "relations": [[10, 11, 1, 5, "USED-FOR"]]},
{"sentence": "We build probabilistic decision trees of different flavors and integrate each of them with the clustering framework .", "ner": [["probabilistic decision trees", 2, 4, "OtherScientificTerm"], ["them", 12, 12, "Generic"], ["clustering framework", 15, 16, "Method"]], "relations": [[15, 16, 12, 12, "CONJUNCTION"]]},
{"sentence": "Experiments with the corpus demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either ofthe two is considered alone .", "ner": [["corpus", 3, 3, "Generic"]], "relations": []},
{"sentence": "In this study , we propose a knowledge-independent method for aligning terms and thus extracting translations from a small , domain-specific corpus consisting of parallel English and Chinese court judgments from Hong Kong .", "ner": [["knowledge-independent method", 7, 8, "Method"], ["small , domain-specific corpus", 18, 21, "Material"], ["parallel English and Chinese court judgments", 24, 29, "Material"]], "relations": [[24, 29, 18, 21, "PART-OF"]]},
{"sentence": "With a sentence-aligned corpus , translation equivalences are suggested by analysing the frequency profiles of parallel concordances .", "ner": [["sentence-aligned corpus", 2, 3, "Material"], ["translation equivalences", 5, 6, "OtherScientificTerm"], ["frequency profiles", 12, 13, "Generic"], ["parallel concordances", 15, 16, "OtherScientificTerm"]], "relations": [[2, 3, 5, 6, "USED-FOR"], [12, 13, 15, 16, "PART-OF"]]},
{"sentence": "The method overcomes the limitations of conventional statistical methods which require large corpora to be effective , and lexical approaches which depend on existing bilingual dictionaries .", "ner": [["method", 1, 1, "Generic"], ["statistical methods", 7, 8, "Method"], ["large corpora", 11, 12, "Material"], ["lexical approaches", 18, 19, "Method"], ["bilingual dictionaries", 24, 25, "Material"]], "relations": [[1, 1, 7, 8, "COMPARE"], [1, 1, 18, 19, "COMPARE"], [11, 12, 7, 8, "USED-FOR"], [24, 25, 18, 19, "USED-FOR"]]},
{"sentence": "Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85 % precision and 45 % recall .", "ner": [["parallel corpus", 4, 5, "Material"], ["precision", 20, 20, "Metric"], ["recall", 24, 24, "Metric"]], "relations": [[20, 20, 24, 24, "CONJUNCTION"]]},
{"sentence": "Future work includes fine-tuning the algorithm upon the analysis of the errors , and acquiring a translation lexicon for legal terminology by filtering out general terms .", "ner": [["algorithm", 5, 5, "Generic"], ["translation lexicon", 16, 17, "OtherScientificTerm"], ["legal terminology", 19, 20, "Material"]], "relations": [[16, 17, 19, 20, "USED-FOR"]]},
{"sentence": "Sentiment Classification seeks to identify a piece of text according to its author 's general feeling toward their subject , be it positive or negative .", "ner": [["Sentiment Classification", 0, 1, "Task"]], "relations": []},
{"sentence": "Traditional machine learning techniques have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic .", "ner": [["machine learning techniques", 1, 3, "Method"], ["problem", 9, 9, "Generic"], ["training and test data", 31, 34, "Material"]], "relations": [[1, 3, 9, 9, "USED-FOR"]]},
{"sentence": "This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with training data labeled with emoticons , which has the potential of being independent of domain , topic and time .", "ner": [["training data", 20, 21, "Material"], ["emoticons", 24, 24, "OtherScientificTerm"]], "relations": [[24, 24, 20, 21, "FEATURE-OF"]]},
{"sentence": "We present a novel algorithm for estimating the broad 3D geometric structure of outdoor video scenes .", "ner": [["algorithm", 4, 4, "Generic"], ["3D geometric structure of outdoor video scenes", 9, 15, "Task"]], "relations": [[4, 4, 9, 15, "USED-FOR"]]},
{"sentence": "Leveraging spatio-temporal video segmentation , we decompose a dynamic scene captured by a video into geometric classes , based on predictions made by region-classifiers that are trained on appearance and motion features .", "ner": [["spatio-temporal video segmentation", 1, 3, "Method"], ["dynamic scene", 8, 9, "OtherScientificTerm"], ["geometric classes", 15, 16, "OtherScientificTerm"], ["region-classifiers", 23, 23, "Method"], ["appearance and motion features", 28, 31, "OtherScientificTerm"]], "relations": [[1, 3, 8, 9, "USED-FOR"], [15, 16, 8, 9, "PART-OF"], [23, 23, 15, 16, "USED-FOR"], [28, 31, 23, 23, "USED-FOR"]]},
{"sentence": "By examining the homogeneity of the prediction , we combine predictions across multiple segmentation hierarchy levels alleviating the need to determine the granularity a priori .", "ner": [["segmentation hierarchy levels", 13, 15, "OtherScientificTerm"], ["granularity a priori", 22, 24, "OtherScientificTerm"]], "relations": []},
{"sentence": "We built a novel , extensive dataset on geometric context of video to evaluate our method , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .", "ner": [["dataset", 6, 6, "Generic"], ["geometric context of video", 8, 11, "OtherScientificTerm"], ["method", 15, 15, "Generic"], ["annotated outdoor videos", 22, 24, "OtherScientificTerm"]], "relations": [[6, 6, 15, 15, "EVALUATE-FOR"], [8, 11, 6, 6, "FEATURE-OF"], [22, 24, 6, 6, "PART-OF"]]},
{"sentence": "To further scale beyond this dataset , we propose a semi-supervised learning framework to expand the pool of labeled data with high confidence predictions obtained from unlabeled data .", "ner": [["dataset", 5, 5, "Generic"], ["semi-supervised learning framework", 10, 12, "Method"], ["labeled data", 18, 19, "Material"], ["high confidence predictions", 21, 23, "OtherScientificTerm"], ["unlabeled data", 26, 27, "OtherScientificTerm"]], "relations": [[10, 12, 18, 19, "USED-FOR"], [21, 23, 10, 12, "USED-FOR"], [26, 27, 21, 23, "USED-FOR"]]},
{"sentence": "Our system produces an accurate prediction of geometric context of video achieving 96 % accuracy across main geometric classes .", "ner": [["system", 1, 1, "Generic"], ["geometric context of video", 7, 10, "OtherScientificTerm"], ["accuracy", 14, 14, "Metric"]], "relations": [[1, 1, 7, 10, "USED-FOR"], [14, 14, 1, 1, "EVALUATE-FOR"]]},
{"sentence": "This paper describes a system -LRB- RAREAS -RRB- which synthesizes marine weather forecasts directly from formatted weather data .", "ner": [["system", 4, 4, "Generic"], ["RAREAS", 6, 6, "Method"], ["marine weather forecasts", 10, 12, "Task"], ["formatted weather data", 15, 17, "Material"]], "relations": [[4, 4, 10, 12, "USED-FOR"], [15, 17, 4, 4, "USED-FOR"]]},
{"sentence": "Such synthesis appears feasible in certain natural sublanguages with stereotyped text structure .", "ner": [["synthesis", 1, 1, "Generic"], ["natural sublanguages with stereotyped text structure", 6, 11, "Material"]], "relations": [[6, 11, 1, 1, "USED-FOR"]]},
{"sentence": "RAREAS draws on several kinds of linguistic and non-linguistic knowledge and mirrors a forecaster 's apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events .", "ner": [["RAREAS", 0, 0, "Method"], ["linguistic and non-linguistic knowledge", 6, 9, "OtherScientificTerm"]], "relations": [[6, 9, 0, 0, "USED-FOR"]]},
{"sentence": "The approach can easily be adapted to synthesize bilingual or multi-lingual texts .", "ner": [["approach", 1, 1, "Generic"], ["bilingual or multi-lingual texts", 8, 11, "Material"]], "relations": [[8, 11, 1, 1, "USED-FOR"]]},
{"sentence": "When people use natural language in natural settings , they often use it ungrammatically , missing out or repeating words , breaking-off and restarting , speaking in fragments , etc. .", "ner": [["natural language", 3, 4, "OtherScientificTerm"], ["it", 12, 12, "Generic"]], "relations": []},
{"sentence": "Their human listeners are usually able to cope with these deviations with little difficulty .", "ner": [], "relations": []},
{"sentence": "If a computer system wishes to accept natural language input from its users on a routine basis , it must display a similar indifference .", "ner": [["computer system", 2, 3, "Method"], ["natural language input", 7, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper , we outline a set of parsing flexibilities that such a system should provide .", "ner": [["parsing flexibilities", 9, 10, "OtherScientificTerm"], ["system", 14, 14, "Generic"]], "relations": []},
{"sentence": "We go , on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system .", "ner": [["FlexP", 6, 6, "Method"], ["bottom-up pattern-matching parser", 9, 11, "Method"], ["flexibilities", 21, 21, "Generic"], ["restricted natural language", 23, 25, "OtherScientificTerm"], ["limited-domain computer system", 29, 31, "Method"]], "relations": [[6, 6, 9, 11, "HYPONYM-OF"], [9, 11, 21, 21, "USED-FOR"], [21, 21, 23, 25, "FEATURE-OF"], [21, 21, 29, 31, "PART-OF"], [23, 25, 9, 11, "USED-FOR"]]},
{"sentence": "Oral communication is ubiquitous and carries important information yet it is also time consuming to document .", "ner": [["Oral communication", 0, 1, "Material"]], "relations": []},
{"sentence": "Given the development of storage media and networks one could just record and store a conversation for documentation .", "ner": [["storage media and networks", 4, 7, "Task"], ["conversation", 15, 15, "Material"]], "relations": []},
{"sentence": "The question is , however , how an interesting information piece would be found in a large database .", "ner": [], "relations": []},
{"sentence": "Traditional information retrieval techniques use a histogram of keywords as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .", "ner": [["information retrieval techniques", 1, 3, "Method"], ["histogram of keywords", 6, 8, "Method"], ["document representation", 11, 12, "Method"], ["oral communication", 14, 15, "Material"]], "relations": [[6, 8, 1, 3, "USED-FOR"], [6, 8, 11, 12, "USED-FOR"]]},
{"sentence": "An alternative index could be the activity such as discussing , planning , informing , story-telling , etc. .", "ner": [["activity", 6, 6, "OtherScientificTerm"], ["discussing", 9, 9, "OtherScientificTerm"], ["planning", 11, 11, "OtherScientificTerm"], ["informing", 13, 13, "OtherScientificTerm"], ["story-telling", 15, 15, "OtherScientificTerm"]], "relations": [[9, 9, 6, 6, "HYPONYM-OF"], [9, 9, 11, 11, "CONJUNCTION"], [11, 11, 6, 6, "HYPONYM-OF"], [11, 11, 13, 13, "CONJUNCTION"], [13, 13, 6, 6, "HYPONYM-OF"], [13, 13, 15, 15, "CONJUNCTION"], [15, 15, 6, 6, "HYPONYM-OF"]]},
{"sentence": "This paper addresses the problem of the automatic detection of those activities in meeting situation and everyday rejoinders .", "ner": [["automatic detection", 7, 8, "Task"], ["activities", 11, 11, "Generic"]], "relations": [[11, 11, 7, 8, "USED-FOR"]]},
{"sentence": "Several extensions of this basic idea are being discussed and/or evaluated : Similar to activities one can define subsets of larger database and detect those automatically which is shown on a large database of TV shows .", "ner": [["activities", 14, 14, "Generic"], ["database of TV shows", 32, 35, "Material"]], "relations": []},
{"sentence": "Emotions and other indices such as the dominance distribution of speakers might be available on the surface and could be used directly .", "ner": [["Emotions", 0, 0, "Material"], ["dominance distribution of speakers", 7, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "Despite the small size of the databases used some results about the effectiveness of these indices can be obtained .", "ner": [], "relations": []},
{"sentence": "Taiwan Child Language Corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from Southern Min Chinese speaking families in Taiwan .", "ner": [["Taiwan Child Language Corpus", 0, 3, "Material"]], "relations": []},
{"sentence": "The format of the corpus adopts the Child Language Data Exchange System -LRB- CHILDES -RRB- .", "ner": [["corpus", 4, 4, "Generic"], ["Child Language Data Exchange System -LRB- CHILDES -RRB-", 7, 14, "Material"]], "relations": [[7, 14, 4, 4, "FEATURE-OF"]]},
{"sentence": "The size of the corpus is about 1.6 million words .", "ner": [["corpus", 4, 4, "Generic"]], "relations": []},
{"sentence": "In this paper , we describe data collection , transcription , word segmentation , and part-of-speech annotation of this corpus .", "ner": [["data collection", 6, 7, "Task"], ["transcription", 9, 9, "Task"], ["word segmentation", 11, 12, "Task"], ["part-of-speech annotation", 15, 16, "Task"], ["corpus", 19, 19, "Generic"]], "relations": [[6, 7, 9, 9, "CONJUNCTION"], [6, 7, 19, 19, "USED-FOR"], [9, 9, 11, 12, "CONJUNCTION"], [9, 9, 19, 19, "USED-FOR"], [11, 12, 15, 16, "CONJUNCTION"], [11, 12, 19, 19, "USED-FOR"], [15, 16, 19, 19, "USED-FOR"]]},
{"sentence": "Applications of the corpus are also discussed .", "ner": [["corpus", 3, 3, "Generic"]], "relations": []},
{"sentence": "This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of phrasal patterns .", "ner": [["dictionary word sense definitions", 4, 7, "OtherScientificTerm"], ["hierarchy of phrasal patterns", 14, 17, "Method"]], "relations": [[14, 17, 4, 7, "USED-FOR"]]},
{"sentence": "An experimental system embodying this mechanism has been implemented for processing definitions from the Longman Dictionary of Contemporary English .", "ner": [["system", 2, 2, "Generic"], ["mechanism", 5, 5, "Generic"], ["Longman Dictionary of Contemporary English", 14, 18, "Material"]], "relations": [[5, 5, 2, 2, "PART-OF"]]},
{"sentence": "A property of this dictionary , exploited by the system , is that it uses a restricted vocabulary in its word sense definitions .", "ner": [["dictionary", 4, 4, "Generic"], ["system", 9, 9, "Generic"], ["it", 13, 13, "Generic"], ["restricted vocabulary", 16, 17, "OtherScientificTerm"], ["word sense definitions", 20, 22, "OtherScientificTerm"]], "relations": [[16, 17, 13, 13, "USED-FOR"], [16, 17, 20, 22, "USED-FOR"]]},
{"sentence": "The structures generated by the experimental system are intended to be used for the classification of new word senses in terms of the senses of words in the restricted vocabulary .", "ner": [["classification of new word senses", 14, 18, "Task"], ["restricted vocabulary", 28, 29, "OtherScientificTerm"]], "relations": [[28, 29, 14, 18, "USED-FOR"]]},
{"sentence": "Examples illustrating the output generated are presented , and some qualitative performance results and problems that were encountered are discussed .", "ner": [], "relations": []},
{"sentence": "The analysis process applies successively more specific phrasal analysis rules as determined by a hierarchy of patterns in which less specific patterns dominate more specific ones .", "ner": [["phrasal analysis rules", 7, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "This ensures that reasonable incomplete analyses of the definitions are produced when more complete analyses are not possible , resulting in a relatively robust analysis mechanism .", "ner": [], "relations": []},
{"sentence": "Thus the work reported addresses two robustness problems faced by current experimental natural language processing systems : coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions .", "ner": [["robustness problems", 6, 7, "Task"], ["natural language processing systems", 12, 15, "Method"], ["incomplete lexicon", 20, 21, "Material"], ["incomplete knowledge of phrasal constructions", 24, 28, "OtherScientificTerm"]], "relations": [[6, 7, 12, 15, "FEATURE-OF"], [20, 21, 6, 7, "HYPONYM-OF"], [24, 28, 6, 7, "HYPONYM-OF"]]},
{"sentence": "This paper presents a word segmentation system in France Telecom R&D Beijing , which uses a unified approach to word breaking and OOV identification .", "ner": [["word segmentation system", 4, 6, "Method"], ["approach", 17, 17, "Generic"], ["word breaking", 19, 20, "Task"], ["OOV identification", 22, 23, "Task"]], "relations": [[17, 17, 4, 6, "USED-FOR"], [17, 17, 19, 20, "USED-FOR"], [17, 17, 22, 23, "USED-FOR"], [19, 20, 22, 23, "CONJUNCTION"]]},
{"sentence": "The output can be customized to meet different segmentation standards through the application of an ordered list of transformation .", "ner": [], "relations": []},
{"sentence": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "ner": [["system", 1, 1, "Generic"], ["segmentation bakeoff", 9, 10, "OtherScientificTerm"], ["PK-open", 12, 12, "OtherScientificTerm"], ["PK-closed", 14, 14, "OtherScientificTerm"], ["AS-open", 16, 16, "OtherScientificTerm"], ["AS-closed", 18, 18, "OtherScientificTerm"], ["HK-open", 20, 20, "OtherScientificTerm"], ["HK-closed", 22, 22, "OtherScientificTerm"], ["MSR-open", 24, 24, "OtherScientificTerm"], ["MSR - closed", 26, 28, "OtherScientificTerm"], ["MSR-open", 36, 36, "OtherScientificTerm"], ["MSR-close", 38, 38, "OtherScientificTerm"], ["PK-open", 40, 40, "OtherScientificTerm"]], "relations": [[12, 12, 9, 10, "HYPONYM-OF"], [12, 12, 14, 14, "CONJUNCTION"], [14, 14, 9, 10, "HYPONYM-OF"], [14, 14, 16, 16, "CONJUNCTION"], [16, 16, 9, 10, "HYPONYM-OF"], [16, 16, 18, 18, "CONJUNCTION"], [18, 18, 9, 10, "HYPONYM-OF"], [18, 18, 20, 20, "CONJUNCTION"], [20, 20, 9, 10, "HYPONYM-OF"], [20, 20, 22, 22, "CONJUNCTION"], [22, 22, 9, 10, "HYPONYM-OF"], [22, 22, 24, 24, "CONJUNCTION"], [24, 24, 9, 10, "HYPONYM-OF"], [24, 24, 26, 28, "CONJUNCTION"], [26, 28, 9, 10, "HYPONYM-OF"], [36, 36, 38, 38, "CONJUNCTION"], [38, 38, 40, 40, "CONJUNCTION"]]},
{"sentence": "Analysis of the results shows that each component of the system contributed to the scores .", "ner": [["system", 10, 10, "Generic"]], "relations": []},
{"sentence": "This paper describes a method of interactively visualizing and directing the process of translating a sentence .", "ner": [["method", 4, 4, "Generic"], ["interactively visualizing and directing the process of translating", 6, 13, "Task"]], "relations": [[4, 4, 6, 13, "USED-FOR"]]},
{"sentence": "The method allows a user to explore a model of syntax-based statistical machine translation -LRB- MT -RRB- , to understand the model 's strengths and weaknesses , and to compare it to other MT systems .", "ner": [["method", 1, 1, "Generic"], ["model", 8, 8, "Generic"], ["syntax-based statistical machine translation -LRB- MT -RRB-", 10, 16, "Task"], ["model", 21, 21, "Generic"], ["it", 30, 30, "Generic"], ["MT systems", 33, 34, "Method"]], "relations": [[1, 1, 8, 8, "USED-FOR"], [8, 8, 10, 16, "USED-FOR"], [30, 30, 33, 34, "COMPARE"]]},
{"sentence": "Using this visualization method , we can find and address conceptual and practical problems in an MT system .", "ner": [["visualization method", 2, 3, "Method"], ["MT system", 16, 17, "Method"]], "relations": [[2, 3, 16, 17, "USED-FOR"]]},
{"sentence": "In our demonstration at ACL , new users of our tool will drive a syntax-based decoder for themselves .", "ner": [["syntax-based decoder", 14, 15, "Method"]], "relations": []},
{"sentence": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations -LRB- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "ner": [["method", 1, 1, "Generic"], ["sense resolution", 3, 4, "Task"], ["WordNet", 11, 11, "Material"], ["on-line lexical database", 14, 16, "Material"], ["semantic relations", 19, 20, "OtherScientificTerm"], ["synonymy", 22, 22, "OtherScientificTerm"], ["antonymy", 24, 24, "OtherScientificTerm"], ["hyponymy", 26, 26, "OtherScientificTerm"], ["meronymy", 28, 28, "OtherScientificTerm"], ["causal and troponymic entailment", 30, 33, "OtherScientificTerm"]], "relations": [[1, 1, 3, 4, "USED-FOR"], [11, 11, 1, 1, "USED-FOR"], [11, 11, 14, 16, "HYPONYM-OF"], [19, 20, 11, 11, "PART-OF"], [22, 22, 19, 20, "HYPONYM-OF"], [22, 22, 24, 24, "CONJUNCTION"], [24, 24, 19, 20, "HYPONYM-OF"], [24, 24, 26, 26, "CONJUNCTION"], [26, 26, 19, 20, "HYPONYM-OF"], [26, 26, 28, 28, "CONJUNCTION"], [28, 28, 19, 20, "HYPONYM-OF"], [28, 28, 30, 33, "CONJUNCTION"], [30, 33, 19, 20, "HYPONYM-OF"]]},
{"sentence": "With WordNet , it is easy to retrieve sets of semantically related words , a facility that will be used for sense resolution during text processing , as follows .", "ner": [["WordNet", 1, 1, "Material"], ["semantically related words", 10, 12, "OtherScientificTerm"], ["sense resolution", 21, 22, "Task"], ["text processing", 24, 25, "Task"]], "relations": [[1, 1, 10, 12, "USED-FOR"], [10, 12, 21, 22, "USED-FOR"], [21, 22, 24, 25, "USED-FOR"]]},
{"sentence": "When a word with multiple senses is encountered , one of two procedures will be followed .", "ner": [["procedures", 12, 12, "Generic"]], "relations": []},
{"sentence": "Either , -LRB- 1 -RRB- words related in meaning to the alternative senses of the polysemous word will be retrieved ; new strings will be derived by substituting these related words into the context of the polysemous word ; a large textual corpus will then be searched for these derived strings ; and that sense will be chosen that corresponds to the derived string that is found most often in the corpus .", "ner": [["alternative senses", 11, 12, "OtherScientificTerm"], ["polysemous word", 15, 16, "OtherScientificTerm"], ["polysemous word", 36, 37, "OtherScientificTerm"]], "relations": []},
{"sentence": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .", "ner": [["polysemous word", 9, 10, "OtherScientificTerm"], ["WordNet", 35, 35, "Material"], ["semantic distance", 43, 44, "OtherScientificTerm"], ["procedure", 80, 80, "Generic"], ["information retrieval", 88, 89, "Task"], ["mechanical translation", 91, 92, "Task"], ["intelligent tutoring systems", 94, 96, "Task"]], "relations": [[35, 35, 43, 44, "USED-FOR"], [80, 80, 88, 89, "USED-FOR"], [80, 80, 91, 92, "USED-FOR"], [80, 80, 94, 96, "USED-FOR"], [88, 89, 91, 92, "CONJUNCTION"], [91, 92, 94, 96, "CONJUNCTION"]]},
{"sentence": "The interlingual approach to MT has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application .", "ner": [["interlingual approach", 1, 2, "Method"], ["MT", 4, 4, "Generic"], ["natural language understanding", 14, 16, "Task"], ["machine translation", 19, 20, "Task"]], "relations": [[1, 2, 4, 4, "USED-FOR"]]},
{"sentence": "However , not only the ambiguity but also the vagueness which every natural language inevitably has leads this approach into essential difficulties .", "ner": [], "relations": []},
{"sentence": "In contrast , our project , the Mu-project , adopts the transfer approach as the basic framework of MT .", "ner": [["Mu-project", 7, 7, "Method"], ["transfer approach", 11, 12, "Method"], ["MT", 18, 18, "Task"]], "relations": [[7, 7, 18, 18, "USED-FOR"], [11, 12, 7, 7, "USED-FOR"]]},
{"sentence": "This paper describes the detailed construction of the transfer phase of our system from Japanese to English , and gives some examples of problems which seem difficult to treat in the interlingual approach .", "ner": [["transfer phase", 8, 9, "Generic"], ["system", 12, 12, "Generic"], ["Japanese", 14, 14, "Material"], ["English", 16, 16, "Material"], ["interlingual approach", 31, 32, "Method"]], "relations": [[8, 9, 12, 12, "PART-OF"]]},
{"sentence": "The basic design principles of the transfer phase of our system have already been mentioned in -LRB- 1 -RRB- -LRB- 2 -RRB- .", "ner": [["transfer phase", 6, 7, "Generic"], ["system", 10, 10, "Generic"]], "relations": [[6, 7, 10, 10, "PART-OF"]]},
{"sentence": "Some of the principles which are relevant to the topic of this paper are : -LRB- a -RRB- Multiple Layer of Grammars -LRB- b -RRB- Multiple Layer Presentation -LRB- c -RRB- Lexicon Driven Processing -LRB- d -RRB- Form-Oriented Dictionary Description .", "ner": [["principles", 3, 3, "Generic"], ["Multiple Layer of Grammars", 18, 21, "Method"], ["Multiple Layer Presentation", 25, 27, "Method"], ["Lexicon Driven Processing", 31, 33, "Method"], ["Form-Oriented Dictionary Description", 37, 39, "Method"]], "relations": [[18, 21, 3, 3, "PART-OF"], [18, 21, 25, 27, "CONJUNCTION"], [25, 27, 3, 3, "PART-OF"], [25, 27, 31, 33, "CONJUNCTION"], [31, 33, 3, 3, "PART-OF"], [31, 33, 37, 39, "CONJUNCTION"], [37, 39, 3, 3, "PART-OF"]]},
{"sentence": "This paper also shows how these principles are realized in the current system .", "ner": [["principles", 6, 6, "Generic"], ["system", 12, 12, "Generic"]], "relations": [[6, 6, 12, 12, "PART-OF"]]},
{"sentence": "In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed .", "ner": [["discourse segments", 3, 4, "OtherScientificTerm"], ["method", 9, 9, "Generic"], ["discourse segmentation", 11, 12, "Task"], ["abduction of temporal relations", 16, 19, "Task"]], "relations": [[9, 9, 11, 12, "USED-FOR"], [16, 19, 11, 12, "USED-FOR"]]},
{"sentence": "This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora resolution .", "ner": [["method", 1, 1, "Generic"], ["temporal anaphora resolution", 17, 19, "Task"]], "relations": [[17, 19, 1, 1, "USED-FOR"]]},
{"sentence": "This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish .", "ner": [["deep processing", 6, 7, "Task"], ["shallow techniques", 11, 12, "Method"], ["NLP system", 17, 18, "Method"], ["linguistic PoS tagger and chunker", 22, 26, "Method"], ["broad coverage unification based grammar of Spanish", 33, 39, "Method"]], "relations": [[11, 12, 6, 7, "USED-FOR"], [22, 26, 33, 39, "PART-OF"], [33, 39, 17, 18, "USED-FOR"]]},
{"sentence": "Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the accuracy and the precision of the grammar .", "ner": [["system", 14, 14, "Generic"], ["robustness", 17, 17, "Metric"], ["linguistic processing", 20, 21, "Task"], ["accuracy", 26, 26, "Metric"], ["precision", 29, 29, "Metric"]], "relations": [[14, 14, 20, 21, "USED-FOR"], [17, 17, 14, 14, "EVALUATE-FOR"], [26, 26, 14, 14, "EVALUATE-FOR"], [26, 26, 29, 29, "CONJUNCTION"], [29, 29, 14, 14, "EVALUATE-FOR"]]},
{"sentence": "Joint image filters can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution .", "ner": [["Joint image filters", 0, 2, "Method"], ["guidance image", 6, 7, "Material"], ["guidance image", 18, 19, "Material"], ["suppressing noise", 25, 26, "Task"], ["enhancing spatial resolution", 28, 30, "Task"]], "relations": [[0, 2, 25, 26, "USED-FOR"], [0, 2, 28, 30, "USED-FOR"], [6, 7, 0, 2, "USED-FOR"], [25, 26, 28, 30, "CONJUNCTION"]]},
{"sentence": "Existing methods rely on various kinds of explicit filter construction or hand-designed objective functions .", "ner": [["Existing methods", 0, 1, "Generic"], ["explicit filter construction", 7, 9, "Method"], ["hand-designed objective functions", 11, 13, "Method"]], "relations": [[7, 9, 0, 1, "USED-FOR"], [7, 9, 11, 13, "CONJUNCTION"], [11, 13, 0, 1, "USED-FOR"]]},
{"sentence": "It is thus difficult to understand , improve , and accelerate them in a coherent framework .", "ner": [["them", 11, 11, "Generic"], ["coherent framework", 14, 15, "Generic"]], "relations": [[14, 15, 11, 11, "USED-FOR"]]},
{"sentence": "In this paper , we propose a learning-based approach to construct a joint filter based on Convolution-al Neural Networks .", "ner": [["learning-based approach", 7, 8, "Method"], ["joint filter", 12, 13, "Method"], ["Convolution-al Neural Networks", 16, 18, "Method"]], "relations": [[7, 8, 12, 13, "USED-FOR"], [16, 18, 7, 8, "USED-FOR"]]},
{"sentence": "In contrast to existing methods that consider only the guidance image , our method can selectively transfer salient structures that are consistent in both guidance and target images .", "ner": [["methods", 4, 4, "Generic"], ["guidance image", 9, 10, "Material"], ["method", 13, 13, "Generic"], ["transfer salient structures", 16, 18, "Task"]], "relations": [[4, 4, 13, 13, "COMPARE"], [9, 10, 4, 4, "USED-FOR"], [13, 13, 16, 18, "USED-FOR"]]},
{"sentence": "We show that the model trained on a certain type of data , e.g. , RGB and depth images , generalizes well for other modalities , e.g. , Flash/Non-Flash and RGB/NIR images .", "ner": [["model", 4, 4, "Generic"], ["data", 11, 11, "Generic"], ["RGB and depth images", 15, 18, "Material"], ["modalities", 24, 24, "Generic"], ["Flash/Non-Flash and RGB/NIR images", 28, 31, "Material"]], "relations": [[4, 4, 24, 24, "USED-FOR"], [11, 11, 4, 4, "USED-FOR"], [15, 18, 11, 11, "HYPONYM-OF"], [28, 31, 24, 24, "HYPONYM-OF"]]},
{"sentence": "We validate the effectiveness of the proposed joint filter through extensive comparisons with state-of-the-art methods .", "ner": [["joint filter", 7, 8, "Method"], ["state-of-the-art methods", 13, 14, "Generic"]], "relations": [[7, 8, 13, 14, "COMPARE"]]},
{"sentence": "In our current research into the design of cognitively well-motivated interfaces relying primarily on the display of graphical information , we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations .", "ner": [["cognitively well-motivated interfaces", 8, 10, "Task"], ["display of graphical information", 15, 18, "OtherScientificTerm"], ["graphical information", 24, 25, "OtherScientificTerm"]], "relations": [[15, 18, 8, 10, "USED-FOR"]]},
{"sentence": "This can occur due to too much information being requested , too little , information of the wrong kind , etc. .", "ner": [], "relations": []},
{"sentence": "To solve this problem , we are working towards the integration of natural language generation to augment the interaction", "ner": [["natural language generation", 12, 14, "Task"], ["interaction", 18, 18, "Generic"]], "relations": [[12, 14, 18, 18, "USED-FOR"]]},
{"sentence": "A central problem of word sense disambiguation -LRB- WSD -RRB- is the lack of manually sense-tagged data required for supervised learning .", "ner": [["word sense disambiguation -LRB- WSD -RRB-", 4, 9, "Task"], ["manually sense-tagged data", 14, 16, "Material"], ["supervised learning", 19, 20, "Method"]], "relations": [[14, 16, 19, 20, "USED-FOR"]]},
{"sentence": "In this paper , we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task .", "ner": [["approach", 7, 7, "Generic"], ["sense-tagged training data", 11, 13, "Material"], ["English-Chinese parallel corpora", 15, 17, "Material"], ["nouns", 26, 26, "OtherScientificTerm"], ["SENSEVAL-2 English lexical sample task", 29, 33, "Material"]], "relations": [[7, 7, 11, 13, "USED-FOR"], [7, 7, 26, 26, "USED-FOR"], [15, 17, 11, 13, "USED-FOR"], [26, 26, 29, 33, "PART-OF"]]},
{"sentence": "Our investigation reveals that this method of acquiring sense-tagged data is promising .", "ner": [["method", 5, 5, "Generic"], ["acquiring sense-tagged data", 7, 9, "Task"]], "relations": [[5, 5, 7, 9, "USED-FOR"]]},
{"sentence": "On a subset of the most difficult SENSEVAL-2 nouns , the accuracy difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that manually sense-tagged data have in their sense coverage .", "ner": [["SENSEVAL-2 nouns", 7, 8, "OtherScientificTerm"], ["accuracy", 11, 11, "Metric"], ["manually sense-tagged data", 37, 39, "Material"], ["sense coverage", 43, 44, "Metric"]], "relations": [[43, 44, 37, 39, "FEATURE-OF"]]},
{"sentence": "Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs .", "ner": [["domain dependence", 10, 11, "OtherScientificTerm"], ["evaluating WSD programs", 13, 15, "Task"], ["WSD programs", 14, 15, "Task"]], "relations": [[10, 11, 13, 15, "FEATURE-OF"]]},
{"sentence": "This paper presents an analysis of temporal anaphora in sentences which contain quantification over events , within the framework of Discourse Representation Theory .", "ner": [["temporal anaphora", 6, 7, "Task"], ["quantification over events", 12, 14, "OtherScientificTerm"], ["Discourse Representation Theory", 20, 22, "Method"]], "relations": [[12, 14, 6, 7, "PART-OF"], [20, 22, 6, 7, "USED-FOR"]]},
{"sentence": "The analysis in -LRB- Partee , 1984 -RRB- of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the temporal connective in the subordinate clause is before or after .", "ner": [["quantified sentences", 9, 10, "Material"], ["temporal connective", 15, 16, "OtherScientificTerm"], ["temporal connective", 24, 25, "OtherScientificTerm"], ["subordinate clause", 28, 29, "OtherScientificTerm"]], "relations": [[24, 25, 28, 29, "PART-OF"]]},
{"sentence": "This problem has been previously analyzed in -LRB- de Swart , 1991 -RRB- as an instance of the proportion problem and given a solution from a Generalized Quantifier approach .", "ner": [["problem", 1, 1, "Generic"], ["proportion problem", 18, 19, "OtherScientificTerm"], ["solution", 23, 23, "Generic"], ["Generalized Quantifier approach", 26, 28, "Method"]], "relations": [[26, 28, 1, 1, "USED-FOR"]]},
{"sentence": "By using a careful distinction between the different notions of reference time based on -LRB- Kamp and Reyle , 1993 -RRB- , we propose a solution to this problem , within the framework of DRT .", "ner": [["reference time", 10, 11, "OtherScientificTerm"], ["solution", 25, 25, "Generic"], ["problem", 28, 28, "Generic"], ["DRT", 34, 34, "Method"]], "relations": [[25, 25, 28, 28, "USED-FOR"], [34, 34, 28, 28, "USED-FOR"]]},
{"sentence": "We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences .", "ner": [["solution", 6, 6, "Generic"], ["temporal anaphora phenomena", 9, 11, "Task"], ["quantified sentences", 13, 14, "Material"]], "relations": [[6, 6, 9, 11, "USED-FOR"], [13, 14, 6, 6, "USED-FOR"]]},
{"sentence": "In this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction .", "ner": [["correlation of dependency relation paths", 6, 10, "OtherScientificTerm"], ["answer extraction", 16, 17, "Task"]], "relations": [[6, 10, 16, 17, "USED-FOR"]]},
{"sentence": "Using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .", "ner": [["correlation measure", 2, 3, "Metric"], ["dependency relations", 7, 8, "OtherScientificTerm"]], "relations": [[2, 3, 7, 8, "USED-FOR"]]},
{"sentence": "Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure .", "ner": [["approximate phrase mapping algorithm", 8, 11, "Method"], ["mapping score", 15, 16, "OtherScientificTerm"], ["correlation measure", 19, 20, "Metric"]], "relations": [[15, 16, 19, 20, "PART-OF"]]},
{"sentence": "The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training .", "ner": [["correlations", 1, 1, "Generic"], ["Maximum Entropy-based ranking model", 7, 10, "Method"]], "relations": [[1, 1, 7, 10, "PART-OF"]]},
{"sentence": "Experimental results show that our method significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in MRR .", "ner": [["method", 5, 5, "Generic"], ["syntactic relation-based methods", 9, 11, "Method"], ["MRR", 18, 18, "Metric"]], "relations": [[5, 5, 9, 11, "COMPARE"], [18, 18, 5, 5, "EVALUATE-FOR"], [18, 18, 9, 11, "EVALUATE-FOR"]]},
{"sentence": "A meaningful evaluation methodology can advance the state-of-the-art by encouraging mature , practical applications rather than `` toy '' implementations .", "ner": [["evaluation methodology", 2, 3, "Task"]], "relations": []},
{"sentence": "Evaluation is also crucial to assessing competing claims and identifying promising technical approaches .", "ner": [["Evaluation", 0, 0, "Task"], ["approaches", 12, 12, "Generic"]], "relations": [[0, 0, 12, 12, "USED-FOR"]]},
{"sentence": "While work in speech recognition -LRB- SR -RRB- has a history of evaluation methodologies that permit comparison among various systems , until recently no methodology existed for either developers of natural language -LRB- NL -RRB- interfaces or researchers in speech understanding -LRB- SU -RRB- to evaluate and compare the systems they developed .", "ner": [["speech recognition -LRB- SR -RRB-", 3, 7, "Task"], ["evaluation methodologies", 12, 13, "Generic"], ["natural language -LRB- NL -RRB- interfaces", 30, 35, "OtherScientificTerm"], ["speech understanding -LRB- SU -RRB-", 39, 43, "Task"]], "relations": []},
{"sentence": "Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems -LRB- SLS -RRB- program to agree on a methodology for comparative evaluation of SLS systems , and that methodology has been put into practice several times in comparative tests of several SLS systems .", "ner": [["methodology", 26, 26, "Generic"], ["evaluation of SLS systems", 29, 32, "Task"], ["SLS systems", 31, 32, "Method"], ["methodology", 36, 36, "Generic"], ["SLS systems", 49, 50, "Method"]], "relations": [[26, 26, 29, 32, "EVALUATE-FOR"], [36, 36, 49, 50, "EVALUATE-FOR"]]},
{"sentence": "These evaluations are probably the only NL evaluations other than the series of Message Understanding Conferences -LRB- Sundheim , 1989 ; Sundheim , 1991 -RRB- to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems -LRB- Palmer et al. , 1989 ; Neal et al. , 1991 -RRB- .", "ner": [["evaluations", 1, 1, "Generic"], ["NL evaluations", 6, 7, "Task"], ["Message Understanding Conferences", 13, 15, "Material"]], "relations": [[1, 1, 6, 7, "HYPONYM-OF"], [1, 1, 13, 15, "CONJUNCTION"]]},
{"sentence": "This paper describes a practical `` black-box '' methodology for automatic evaluation of question-answering NL systems .", "ner": [["`` black-box '' methodology", 5, 8, "Metric"], ["automatic evaluation of question-answering NL systems", 10, 15, "Task"]], "relations": [[5, 8, 10, 15, "EVALUATE-FOR"]]},
{"sentence": "While each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and it can be used with either speech or text input .", "ner": [["methodology", 17, 17, "Generic"], ["it", 22, 22, "Generic"], ["speech or text input", 28, 31, "OtherScientificTerm"]], "relations": [[28, 31, 22, 22, "USED-FOR"]]},
{"sentence": "The particular characteristics of the approach are described in the following section : subsequent sections present its implementation in the DARPA SLS community , and some problems and directions for future development .", "ner": [["approach", 5, 5, "Generic"]], "relations": []},
{"sentence": "In this paper we present a novel autonomous pipeline to build a personalized parametric model -LRB- pose-driven avatar -RRB- using a single depth sensor .", "ner": [["autonomous pipeline", 7, 8, "Generic"], ["personalized parametric model -LRB- pose-driven avatar -RRB-", 12, 18, "Method"], ["single depth sensor", 21, 23, "OtherScientificTerm"]], "relations": [[7, 8, 12, 18, "USED-FOR"], [21, 23, 7, 8, "USED-FOR"]]},
{"sentence": "Our method first captures a few high-quality scans of the user rotating herself at multiple poses from different views .", "ner": [["method", 1, 1, "Generic"]], "relations": []},
{"sentence": "We fit each incomplete scan using template fitting techniques with a generic human template , and register all scans to every pose using global consistency constraints .", "ner": [["template fitting techniques", 6, 8, "Method"], ["human template", 12, 13, "OtherScientificTerm"], ["global consistency constraints", 23, 25, "OtherScientificTerm"]], "relations": [[12, 13, 6, 8, "USED-FOR"]]},
{"sentence": "After registration , these watertight models with different poses are used to train a parametric model in a fashion similar to the SCAPE method .", "ner": [["watertight models", 4, 5, "Method"], ["parametric model", 14, 15, "Method"], ["SCAPE method", 22, 23, "Method"]], "relations": [[4, 5, 14, 15, "USED-FOR"], [22, 23, 14, 15, "USED-FOR"]]},
{"sentence": "Once the parametric model is built , it can be used as an anim-itable avatar or more interestingly synthesizing dynamic 3D models from single-view depth videos .", "ner": [["parametric model", 2, 3, "Method"], ["it", 7, 7, "Generic"], ["anim-itable avatar", 13, 14, "OtherScientificTerm"], ["dynamic 3D models", 19, 21, "Method"], ["single-view depth videos", 23, 25, "Material"]], "relations": [[7, 7, 13, 14, "USED-FOR"], [7, 7, 19, 21, "USED-FOR"], [23, 25, 19, 21, "USED-FOR"]]},
{"sentence": "Experimental results demonstrate the effectiveness of our system to produce dynamic models .", "ner": [["system", 7, 7, "Generic"], ["dynamic models", 10, 11, "Method"]], "relations": [[7, 7, 10, 11, "USED-FOR"]]},
{"sentence": "In this paper , we propose a novel algorithm to detect/compensate on-line interference effects when integrating Global Navigation Satellite System -LRB- GNSS -RRB- and Inertial Navigation System -LRB- INS -RRB- .", "ner": [["algorithm", 8, 8, "Generic"], ["on-line interference effects", 11, 13, "Task"], ["Global Navigation Satellite System -LRB- GNSS -RRB-", 16, 22, "Task"], ["Inertial Navigation System -LRB- INS -RRB-", 24, 29, "Task"]], "relations": [[8, 8, 11, 13, "USED-FOR"], [16, 22, 24, 29, "CONJUNCTION"]]},
{"sentence": "The GNSS/INS coupling is usually performed by an Extended Kalman Filter -LRB- EKF -RRB- which yields an accurate and robust localization .", "ner": [["GNSS/INS coupling", 1, 2, "Task"], ["Extended Kalman Filter -LRB- EKF -RRB-", 8, 13, "Method"], ["accurate and robust localization", 17, 20, "Task"]], "relations": [[8, 13, 1, 2, "USED-FOR"], [8, 13, 17, 20, "USED-FOR"]]},
{"sentence": "However , interference cause the GNSS measurement noise to increase unexpectedly , hence degrade the positioning accuracy .", "ner": [["interference", 2, 2, "OtherScientificTerm"], ["GNSS measurement noise", 5, 7, "OtherScientificTerm"], ["positioning accuracy", 15, 16, "Metric"]], "relations": []},
{"sentence": "In this context , our contribution is twofold .", "ner": [], "relations": []},
{"sentence": "We first study the impact of the GNSS noise inflation on the covariance of the EKF outputs so as to compute a least square estimate of the potential variance jumps .", "ner": [["covariance", 12, 12, "OtherScientificTerm"], ["EKF outputs", 15, 16, "OtherScientificTerm"], ["least square estimate", 22, 24, "Method"], ["variance jumps", 28, 29, "OtherScientificTerm"]], "relations": [[15, 16, 12, 12, "FEATURE-OF"], [22, 24, 28, 29, "USED-FOR"]]},
{"sentence": "Then , this estimation is used in a Bayesian test which decides whether interference are corrupting the GNSS signal or not .", "ner": [["estimation", 3, 3, "Generic"], ["Bayesian test", 8, 9, "Method"], ["GNSS signal", 17, 18, "OtherScientificTerm"]], "relations": [[3, 3, 8, 9, "USED-FOR"]]},
{"sentence": "It allows us to estimate their times of occurrence as well .", "ner": [], "relations": []},
{"sentence": "In this way , the impaired measurements can be discarded while their impact on the navigation solution can be compensated .", "ner": [["navigation solution", 15, 16, "Task"]], "relations": []},
{"sentence": "The results show the performance of the proposed approach on simulated data .", "ner": [["approach", 8, 8, "Generic"], ["simulated data", 10, 11, "Material"]], "relations": [[10, 11, 8, 8, "EVALUATE-FOR"]]},
{"sentence": "We propose a unified variational formulation for joint motion estimation and segmentation with explicit occlusion handling .", "ner": [["unified variational formulation", 3, 5, "Method"], ["joint motion estimation and segmentation", 7, 11, "Task"], ["explicit occlusion handling", 13, 15, "Method"]], "relations": [[3, 5, 7, 11, "USED-FOR"], [13, 15, 3, 5, "USED-FOR"]]},
{"sentence": "This is done by a multi-label representation of the flow field , where each label corresponds to a parametric representation of the motion .", "ner": [["multi-label representation of the flow field", 5, 10, "Method"], ["parametric representation of the motion", 18, 22, "Task"]], "relations": []},
{"sentence": "We use a convex formulation of the multi-label Potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints .", "ner": [["convex formulation", 3, 4, "Method"], ["multi-label Potts model", 7, 9, "Method"], ["asymmetric map-uniqueness criterion", 17, 19, "Metric"], ["formulation", 25, 25, "Generic"], ["convex constraints", 29, 30, "OtherScientificTerm"]], "relations": [[3, 4, 7, 9, "USED-FOR"], [17, 19, 25, 25, "PART-OF"], [29, 30, 25, 25, "USED-FOR"]]},
{"sentence": "Explicit occlusion handling eliminates errors otherwise created by the regularization .", "ner": [["Explicit occlusion handling", 0, 2, "Method"], ["regularization", 9, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "As occlusions can occur only at object boundaries , a large number of objects may be required .", "ner": [["occlusions", 1, 1, "OtherScientificTerm"], ["object boundaries", 6, 7, "OtherScientificTerm"]], "relations": []},
{"sentence": "By using a fast primal-dual algorithm we are able to handle several hundred motion segments .", "ner": [["primal-dual algorithm", 4, 5, "Method"], ["motion segments", 13, 14, "OtherScientificTerm"]], "relations": [[4, 5, 13, 14, "USED-FOR"]]},
{"sentence": "Results are shown on several classical motion segmentation and optical flow examples .", "ner": [["classical motion segmentation", 5, 7, "Task"], ["optical flow", 9, 10, "Task"]], "relations": []},
{"sentence": "Two main classes of approaches have been studied to perform monocular nonrigid 3D reconstruction : Template-based methods and Non-rigid Structure from Motion techniques .", "ner": [["approaches", 4, 4, "Generic"], ["monocular nonrigid 3D reconstruction", 10, 13, "Task"], ["Template-based methods", 15, 16, "Method"], ["Non-rigid Structure from Motion techniques", 18, 22, "Method"]], "relations": [[4, 4, 10, 13, "USED-FOR"], [15, 16, 4, 4, "HYPONYM-OF"], [15, 16, 18, 22, "CONJUNCTION"], [18, 22, 4, 4, "HYPONYM-OF"]]},
{"sentence": "While the first ones have been applied to reconstruct poorly-textured surfaces , they assume the availability of a 3D shape model prior to reconstruction .", "ner": [["ones", 3, 3, "Generic"], ["poorly-textured surfaces", 9, 10, "OtherScientificTerm"], ["they", 12, 12, "Generic"], ["3D shape model", 18, 20, "Method"], ["reconstruction", 23, 23, "Task"]], "relations": [[3, 3, 9, 10, "USED-FOR"], [18, 20, 12, 12, "USED-FOR"]]},
{"sentence": "By contrast , the second ones do not require such a shape template , but , instead , rely on points being tracked throughout a video sequence , and are thus ill-suited to handle poorly-textured surfaces .", "ner": [["ones", 5, 5, "Generic"], ["shape template", 11, 12, "OtherScientificTerm"], ["video sequence", 25, 26, "Material"], ["poorly-textured surfaces", 34, 35, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper , we introduce a template-free approach to reconstructing a poorly-textured , deformable surface .", "ner": [["template-free approach", 7, 8, "Method"], ["poorly-textured , deformable surface", 12, 15, "OtherScientificTerm"]], "relations": [[7, 8, 12, 15, "USED-FOR"]]},
{"sentence": "To this end , we leverage surface isometry and formulate 3D reconstruction as the joint problem of non-rigid image registration and depth estimation .", "ner": [["surface isometry", 6, 7, "Method"], ["3D reconstruction", 10, 11, "Task"], ["joint problem of non-rigid image registration and depth estimation", 14, 22, "Task"]], "relations": [[6, 7, 10, 11, "USED-FOR"], [14, 22, 10, 11, "USED-FOR"]]},
{"sentence": "Our experiments demonstrate that our approach yields much more accurate 3D reconstructions than state-of-the-art techniques .", "ner": [["approach", 5, 5, "Generic"], ["3D reconstructions", 10, 11, "Task"], ["state-of-the-art techniques", 13, 14, "Generic"]], "relations": [[5, 5, 13, 14, "COMPARE"], [10, 11, 5, 5, "EVALUATE-FOR"], [10, 11, 13, 14, "EVALUATE-FOR"]]},
{"sentence": "Many computer vision applications , such as image classification and video indexing , are usually multi-label classification problems in which an instance can be assigned to more than one category .", "ner": [["computer vision applications", 1, 3, "Task"], ["image classification", 7, 8, "Task"], ["video indexing", 10, 11, "Task"], ["multi-label classification problems", 15, 17, "Task"]], "relations": [[7, 8, 1, 3, "HYPONYM-OF"], [7, 8, 10, 11, "CONJUNCTION"], [10, 11, 1, 3, "HYPONYM-OF"], [15, 17, 1, 3, "USED-FOR"]]},
{"sentence": "In this paper , we present a novel multi-label classification approach with hypergraph regu-larization that addresses the correlations among different categories .", "ner": [["multi-label classification approach", 8, 10, "Method"], ["hypergraph regu-larization", 12, 13, "OtherScientificTerm"]], "relations": [[12, 13, 8, 10, "FEATURE-OF"]]},
{"sentence": "First , a hypergraph is constructed to capture the correlations among different categories , in which each vertex represents one training instance and each hyperedge for one category contains all the instances belonging to the same category .", "ner": [["hypergraph", 3, 3, "OtherScientificTerm"]], "relations": []},
{"sentence": "Then , an improved SVM like learning system incorporating the hypergraph regularization , called Rank-HLapSVM , is proposed to handle the multi-label classification problems .", "ner": [["SVM like learning system", 4, 7, "Method"], ["hypergraph regularization", 10, 11, "OtherScientificTerm"], ["Rank-HLapSVM", 14, 14, "Method"], ["multi-label classification problems", 21, 23, "Task"]], "relations": [[4, 7, 21, 23, "USED-FOR"], [10, 11, 4, 7, "PART-OF"], [14, 14, 4, 7, "HYPONYM-OF"]]},
{"sentence": "We find that the corresponding optimization problem can be efficiently solved by the dual coordinate descent method .", "ner": [["optimization problem", 5, 6, "Task"], ["dual coordinate descent method", 13, 16, "Method"]], "relations": [[13, 16, 5, 6, "USED-FOR"]]},
{"sentence": "Many promising experimental results on the real datasets including ImageCLEF and Me-diaMill demonstrate the effectiveness and efficiency of the proposed algorithm .", "ner": [["real datasets", 6, 7, "Generic"], ["ImageCLEF", 9, 9, "Material"], ["Me-diaMill", 11, 11, "Material"], ["algorithm", 20, 20, "Generic"]], "relations": [[6, 7, 20, 20, "EVALUATE-FOR"], [9, 9, 6, 7, "HYPONYM-OF"], [9, 9, 11, 11, "CONJUNCTION"], [11, 11, 6, 7, "HYPONYM-OF"]]},
{"sentence": "We derive a convex optimization problem for the task of segmenting sequential data , which explicitly treats presence of outliers .", "ner": [["convex optimization problem", 3, 5, "Task"], ["segmenting sequential data", 10, 12, "Task"], ["outliers", 19, 19, "OtherScientificTerm"]], "relations": [[3, 5, 10, 12, "USED-FOR"], [10, 12, 19, 19, "USED-FOR"]]},
{"sentence": "We describe two algorithms for solving this problem , one exact and one a top-down novel approach , and we derive a consistency results for the case of two segments and no outliers .", "ner": [["algorithms", 3, 3, "Generic"], ["problem", 7, 7, "Generic"]], "relations": [[3, 3, 7, 7, "USED-FOR"]]},
{"sentence": "Robustness to outliers is evaluated on two real-world tasks related to speech segmentation .", "ner": [["Robustness", 0, 0, "Metric"], ["outliers", 2, 2, "OtherScientificTerm"], ["real-world tasks", 7, 8, "Task"], ["speech segmentation", 11, 12, "Task"]], "relations": [[2, 2, 0, 0, "FEATURE-OF"], [7, 8, 0, 0, "EVALUATE-FOR"], [11, 12, 0, 0, "EVALUATE-FOR"], [11, 12, 7, 8, "FEATURE-OF"]]},
{"sentence": "Our algorithms outperform baseline seg-mentation algorithms .", "ner": [["algorithms", 1, 1, "Generic"], ["baseline seg-mentation algorithms", 3, 5, "Method"]], "relations": [[1, 1, 3, 5, "COMPARE"]]},
{"sentence": "This paper examines the properties of feature-based partial descriptions built on top of Halliday 's systemic networks .", "ner": [["feature-based partial descriptions", 6, 8, "OtherScientificTerm"], ["Halliday 's systemic networks", 13, 16, "Method"]], "relations": [[13, 16, 6, 8, "USED-FOR"]]},
{"sentence": "We show that the crucial operation of consistency checking for such descriptions is NP-complete , and therefore probably intractable , but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability .", "ner": [["consistency checking", 7, 8, "Method"], ["descriptions", 11, 11, "Generic"]], "relations": [[7, 8, 11, 11, "USED-FOR"]]},
{"sentence": "We describe Yoopick , a combinatorial sports prediction market that implements a flexible betting language , and in turn facilitates fine-grained probabilistic estimation of outcomes .", "ner": [["Yoopick", 2, 2, "Method"], ["combinatorial sports prediction market", 5, 8, "Method"], ["flexible betting language", 12, 14, "OtherScientificTerm"], ["fine-grained probabilistic estimation of outcomes", 20, 24, "Task"]], "relations": [[2, 2, 5, 8, "HYPONYM-OF"], [2, 2, 20, 24, "USED-FOR"], [12, 14, 2, 2, "USED-FOR"]]},
{"sentence": "The goal of this paper is to discover a set of discriminative patches which can serve as a fully unsupervised mid-level visual representation .", "ner": [["discriminative patches", 11, 12, "Method"], ["unsupervised mid-level visual representation", 19, 22, "Task"]], "relations": [[11, 12, 19, 22, "USED-FOR"]]},
{"sentence": "The desired patches need to satisfy two requirements : 1 -RRB- to be representative , they need to occur frequently enough in the visual world ; 2 -RRB- to be discriminative , they need to be different enough from the rest of the visual world .", "ner": [], "relations": []},
{"sentence": "The patches could correspond to parts , objects , '' visual phrases '' , etc. but are not restricted to be any one of them .", "ner": [["patches", 1, 1, "Generic"]], "relations": []},
{"sentence": "We pose this as an unsupervised discriminative clustering problem on a huge dataset of image patches .", "ner": [["this", 2, 2, "Generic"], ["unsupervised discriminative clustering problem", 5, 8, "Task"], ["image patches", 14, 15, "Material"]], "relations": [[14, 15, 5, 8, "USED-FOR"]]},
{"sentence": "We use an iterative procedure which alternates between clustering and training discriminative classifiers , while applying careful cross-validation at each step to prevent overfitting .", "ner": [["iterative procedure", 3, 4, "Method"], ["clustering", 8, 8, "Method"], ["training discriminative classifiers", 10, 12, "Method"], ["cross-validation", 17, 17, "Method"], ["overfitting", 23, 23, "OtherScientificTerm"]], "relations": [[17, 17, 23, 23, "USED-FOR"]]},
{"sentence": "The paper experimentally demonstrates the effectiveness of discriminative patches as an unsupervised mid-level visual representation , suggesting that it could be used in place of visual words for many tasks .", "ner": [["discriminative patches", 7, 8, "Method"], ["unsupervised mid-level visual representation", 11, 14, "Task"], ["it", 18, 18, "Generic"]], "relations": [[7, 8, 11, 14, "USED-FOR"], [18, 18, 7, 8, "USED-FOR"]]},
{"sentence": "Furthermore , discrim-inative patches can also be used in a supervised regime , such as scene classification , where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset .", "ner": [["discrim-inative patches", 2, 3, "Method"], ["supervised regime", 10, 11, "Task"], ["scene classification", 15, 16, "Task"], ["they", 19, 19, "Generic"], ["MIT Indoor-67 dataset", 25, 27, "Material"]], "relations": [[2, 3, 10, 11, "USED-FOR"], [15, 16, 10, 11, "HYPONYM-OF"], [25, 27, 19, 19, "EVALUATE-FOR"]]},
{"sentence": "We investigate the utility of an algorithm for translation lexicon acquisition -LRB- SABLE -RRB- , used previously on a very large corpus to acquire general translation lexicons , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .", "ner": [["algorithm", 6, 6, "Generic"], ["translation lexicon acquisition -LRB- SABLE -RRB-", 8, 13, "Task"], ["general translation lexicons", 24, 26, "OtherScientificTerm"], ["algorithm", 30, 30, "Generic"], ["domain-specific translation lexicons", 42, 44, "OtherScientificTerm"]], "relations": [[6, 6, 8, 13, "USED-FOR"], [6, 6, 24, 26, "USED-FOR"], [30, 30, 42, 44, "USED-FOR"]]},
{"sentence": "This paper describes a computational model of word segmentation and presents simulation results on realistic acquisition .", "ner": [["computational model", 4, 5, "Method"], ["word segmentation", 7, 8, "Task"], ["realistic acquisition", 14, 15, "Task"]], "relations": [[4, 5, 7, 8, "USED-FOR"], [4, 5, 14, 15, "USED-FOR"]]},
{"sentence": "In particular , we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics .", "ner": [["statistical learning mechanisms", 10, 12, "Method"], ["cognitive psychology", 19, 20, "Material"], ["linguistics", 22, 22, "Material"]], "relations": [[19, 20, 10, 12, "USED-FOR"], [19, 20, 22, 22, "CONJUNCTION"], [22, 22, 10, 12, "USED-FOR"]]},
{"sentence": "In the model-based policy search approach to reinforcement learning -LRB- RL -RRB- , policies are found using a model -LRB- or `` simulator '' -RRB- of the Markov decision process .", "ner": [["model-based policy search approach", 2, 5, "Method"], ["reinforcement learning -LRB- RL -RRB-", 7, 11, "Task"], ["policies", 13, 13, "OtherScientificTerm"], ["Markov decision process", 27, 29, "OtherScientificTerm"]], "relations": [[2, 5, 7, 11, "USED-FOR"], [27, 29, 13, 13, "USED-FOR"]]},
{"sentence": "However , for high-dimensional continuous-state tasks , it can be extremely difficult to build an accurate model , and thus often the algorithm returns a policy that works in simulation but not in real-life .", "ner": [["high-dimensional continuous-state tasks", 3, 5, "Task"], ["model", 16, 16, "Generic"], ["algorithm", 22, 22, "Generic"], ["policy", 25, 25, "OtherScientificTerm"]], "relations": [[16, 16, 3, 5, "USED-FOR"], [22, 22, 25, 25, "USED-FOR"]]},
{"sentence": "The other extreme , model-free RL , tends to require infeasibly large numbers of real-life trials .", "ner": [["model-free RL", 4, 5, "Task"], ["real-life trials", 14, 15, "OtherScientificTerm"]], "relations": [[14, 15, 4, 5, "USED-FOR"]]},
{"sentence": "In this paper , we present a hybrid algorithm that requires only an approximate model , and only a small number of real-life trials .", "ner": [["hybrid algorithm", 7, 8, "Method"], ["approximate model", 13, 14, "Method"], ["real-life trials", 22, 23, "OtherScientificTerm"]], "relations": [[13, 14, 7, 8, "USED-FOR"], [22, 23, 13, 14, "USED-FOR"]]},
{"sentence": "The key idea is to successively `` ground '' the policy evaluations using real-life trials , but to rely on the approximate model to suggest local changes .", "ner": [["policy evaluations", 10, 11, "Task"], ["real-life trials", 13, 14, "OtherScientificTerm"], ["approximate model", 21, 22, "Method"]], "relations": [[13, 14, 10, 11, "USED-FOR"]]},
{"sentence": "Our theoretical results show that this algorithm achieves near-optimal performance in the real system , even when the model is only approximate .", "ner": [["algorithm", 6, 6, "Generic"], ["near-optimal performance", 8, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "Empirical results also demonstrate that -- when given only a crude model and a small number of real-life trials -- our algorithm can obtain near-optimal performance in the real system .", "ner": [["crude model", 10, 11, "Generic"], ["real-life trials", 17, 18, "OtherScientificTerm"], ["algorithm", 21, 21, "Generic"], ["near-optimal performance", 24, 25, "OtherScientificTerm"]], "relations": [[10, 11, 17, 18, "CONJUNCTION"], [10, 11, 21, 21, "USED-FOR"], [17, 18, 21, 21, "USED-FOR"]]},
{"sentence": "Although every natural language system needs a computational lexicon , each system puts different amounts and types of information into its lexicon according to its individual needs .", "ner": [["natural language system", 2, 4, "Method"], ["computational lexicon", 7, 8, "OtherScientificTerm"], ["system", 11, 11, "Generic"]], "relations": [[7, 8, 2, 4, "USED-FOR"]]},
{"sentence": "However , some of the information needed across systems is shared or identical information .", "ner": [], "relations": []},
{"sentence": "This paper presents our experience in planning and building COMPLEX , a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing -LRB- NLP -RRB- systems .", "ner": [["COMPLEX", 9, 9, "Method"], ["computational lexicon", 12, 13, "OtherScientificTerm"], ["shared lexical information", 20, 22, "OtherScientificTerm"], ["Natural Language Processing -LRB- NLP -RRB- systems", 26, 32, "Task"]], "relations": [[9, 9, 12, 13, "HYPONYM-OF"], [9, 9, 26, 32, "USED-FOR"]]},
{"sentence": "We have drawn primarily on explicit and implicit information from machine-readable dictionaries -LRB- MRD 's -RRB- to create a broad coverage lexicon .", "ner": [["machine-readable dictionaries -LRB- MRD 's -RRB-", 10, 15, "Material"]], "relations": []},
{"sentence": "Sentence planning is a set of inter-related but distinct tasks , one of which is sentence scoping , i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences .", "ner": [["Sentence planning", 0, 1, "Task"], ["tasks", 9, 9, "Generic"], ["sentence scoping", 15, 16, "Task"], ["syntactic structure", 22, 23, "OtherScientificTerm"], ["speech acts", 26, 27, "OtherScientificTerm"]], "relations": [[15, 16, 9, 9, "PART-OF"], [22, 23, 26, 27, "USED-FOR"]]},
{"sentence": "In this paper , we present SPoT , a sentence planner , and a new methodology for automatically training SPoT on the basis of feedback provided by human judges .", "ner": [["SPoT", 6, 6, "Method"], ["sentence planner", 9, 10, "Method"], ["methodology", 15, 15, "Generic"], ["SPoT", 19, 19, "Method"]], "relations": [[6, 6, 9, 10, "HYPONYM-OF"], [15, 15, 19, 19, "USED-FOR"]]},
{"sentence": "We reconceptualize the task into two distinct phases .", "ner": [], "relations": []},
{"sentence": "First , a very simple , randomized sentence-plan-generator -LRB- SPG -RRB- generates a potentially large list of possible sentence plans for a given text-plan input .", "ner": [["randomized sentence-plan-generator -LRB- SPG -RRB-", 6, 10, "Method"], ["sentence plans", 18, 19, "OtherScientificTerm"], ["text-plan input", 23, 24, "OtherScientificTerm"]], "relations": [[6, 10, 18, 19, "USED-FOR"], [23, 24, 6, 10, "USED-FOR"]]},
{"sentence": "Second , the sentence-plan-ranker -LRB- SPR -RRB- ranks the list of output sentence plans , and then selects the top-ranked plan .", "ner": [["sentence-plan-ranker -LRB- SPR -RRB-", 3, 6, "Method"], ["sentence plans", 12, 13, "OtherScientificTerm"]], "relations": [[3, 6, 12, 13, "USED-FOR"]]},
{"sentence": "The SPR uses ranking rules automatically learned from training data .", "ner": [["SPR", 1, 1, "Method"], ["ranking rules", 3, 4, "OtherScientificTerm"]], "relations": [[3, 4, 1, 1, "USED-FOR"]]},
{"sentence": "We show that the trained SPR learns to select a sentence plan whose rating on average is only 5 % worse than the top human-ranked sentence plan .", "ner": [["SPR", 5, 5, "Method"], ["sentence plan", 10, 11, "OtherScientificTerm"], ["top human-ranked sentence plan", 23, 26, "OtherScientificTerm"]], "relations": [[5, 5, 10, 11, "USED-FOR"], [10, 11, 23, 26, "COMPARE"]]},
{"sentence": "We discuss maximum a posteriori estimation of continuous density hidden Markov models -LRB- CDHMM -RRB- .", "ner": [["maximum a posteriori estimation", 2, 5, "Method"], ["continuous density hidden Markov models -LRB- CDHMM -RRB-", 7, 14, "Method"]], "relations": [[2, 5, 7, 14, "USED-FOR"]]},
{"sentence": "The classical MLE reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .", "ner": [["MLE reestimation algorithms", 2, 4, "Method"], ["forward-backward algorithm", 8, 9, "Method"], ["segmental k-means algorithm", 12, 14, "Method"], ["reestimation formulas", 19, 20, "OtherScientificTerm"], ["HMM with Gaussian mixture observation densities", 24, 29, "Method"]], "relations": [[8, 9, 2, 4, "HYPONYM-OF"], [12, 14, 2, 4, "HYPONYM-OF"], [12, 14, 8, 9, "CONJUNCTION"], [19, 20, 24, 29, "USED-FOR"]]},
{"sentence": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training .", "ner": [["Bayesian learning", 6, 7, "Method"], ["speech recognition applications", 17, 19, "Task"], ["parameter smoothing", 22, 23, "Task"], ["speaker adaptation", 25, 26, "Task"], ["speaker group modeling", 28, 30, "Task"], ["corrective training", 32, 33, "Task"]], "relations": [[6, 7, 17, 19, "USED-FOR"], [22, 23, 17, 19, "HYPONYM-OF"], [22, 23, 25, 26, "CONJUNCTION"], [25, 26, 17, 19, "HYPONYM-OF"], [25, 26, 28, 30, "CONJUNCTION"], [28, 30, 17, 19, "HYPONYM-OF"], [28, 30, 32, 33, "CONJUNCTION"], [32, 33, 17, 19, "HYPONYM-OF"]]},
{"sentence": "New experimental results on all four applications are provided to show the effectiveness of the MAP estimation approach .", "ner": [["applications", 6, 6, "Generic"], ["MAP estimation approach", 15, 17, "Method"]], "relations": [[6, 6, 15, 17, "EVALUATE-FOR"]]},
{"sentence": "This paper describes a characters-based Chinese collocation system and discusses the advantages of it over a traditional word-based system .", "ner": [["characters-based Chinese collocation system", 4, 7, "Method"], ["it", 13, 13, "Generic"], ["word-based system", 17, 18, "Method"]], "relations": [[13, 13, 17, 18, "COMPARE"]]},
{"sentence": "Since wordbreaks are not conventionally marked in Chinese text corpora , a character-based collocation system has the dual advantages of avoiding pre-processing distortion and directly accessing sub-lexical information .", "ner": [["Chinese text corpora", 7, 9, "Material"], ["character-based collocation system", 12, 14, "Method"], ["avoiding pre-processing distortion", 20, 22, "Task"], ["accessing sub-lexical information", 25, 27, "Task"]], "relations": [[20, 22, 12, 14, "FEATURE-OF"], [20, 22, 25, 27, "CONJUNCTION"], [25, 27, 12, 14, "FEATURE-OF"]]},
{"sentence": "Furthermore , word-based collocational properties can be obtained through an auxiliary module of automatic segmentation .", "ner": [["word-based collocational properties", 2, 4, "OtherScientificTerm"], ["auxiliary module of automatic segmentation", 10, 14, "Method"]], "relations": [[10, 14, 2, 4, "USED-FOR"]]},
{"sentence": "This paper describes a method for utterance classification that does not require manual transcription of training data .", "ner": [["method", 4, 4, "Generic"], ["utterance classification", 6, 7, "Task"], ["manual transcription", 12, 13, "Material"]], "relations": [[4, 4, 6, 7, "USED-FOR"]]},
{"sentence": "The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .", "ner": [["method", 1, 1, "Generic"], ["domain independent acoustic models", 3, 6, "Method"], ["classifiers", 9, 9, "Method"], ["utterance classification", 12, 13, "Task"], ["word-trigram recognition", 26, 27, "Method"], ["manual transcription", 29, 30, "Material"]], "relations": [[1, 1, 12, 13, "USED-FOR"], [3, 6, 1, 1, "PART-OF"], [9, 9, 1, 1, "PART-OF"], [9, 9, 3, 6, "CONJUNCTION"], [26, 27, 1, 1, "USED-FOR"], [29, 30, 26, 27, "USED-FOR"]]},
{"sentence": "In our method , unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .", "ner": [["method", 2, 2, "Generic"], ["unsupervised training", 4, 5, "Method"], ["phone n-gram model", 12, 14, "Method"], ["domain", 18, 18, "Generic"], ["model", 26, 26, "Generic"], ["phone-string classifier", 32, 33, "Method"]], "relations": [[4, 5, 2, 2, "PART-OF"], [4, 5, 12, 14, "USED-FOR"], [4, 5, 18, 18, "USED-FOR"]]},
{"sentence": "The classification accuracy of the method is evaluated on three different spoken language system domains .", "ner": [["classification accuracy", 1, 2, "Metric"], ["method", 5, 5, "Generic"], ["spoken language system domains", 11, 14, "Material"]], "relations": [[1, 2, 5, 5, "EVALUATE-FOR"], [11, 14, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "The Interval Algebra -LRB- IA -RRB- and a subset of the Region Connection Calculus -LRB- RCC -RRB- , namely RCC-8 , are the dominant Artificial Intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively .", "ner": [["Interval Algebra -LRB- IA -RRB-", 1, 5, "Method"], ["Region Connection Calculus -LRB- RCC -RRB-", 11, 16, "Method"], ["RCC-8", 19, 19, "Method"], ["Artificial Intelligence approaches", 24, 26, "Method"], ["representing and reasoning about qualitative temporal and topological relations", 28, 36, "Task"], ["qualitative temporal and topological relations", 32, 36, "OtherScientificTerm"]], "relations": [[1, 5, 11, 16, "CONJUNCTION"], [1, 5, 24, 26, "HYPONYM-OF"], [11, 16, 24, 26, "HYPONYM-OF"], [19, 19, 11, 16, "HYPONYM-OF"], [24, 26, 28, 36, "USED-FOR"]]},
{"sentence": "Such qualitative information can be formulated as a Qualitative Constraint Network -LRB- QCN -RRB- .", "ner": [["qualitative information", 1, 2, "OtherScientificTerm"], ["Qualitative Constraint Network -LRB- QCN -RRB-", 8, 13, "Method"]], "relations": [[8, 13, 1, 2, "USED-FOR"]]},
{"sentence": "In this paper , we focus on the minimal labeling problem -LRB- MLP -RRB- and we propose an algorithm to efficiently derive all the feasible base relations of a QCN .", "ner": [["minimal labeling problem -LRB- MLP -RRB-", 8, 13, "Task"], ["algorithm", 18, 18, "Generic"], ["QCN", 29, 29, "Method"]], "relations": [[18, 18, 29, 29, "USED-FOR"]]},
{"sentence": "Our algorithm considers chordal QCNs and a new form of partial consistency which we define as \u25c6 G-consistency .", "ner": [["algorithm", 1, 1, "Generic"], ["chordal QCNs", 3, 4, "Method"], ["partial consistency", 10, 11, "OtherScientificTerm"], ["\u25c6 G-consistency", 16, 17, "OtherScientificTerm"]], "relations": [[3, 4, 1, 1, "PART-OF"], [3, 4, 10, 11, "CONJUNCTION"], [10, 11, 1, 1, "PART-OF"], [16, 17, 10, 11, "HYPONYM-OF"]]},
{"sentence": "Further , the proposed algorithm uses tractable subclasses of relations having a specific patchwork property for which-consistency implies the consistency of the input QCN .", "ner": [["algorithm", 4, 4, "Generic"], ["patchwork property", 13, 14, "OtherScientificTerm"], ["QCN", 23, 23, "Method"]], "relations": []},
{"sentence": "Experi-mentations with QCNs of IA and RCC-8 show the importance and efficiency of this new approach .", "ner": [["QCNs", 2, 2, "Method"], ["QCNs of IA and RCC-8", 2, 6, "Method"], ["IA", 4, 4, "Method"], ["RCC-8", 6, 6, "Method"], ["approach", 15, 15, "Generic"]], "relations": [[2, 6, 15, 15, "EVALUATE-FOR"]]},
{"sentence": "In this paper a morphological component with a limited capability to automatically interpret -LRB- and generate -RRB- derived words is presented .", "ner": [["morphological component", 4, 5, "Method"], ["derived words", 17, 18, "OtherScientificTerm"]], "relations": [[4, 5, 17, 18, "USED-FOR"]]},
{"sentence": "The system combines an extended two-level morphology -LSB- Trost , 1991a ; Trost , 1991b -RSB- with a feature-based word grammar building on a hierarchical lexicon .", "ner": [["system", 1, 1, "Generic"], ["two-level morphology", 5, 6, "OtherScientificTerm"], ["feature-based word grammar", 18, 20, "Method"], ["hierarchical lexicon", 24, 25, "OtherScientificTerm"]], "relations": [[5, 6, 1, 1, "USED-FOR"], [5, 6, 18, 20, "CONJUNCTION"], [24, 25, 18, 20, "USED-FOR"]]},
{"sentence": "Polymorphemic stems not explicitly stored in the lexicon are given a compositional interpretation .", "ner": [["Polymorphemic stems", 0, 1, "OtherScientificTerm"], ["compositional interpretation", 11, 12, "OtherScientificTerm"]], "relations": [[11, 12, 0, 1, "FEATURE-OF"]]},
{"sentence": "That way the system allows to minimize redundancy in the lexicon because derived words that are transparent need not to be stored explicitly .", "ner": [["system", 3, 3, "Generic"], ["derived words", 12, 13, "OtherScientificTerm"]], "relations": []},
{"sentence": "Also , words formed ad-hoc can be recognized correctly .", "ner": [["words formed ad-hoc", 2, 4, "OtherScientificTerm"]], "relations": []},
{"sentence": "The system is implemented in CommonLisp and has been tested on examples from German derivation .", "ner": [["system", 1, 1, "Generic"], ["CommonLisp", 5, 5, "Method"], ["German derivation", 13, 14, "Material"]], "relations": [[5, 5, 1, 1, "USED-FOR"], [13, 14, 1, 1, "EVALUATE-FOR"]]},
{"sentence": "Lyric-based song sentiment classification seeks to assign songs appropriate sentiment labels such as light-hearted heavy-hearted .", "ner": [["Lyric-based song sentiment classification", 0, 3, "Task"]], "relations": []},
{"sentence": "Four problems render vector space model -LRB- VSM -RRB- - based text classification approach ineffective : 1 -RRB- Many words within song lyrics actually contribute little to sentiment ; 2 -RRB- Nouns and verbs used to express sentiment are ambiguous ; 3 -RRB- Negations and modifiers around the sentiment keywords make particular contributions to sentiment ; 4 -RRB- Song lyric is usually very short .", "ner": [["vector space model -LRB- VSM -RRB- - based text classification approach", 3, 13, "Method"], ["song lyrics", 21, 22, "Material"], ["sentiment", 27, 27, "OtherScientificTerm"], ["Negations", 43, 43, "OtherScientificTerm"], ["modifiers", 45, 45, "OtherScientificTerm"], ["sentiment keywords", 48, 49, "OtherScientificTerm"], ["sentiment", 54, 54, "OtherScientificTerm"], ["Song lyric", 58, 59, "Material"]], "relations": [[43, 43, 45, 45, "CONJUNCTION"], [43, 43, 54, 54, "USED-FOR"], [45, 45, 54, 54, "USED-FOR"]]},
{"sentence": "To address these problems , the sentiment vector space model -LRB- s-VSM -RRB- is proposed to represent song lyric document .", "ner": [["sentiment vector space model -LRB- s-VSM -RRB-", 6, 12, "Method"], ["song lyric document", 17, 19, "Material"]], "relations": [[6, 12, 17, 19, "USED-FOR"]]},
{"sentence": "The preliminary experiments prove that the s-VSM model outperforms the VSM model in the lyric-based song sentiment classification task .", "ner": [["s-VSM model", 6, 7, "Method"], ["VSM model", 10, 11, "Method"], ["lyric-based song sentiment classification task", 14, 18, "Task"]], "relations": [[6, 7, 10, 11, "COMPARE"], [14, 18, 6, 7, "EVALUATE-FOR"], [14, 18, 10, 11, "EVALUATE-FOR"]]},
{"sentence": "We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation -LRB- USR -RRB- of a scope ambiguity , compute an USR with fewer mutually equivalent readings .", "ner": [["algorithm", 4, 4, "Generic"], ["redundancy elimination problem", 7, 9, "Task"], ["underspecified semantic representation -LRB- USR -RRB-", 13, 18, "Method"], ["scope ambiguity", 21, 22, "OtherScientificTerm"], ["USR", 26, 26, "Method"], ["equivalent readings", 30, 31, "Material"]], "relations": [[4, 4, 7, 9, "USED-FOR"], [13, 18, 21, 22, "USED-FOR"], [30, 31, 26, 26, "USED-FOR"]]},
{"sentence": "The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .", "ner": [["algorithm", 1, 1, "Generic"], ["underspecified chart representations", 4, 6, "Method"], ["dominance graphs", 11, 12, "OtherScientificTerm"], ["it", 14, 14, "Generic"], ["USRs", 20, 20, "Method"], ["large-scale grammars", 23, 24, "Method"]], "relations": [[1, 1, 4, 6, "USED-FOR"], [11, 12, 4, 6, "USED-FOR"], [14, 14, 20, 20, "USED-FOR"], [23, 24, 20, 20, "USED-FOR"]]},
{"sentence": "We evaluate the algorithm on a corpus , and show that it reduces the degree of ambiguity significantly while taking negligible runtime .", "ner": [["algorithm", 3, 3, "Generic"], ["it", 11, 11, "Generic"], ["degree of ambiguity", 14, 16, "OtherScientificTerm"]], "relations": [[11, 11, 14, 16, "USED-FOR"]]},
{"sentence": "Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. LFG and PATR-II .", "ner": [["grammatical formalisms", 2, 3, "Method"], ["context-free phrase-structure grammar", 11, 13, "Method"], ["LFG", 19, 19, "OtherScientificTerm"], ["PATR-II", 21, 21, "OtherScientificTerm"]], "relations": [[11, 13, 2, 3, "USED-FOR"], [19, 19, 2, 3, "HYPONYM-OF"], [19, 19, 21, 21, "CONJUNCTION"], [21, 21, 2, 3, "HYPONYM-OF"]]},
{"sentence": "Typically the processing of these formalisms is organized within a chart-parsing framework .", "ner": [["formalisms", 5, 5, "Generic"], ["chart-parsing framework", 10, 11, "Method"]], "relations": [[10, 11, 5, 5, "FEATURE-OF"]]},
{"sentence": "The declarative character of the formalisms makes it important to decide upon an overall optimal control strategy on the part of the processor .", "ner": [["formalisms", 5, 5, "Generic"], ["optimal control strategy", 14, 16, "Method"]], "relations": []},
{"sentence": "In particular , this brings the rule-invocation strategy into critical focus : to gain maximal processing efficiency , one has to determine the best way of putting the rules to use .", "ner": [["rule-invocation strategy", 6, 7, "Method"], ["rules", 28, 28, "OtherScientificTerm"]], "relations": []},
{"sentence": "The aim of this paper is to provide a survey and a practical comparison of fundamental rule-invocation strategies within context-free chart parsing .", "ner": [["rule-invocation strategies", 16, 17, "Method"], ["context-free chart parsing", 19, 21, "Method"]], "relations": [[16, 17, 19, 21, "PART-OF"]]},
{"sentence": "Terminology structuring has been the subject of much work in the context of terms extracted from corpora : given a set of terms , obtained from an existing resource or extracted from a corpus , identifying hierarchical -LRB- or other types of -RRB- relations between these terms .", "ner": [["Terminology structuring", 0, 1, "Task"], ["corpus", 33, 33, "Material"], ["hierarchical -LRB- or other types of -RRB- relations", 36, 43, "OtherScientificTerm"]], "relations": []},
{"sentence": "The present paper focusses on terminology structuring by lexical methods , which match terms on the basis on their content words , taking morphological variants into account .", "ner": [["terminology structuring", 5, 6, "Task"], ["lexical methods", 8, 9, "Method"], ["morphological variants", 23, 24, "OtherScientificTerm"]], "relations": [[8, 9, 5, 6, "USED-FOR"]]},
{"sentence": "Experiments are done on a ` flat ' list of terms obtained from an originally hierarchically-structured terminology : the French version of the US National Library of Medicine MeSH thesaurus .", "ner": [["hierarchically-structured terminology", 15, 16, "Material"], ["US National Library of Medicine MeSH thesaurus", 23, 29, "Material"]], "relations": [[23, 29, 15, 16, "HYPONYM-OF"]]},
{"sentence": "We compare the lexically-induced relations with the original MeSH relations : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis ofthe ` new ' relations not present in the MeSH .", "ner": [["lexically-induced relations", 3, 4, "OtherScientificTerm"], ["MeSH relations", 8, 9, "OtherScientificTerm"], ["recall and precision metrics", 19, 22, "Metric"], ["MeSH", 40, 40, "OtherScientificTerm"]], "relations": [[3, 4, 8, 9, "COMPARE"], [19, 22, 8, 9, "EVALUATE-FOR"]]},
{"sentence": "This analysis shows , on the one hand , the limits of the lexical structuring method .", "ner": [["lexical structuring method", 13, 15, "Method"]], "relations": []},
{"sentence": "On the other hand , it also reveals some specific structuring choices and naming conventions made by the MeSH designers , and emphasizes ontological commitments that can not be left to automatic structuring .", "ner": [["MeSH", 18, 18, "OtherScientificTerm"], ["automatic structuring", 31, 32, "Task"]], "relations": []},
{"sentence": "In order to boost the translation quality of EBMT based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and , in addition , the language model of an in-domain monolingual corpus .", "ner": [["EBMT", 8, 8, "Method"], ["small-sized bilingual corpus", 12, 14, "Material"], ["out-of-domain bilingual corpus", 19, 21, "Material"], ["language model", 28, 29, "Method"], ["in-domain monolingual corpus", 32, 34, "Material"]], "relations": [[12, 14, 8, 8, "USED-FOR"], [19, 21, 8, 8, "USED-FOR"], [28, 29, 8, 8, "USED-FOR"], [32, 34, 28, 29, "USED-FOR"]]},
{"sentence": "We conducted experiments with an EBMT system .", "ner": [["EBMT system", 5, 6, "Method"]], "relations": []},
{"sentence": "The two evaluation measures of the BLEU score and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .", "ner": [["evaluation measures", 2, 3, "Metric"], ["BLEU score", 6, 7, "Metric"], ["NIST score", 10, 11, "Metric"], ["out-of-domain bilingual corpus", 18, 20, "Material"], ["language model", 27, 28, "Method"]], "relations": [[2, 3, 18, 20, "USED-FOR"], [2, 3, 27, 28, "EVALUATE-FOR"], [6, 7, 2, 3, "HYPONYM-OF"], [6, 7, 10, 11, "CONJUNCTION"], [10, 11, 2, 3, "HYPONYM-OF"]]},
{"sentence": "Diagrams are common tools for representing complex concepts , relationships and events , often when it would be difficult to portray the same information with natural images .", "ner": [["Diagrams", 0, 0, "Method"], ["complex concepts", 6, 7, "OtherScientificTerm"], ["relationships", 9, 9, "OtherScientificTerm"], ["events", 11, 11, "OtherScientificTerm"], ["natural images", 25, 26, "Material"]], "relations": [[0, 0, 6, 7, "USED-FOR"], [0, 0, 9, 9, "USED-FOR"], [0, 0, 11, 11, "USED-FOR"], [6, 7, 9, 9, "CONJUNCTION"], [9, 9, 11, 11, "CONJUNCTION"]]},
{"sentence": "Understanding natural images has been extensively studied in computer vision , while diagram understanding has received little attention .", "ner": [["Understanding natural images", 0, 2, "Task"], ["natural images", 1, 2, "Material"], ["computer vision", 8, 9, "Task"], ["diagram understanding", 12, 13, "Task"]], "relations": [[0, 2, 8, 9, "PART-OF"], [0, 2, 12, 13, "COMPARE"]]},
{"sentence": "In this paper , we study the problem of diagram interpretation and reasoning , the challenging task of identifying the structure of a diagram and the semantics of its constituents and their relationships .", "ner": [["diagram interpretation and reasoning", 9, 12, "Task"], ["task", 16, 16, "Generic"], ["structure of a diagram", 20, 23, "OtherScientificTerm"]], "relations": [[16, 16, 20, 23, "USED-FOR"]]},
{"sentence": "We introduce Diagram Parse Graphs -LRB- DPG -RRB- as our representation to model the structure of diagrams .", "ner": [["Diagram Parse Graphs -LRB- DPG -RRB-", 2, 7, "Method"], ["structure of diagrams", 14, 16, "OtherScientificTerm"]], "relations": [[2, 7, 14, 16, "USED-FOR"]]},
{"sentence": "We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering .", "ner": [["syntactic parsing of diagrams", 2, 5, "Task"], ["DPGs", 10, 10, "Method"], ["semantic interpretation and reasoning of diagrams", 15, 20, "Task"], ["diagram question answering", 25, 27, "Task"]], "relations": [[2, 5, 10, 10, "USED-FOR"], [15, 20, 25, 27, "USED-FOR"]]},
{"sentence": "We devise an LSTM-based method for syntactic parsing of diagrams and introduce a DPG-based attention model for diagram question answering .", "ner": [["LSTM-based method", 3, 4, "Method"], ["syntactic parsing of diagrams", 6, 9, "Task"], ["DPG-based attention model", 13, 15, "Method"], ["diagram question answering", 17, 19, "Task"]], "relations": [[3, 4, 6, 9, "USED-FOR"], [13, 15, 17, 19, "USED-FOR"]]},
{"sentence": "We compile a new dataset of diagrams with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers .", "ner": [["dataset", 4, 4, "Generic"], ["diagrams", 6, 6, "Material"]], "relations": [[6, 6, 4, 4, "FEATURE-OF"]]},
{"sentence": "Our results show the significance of our models for syntactic parsing and question answering in diagrams using DPGs .", "ner": [["models", 7, 7, "Generic"], ["syntactic parsing and question answering in diagrams", 9, 15, "Task"], ["DPGs", 17, 17, "Method"]], "relations": [[7, 7, 9, 15, "USED-FOR"], [17, 17, 7, 7, "USED-FOR"]]},
{"sentence": "Detecting fine-grained subtle changes among a scene is critically important in practice .", "ner": [["Detecting fine-grained subtle changes among a scene", 0, 6, "Task"]], "relations": []},
{"sentence": "Previous change detection methods , focusing on detecting large-scale significant changes , can not do this well .", "ner": [["change detection methods", 1, 3, "Method"], ["detecting large-scale significant changes", 7, 10, "Task"]], "relations": [[1, 3, 7, 10, "USED-FOR"]]},
{"sentence": "This paper proposes a feasible end-to-end approach to this challenging problem .", "ner": [["end-to-end approach", 5, 6, "Method"], ["problem", 10, 10, "Generic"]], "relations": [[5, 6, 10, 10, "USED-FOR"]]},
{"sentence": "We start from active camera relocation that quickly relocates camera to nearly the same pose and position of the last time observation .", "ner": [["active camera relocation", 3, 5, "Method"]], "relations": []},
{"sentence": "To guarantee detection sensitivity and accuracy of minute changes , in an observation , we capture a group of images under multiple illuminations , which need only to be roughly aligned to the last time lighting conditions .", "ner": [["detection sensitivity", 2, 3, "Metric"], ["accuracy", 5, 5, "Metric"], ["illuminations", 22, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related factors , i.e. , normal-aware lighting difference , camera geometry correction flow , and real scene change mask .", "ner": [["fine-grained change detection", 7, 9, "Task"], ["joint optimization problem", 12, 14, "Task"], ["factors", 18, 18, "Generic"], ["normal-aware lighting difference", 22, 24, "OtherScientificTerm"], ["camera geometry correction flow", 26, 29, "OtherScientificTerm"], ["real scene change mask", 32, 35, "OtherScientificTerm"]], "relations": [[12, 14, 7, 9, "USED-FOR"], [18, 18, 12, 14, "FEATURE-OF"], [22, 24, 18, 18, "HYPONYM-OF"], [22, 24, 26, 29, "CONJUNCTION"], [26, 29, 18, 18, "HYPONYM-OF"], [26, 29, 32, 35, "CONJUNCTION"], [32, 35, 18, 18, "HYPONYM-OF"]]},
{"sentence": "We solve the three factors in a coarse-to-fine manner and achieve reliable change decision by rank minimization .", "ner": [["factors", 4, 4, "Generic"], ["coarse-to-fine manner", 7, 8, "OtherScientificTerm"], ["change decision", 12, 13, "OtherScientificTerm"], ["rank minimization", 15, 16, "Method"]], "relations": [[7, 8, 4, 4, "USED-FOR"], [15, 16, 12, 13, "USED-FOR"]]},
{"sentence": "We build three real-world datasets to benchmark fine-grained change detection of misaligned scenes under varied multiple lighting conditions .", "ner": [["real-world datasets", 3, 4, "Material"], ["fine-grained change detection of misaligned scenes", 7, 12, "Task"], ["varied multiple lighting conditions", 14, 17, "OtherScientificTerm"]], "relations": [[3, 4, 7, 12, "EVALUATE-FOR"], [14, 17, 7, 12, "FEATURE-OF"]]},
{"sentence": "Extensive experiments show the superior performance of our approach over state-of-the-art change detection methods and its ability to distinguish real scene changes from false ones caused by lighting variations .", "ner": [["approach", 8, 8, "Generic"], ["change detection methods", 11, 13, "Method"], ["real scene changes", 19, 21, "OtherScientificTerm"], ["lighting variations", 27, 28, "OtherScientificTerm"]], "relations": [[8, 8, 11, 13, "COMPARE"], [8, 8, 19, 21, "USED-FOR"]]},
{"sentence": "Automatic evaluation metrics for Machine Translation -LRB- MT -RRB- systems , such as BLEU or NIST , are now well established .", "ner": [["Automatic evaluation metrics", 0, 2, "Metric"], ["Machine Translation -LRB- MT -RRB- systems", 4, 9, "Task"], ["BLEU", 13, 13, "Metric"], ["NIST", 15, 15, "Metric"]], "relations": [[0, 2, 4, 9, "EVALUATE-FOR"], [13, 13, 0, 2, "HYPONYM-OF"], [13, 13, 15, 15, "CONJUNCTION"], [15, 15, 0, 2, "HYPONYM-OF"]]},
{"sentence": "Yet , they are scarcely used for the assessment of language pairs like English-Chinese or English-Japanese , because of the word segmentation problem .", "ner": [["they", 2, 2, "Generic"], ["assessment of language pairs", 8, 11, "Task"], ["language pairs", 10, 11, "OtherScientificTerm"], ["English-Chinese", 13, 13, "OtherScientificTerm"], ["English-Japanese", 15, 15, "OtherScientificTerm"], ["word segmentation problem", 20, 22, "Task"]], "relations": [[2, 2, 8, 11, "USED-FOR"], [13, 13, 10, 11, "HYPONYM-OF"], [13, 13, 15, 15, "CONJUNCTION"], [15, 15, 10, 11, "HYPONYM-OF"]]},
{"sentence": "This study establishes the equivalence between the standard use of BLEU in word n-grams and its application at the character level .", "ner": [["BLEU", 10, 10, "Metric"], ["word n-grams", 12, 13, "Method"], ["character level", 19, 20, "OtherScientificTerm"]], "relations": [[10, 10, 12, 13, "USED-FOR"], [10, 10, 19, 20, "USED-FOR"], [12, 13, 19, 20, "CONJUNCTION"]]},
{"sentence": "The use of BLEU at the character level eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical MT systems which usually segment their outputs .", "ner": [["BLEU", 3, 3, "Metric"], ["character level", 6, 7, "OtherScientificTerm"], ["word segmentation problem", 10, 12, "Task"], ["it", 14, 14, "Generic"], ["commercial systems", 21, 22, "Method"], ["statistical MT systems", 31, 33, "Method"]], "relations": [[3, 3, 6, 7, "USED-FOR"], [3, 3, 10, 12, "USED-FOR"], [14, 14, 21, 22, "EVALUATE-FOR"], [14, 14, 31, 33, "EVALUATE-FOR"], [21, 22, 31, 33, "COMPARE"]]},
{"sentence": "This paper proposes a series of modifications to the left corner parsing algorithm for context-free grammars .", "ner": [["left corner parsing algorithm", 9, 12, "Method"], ["context-free grammars", 14, 15, "Method"]], "relations": [[9, 12, 14, 15, "USED-FOR"]]},
{"sentence": "It is argued that the resulting algorithm is both efficient and flexible and is , therefore , a good choice for the parser used in a natural language interface .", "ner": [["algorithm", 6, 6, "Generic"], ["parser", 22, 22, "Method"], ["natural language interface", 26, 28, "Task"]], "relations": [[6, 6, 22, 22, "USED-FOR"], [22, 22, 26, 28, "USED-FOR"]]},
{"sentence": "This paper presents a novel statistical singing voice conversion -LRB- SVC -RRB- technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .", "ner": [["statistical singing voice conversion -LRB- SVC -RRB- technique", 5, 12, "Method"], ["direct waveform modification", 14, 16, "Method"], ["spectrum differential", 20, 21, "OtherScientificTerm"], ["voice timbre", 25, 26, "OtherScientificTerm"], ["vocoder", 40, 40, "Method"], ["converted singing voice waveforms", 43, 46, "OtherScientificTerm"]], "relations": [[14, 16, 5, 12, "USED-FOR"], [20, 21, 14, 16, "USED-FOR"], [20, 21, 25, 26, "USED-FOR"], [40, 40, 43, 46, "USED-FOR"]]},
{"sentence": "SVC makes it possible to convert singing voice characteristics of an arbitrary source singer into those of an arbitrary target singer .", "ner": [["SVC", 0, 0, "Method"], ["singing voice characteristics", 6, 8, "OtherScientificTerm"]], "relations": [[0, 0, 6, 8, "USED-FOR"]]},
{"sentence": "However , speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice due to various factors , such as analysis and modeling errors in the vocoder-based framework .", "ner": [["speech quality", 2, 3, "Metric"], ["converted singing voice", 6, 8, "OtherScientificTerm"], ["natural singing voice", 17, 19, "OtherScientificTerm"], ["vocoder-based framework", 33, 34, "Method"]], "relations": [[2, 3, 6, 8, "EVALUATE-FOR"], [2, 3, 17, 19, "EVALUATE-FOR"], [6, 8, 17, 19, "COMPARE"]]},
{"sentence": "To alleviate this degradation , we propose a statistical conversion process that directly modifies the signal in the waveform domain by estimating the difference in the spectra of the source and target singers ' singing voices .", "ner": [["statistical conversion process", 8, 10, "Method"], ["spectra", 26, 26, "OtherScientificTerm"]], "relations": []},
{"sentence": "The differential spectral feature is directly estimated using a differential Gaussian mixture model -LRB- GMM -RRB- that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC .", "ner": [["differential spectral feature", 1, 3, "OtherScientificTerm"], ["differential Gaussian mixture model -LRB- GMM -RRB-", 9, 15, "Method"], ["GMM", 23, 23, "Method"], ["conversion model", 27, 28, "Method"], ["SVC", 32, 32, "Method"]], "relations": [[9, 15, 1, 3, "USED-FOR"], [23, 23, 9, 15, "USED-FOR"], [23, 23, 27, 28, "USED-FOR"], [27, 28, 32, 32, "USED-FOR"]]},
{"sentence": "The experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC .", "ner": [["method", 7, 7, "Generic"], ["speech quality", 14, 15, "Metric"], ["conversion accuracy of singer identity", 24, 28, "Metric"], ["SVC", 33, 33, "Method"]], "relations": [[7, 7, 33, 33, "COMPARE"], [14, 15, 7, 7, "EVALUATE-FOR"], [14, 15, 33, 33, "EVALUATE-FOR"], [24, 28, 7, 7, "EVALUATE-FOR"], [24, 28, 33, 33, "USED-FOR"]]},
{"sentence": "In this paper , we state the challenges of high-level program execution in multi-agent settings .", "ner": [["high-level program execution", 9, 11, "Task"]], "relations": []},
{"sentence": "We first introduce high-level program execution and the related work .", "ner": [["high-level program execution", 3, 5, "Task"]], "relations": []},
{"sentence": "Then we describe the completed work , the future work and its approaches .", "ner": [], "relations": []},
{"sentence": "We conclude with the expected contributions of our research .", "ner": [], "relations": []},
{"sentence": "During late-2013 through early-2014 NIST coordinated a special i-vector challenge based on data used in previous NIST Speaker Recognition Evaluations -LRB- SREs -RRB- .", "ner": [["i-vector challenge", 8, 9, "Material"], ["NIST Speaker Recognition Evaluations -LRB- SREs -RRB-", 16, 22, "Material"]], "relations": [[16, 22, 8, 9, "USED-FOR"]]},
{"sentence": "Unlike evaluations in the SRE series , the i-vector challenge was run entirely online and used fixed-length feature vectors projected into a low-dimensional space -LRB- i-vectors -RRB- rather than audio recordings .", "ner": [["SRE series", 4, 5, "Material"], ["i-vector challenge", 8, 9, "Material"], ["fixed-length feature vectors", 16, 18, "OtherScientificTerm"], ["low-dimensional space -LRB- i-vectors -RRB-", 22, 26, "OtherScientificTerm"], ["audio recordings", 29, 30, "Material"]], "relations": [[16, 18, 22, 26, "USED-FOR"], [22, 26, 8, 9, "USED-FOR"], [29, 30, 22, 26, "COMPARE"]]},
{"sentence": "These changes made the challenge more readily accessible , especially to participants from outside the audio processing field .", "ner": [["audio processing field", 15, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "Compared to the 2012 SRE , the i-vector challenge saw an increase in the number of participants by nearly a factor of two , and a two orders of magnitude increase in the number of systems submitted for evaluation .", "ner": [["SRE", 4, 4, "Material"], ["i-vector challenge", 7, 8, "Material"]], "relations": [[4, 4, 7, 8, "COMPARE"]]},
{"sentence": "Initial results indicate the leading system achieved an approximate 37 % improvement relative to the baseline system .", "ner": [["leading system", 4, 5, "Generic"], ["baseline system", 15, 16, "Generic"]], "relations": [[4, 5, 15, 16, "COMPARE"]]},
{"sentence": "Theoretical research in the area of machine translation usually involves the search for and creation of an appropriate formalism .", "ner": [["machine translation", 6, 7, "Task"], ["formalism", 18, 18, "Generic"]], "relations": [[18, 18, 6, 7, "USED-FOR"]]},
{"sentence": "An important issue in this respect is the way in which the compositionality of translation is to be defined .", "ner": [["translation", 14, 14, "Task"]], "relations": []},
{"sentence": "In this paper , we will introduce the anaphoric component of the Mimo formalism .", "ner": [["anaphoric component", 8, 9, "Method"], ["Mimo formalism", 12, 13, "Method"]], "relations": [[8, 9, 12, 13, "PART-OF"]]},
{"sentence": "It makes the definition and translation of anaphoric relations possible , relations which are usually problematic for systems that adhere to strict compositionality .", "ner": [["translation of anaphoric relations", 5, 8, "Task"], ["strict compositionality", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "In Mimo , the translation of anaphoric relations is compositional .", "ner": [["Mimo", 1, 1, "OtherScientificTerm"], ["translation of anaphoric relations", 4, 7, "Task"]], "relations": [[1, 1, 4, 7, "USED-FOR"]]},
{"sentence": "The anaphoric component is used to define linguistic phenomena such as wh-movement , the passive and the binding of reflexives and pronouns mono-lingually .", "ner": [["anaphoric component", 1, 2, "Method"], ["linguistic phenomena", 7, 8, "OtherScientificTerm"], ["wh-movement", 11, 11, "OtherScientificTerm"], ["the passive and the binding of reflexives and pronouns", 13, 21, "OtherScientificTerm"]], "relations": [[1, 2, 7, 8, "USED-FOR"], [11, 11, 7, 8, "HYPONYM-OF"], [11, 11, 13, 21, "CONJUNCTION"], [13, 21, 7, 8, "HYPONYM-OF"]]},
{"sentence": "The actual working of the component will be shown in this paper by means of a detailed discussion of wh-movement .", "ner": [["wh-movement", 19, 19, "OtherScientificTerm"]], "relations": []},
{"sentence": "A recognition scheme that scales efficiently to a large number of objects is presented .", "ner": [["recognition scheme", 1, 2, "Method"]], "relations": []},
{"sentence": "The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD 's .", "ner": [["efficiency", 1, 1, "Metric"], ["quality", 3, 3, "Metric"], ["CD-covers", 12, 12, "Material"], ["database", 15, 15, "Generic"], ["images of popular music CD 's", 18, 23, "Material"]], "relations": [[1, 1, 3, 3, "CONJUNCTION"], [18, 23, 15, 15, "FEATURE-OF"]]},
{"sentence": "The scheme builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to background clutter and occlusion .", "ner": [["scheme", 1, 1, "Generic"], ["indexing descriptors", 7, 8, "Method"], ["local regions", 11, 12, "OtherScientificTerm"], ["background clutter", 18, 19, "OtherScientificTerm"], ["occlusion", 21, 21, "OtherScientificTerm"]], "relations": [[1, 1, 18, 19, "USED-FOR"], [1, 1, 21, 21, "USED-FOR"], [7, 8, 1, 1, "USED-FOR"], [11, 12, 7, 8, "USED-FOR"], [18, 19, 21, 21, "CONJUNCTION"]]},
{"sentence": "The local region descriptors are hierarchically quantized in a vocabulary tree .", "ner": [["local region descriptors", 1, 3, "Method"], ["vocabulary tree", 9, 10, "OtherScientificTerm"]], "relations": [[9, 10, 1, 3, "USED-FOR"]]},
{"sentence": "The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently , which we show experimentally leads to a dramatic improvement in retrieval quality .", "ner": [["vocabulary tree", 1, 2, "OtherScientificTerm"], ["retrieval quality", 25, 26, "Metric"]], "relations": []},
{"sentence": "The most significant property of the scheme is that the tree directly defines the quantization .", "ner": [["tree", 10, 10, "OtherScientificTerm"], ["quantization", 14, 14, "OtherScientificTerm"]], "relations": []},
{"sentence": "The quantization and the indexing are therefore fully integrated , essentially being one and the same .", "ner": [["quantization", 1, 1, "OtherScientificTerm"], ["indexing", 4, 4, "OtherScientificTerm"]], "relations": [[1, 1, 4, 4, "CONJUNCTION"]]},
{"sentence": "The recognition quality is evaluated through retrieval on a database with ground truth , showing the power of the vocabulary tree approach , going as high as 1 million images .", "ner": [["recognition quality", 1, 2, "Metric"], ["retrieval", 6, 6, "Task"], ["database with ground truth", 9, 12, "Material"], ["vocabulary tree approach", 19, 21, "Method"]], "relations": [[1, 2, 19, 21, "EVALUATE-FOR"], [6, 6, 1, 2, "EVALUATE-FOR"], [9, 12, 6, 6, "USED-FOR"]]},
{"sentence": "This paper presents a method for blind estimation of reverberation times in reverberant enclosures .", "ner": [["method", 4, 4, "Generic"], ["blind estimation of reverberation times", 6, 10, "Task"], ["reverberant enclosures", 12, 13, "OtherScientificTerm"]], "relations": [[4, 4, 6, 10, "USED-FOR"], [12, 13, 6, 10, "FEATURE-OF"]]},
{"sentence": "The proposed algorithm is based on a statistical model of short-term log-energy sequences for echo-free speech .", "ner": [["algorithm", 2, 2, "Generic"], ["statistical model of short-term log-energy sequences", 7, 12, "Method"], ["echo-free speech", 14, 15, "Material"]], "relations": [[7, 12, 2, 2, "USED-FOR"], [7, 12, 14, 15, "USED-FOR"]]},
{"sentence": "Given a speech utterance recorded in a reverberant room , it computes a Maximum Likelihood estimate of the room full-band reverberation time .", "ner": [["Maximum Likelihood estimate of the room full-band reverberation time", 13, 21, "Method"]], "relations": []},
{"sentence": "The estimation method is shown to require little data and to perform satisfactorily .", "ner": [["estimation method", 1, 2, "Generic"]], "relations": []},
{"sentence": "The method has been successfully applied to robust automatic speech recognition in reverberant environments by model selection .", "ner": [["method", 1, 1, "Generic"], ["robust automatic speech recognition", 7, 10, "Task"], ["reverberant environments", 12, 13, "OtherScientificTerm"], ["model selection", 15, 16, "Method"]], "relations": [[1, 1, 7, 10, "USED-FOR"], [12, 13, 7, 10, "FEATURE-OF"], [15, 16, 1, 1, "USED-FOR"]]},
{"sentence": "For this application , the reverberation time is first estimated from the reverberated speech utterance to be recognized .", "ner": [["application", 2, 2, "Generic"], ["reverberation time", 5, 6, "OtherScientificTerm"], ["reverberated speech utterance", 12, 14, "OtherScientificTerm"]], "relations": [[12, 14, 5, 6, "USED-FOR"]]},
{"sentence": "The estimation is then used to select the best acoustic model out of a library of models trained in various artificial re-verberant conditions .", "ner": [["estimation", 1, 1, "Generic"], ["acoustic model", 9, 10, "Method"], ["models", 16, 16, "Generic"], ["artificial re-verberant conditions", 20, 22, "OtherScientificTerm"]], "relations": [[1, 1, 9, 10, "USED-FOR"], [9, 10, 16, 16, "PART-OF"], [20, 22, 16, 16, "FEATURE-OF"]]},
{"sentence": "Speech recognition experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard channel normaliza-tion techniques .", "ner": [["Speech recognition", 0, 1, "Task"], ["simulated and real reverberant environments", 4, 8, "OtherScientificTerm"], ["approach", 14, 14, "Generic"], ["channel normaliza-tion techniques", 18, 20, "Method"]], "relations": [[0, 1, 14, 14, "EVALUATE-FOR"], [0, 1, 18, 20, "EVALUATE-FOR"], [4, 8, 0, 1, "FEATURE-OF"], [18, 20, 14, 14, "COMPARE"]]},
{"sentence": "Computer programs so far have not fared well in modeling language acquisition .", "ner": [["Computer programs", 0, 1, "Generic"], ["language acquisition", 10, 11, "Task"]], "relations": []},
{"sentence": "For one thing , learning methodology applicable in general domains does not readily lend itself in the linguistic domain .", "ner": [["learning methodology", 4, 5, "Method"], ["general domains", 8, 9, "Material"], ["linguistic domain", 17, 18, "Material"]], "relations": [[4, 5, 8, 9, "USED-FOR"], [8, 9, 17, 18, "COMPARE"]]},
{"sentence": "For another , linguistic representation used by language processing systems is not geared to learning .", "ner": [["linguistic representation", 3, 4, "Method"], ["language processing systems", 7, 9, "Method"]], "relations": [[3, 4, 7, 9, "USED-FOR"]]},
{"sentence": "We introduced a new linguistic representation , the Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB- -LSB- Zernik88 -RSB- , to facilitate language acquisition .", "ner": [["linguistic representation", 4, 5, "Method"], ["Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB-", 8, 14, "OtherScientificTerm"], ["language acquisition", 21, 22, "Task"]], "relations": [[4, 5, 21, 22, "USED-FOR"], [8, 14, 4, 5, "HYPONYM-OF"], [8, 14, 21, 22, "USED-FOR"]]},
{"sentence": "From this , a language learning model was implemented in the program RINA , which enhances its own lexical hierarchy by processing examples in context .", "ner": [["language learning model", 4, 6, "Method"], ["RINA", 12, 12, "Method"], ["lexical hierarchy", 18, 19, "OtherScientificTerm"]], "relations": [[4, 6, 12, 12, "PART-OF"]]},
{"sentence": "We identified two tasks : First , how linguistic concepts are acquired from training examples and organized in a hierarchy ; this task was discussed in previous papers -LSB- Zernik87 -RSB- .", "ner": [["linguistic concepts", 8, 9, "OtherScientificTerm"], ["hierarchy", 19, 19, "Generic"]], "relations": [[8, 9, 19, 19, "PART-OF"]]},
{"sentence": "Second , we show in this paper how a lexical hierarchy is used in predicting new linguistic concepts .", "ner": [["lexical hierarchy", 9, 10, "OtherScientificTerm"], ["linguistic concepts", 16, 17, "OtherScientificTerm"]], "relations": [[9, 10, 16, 17, "USED-FOR"]]},
{"sentence": "Thus , a program does not stall even in the presence of a lexical unknown , and a hypothesis can be produced for covering that lexical gap .", "ner": [["program", 3, 3, "Generic"]], "relations": []},
{"sentence": "This paper presents a novel ensemble learning approach to resolving German pronouns .", "ner": [["ensemble learning approach", 5, 7, "Method"], ["German pronouns", 10, 11, "OtherScientificTerm"]], "relations": [[5, 7, 10, 11, "USED-FOR"]]},
{"sentence": "Boosting , the method in question , combines the moderately accurate hypotheses of several classifiers to form a highly accurate one .", "ner": [["Boosting", 0, 0, "Method"], ["classifiers", 14, 14, "Method"]], "relations": []},
{"sentence": "Experiments show that this approach is superior to a single decision-tree classifier .", "ner": [["approach", 4, 4, "Generic"], ["decision-tree classifier", 10, 11, "Method"]], "relations": [[4, 4, 10, 11, "COMPARE"]]},
{"sentence": "Furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .", "ner": [["standalone system", 5, 6, "Method"], ["pronouns", 9, 9, "OtherScientificTerm"], ["unannotated text", 11, 12, "Material"], ["preprocessing modules", 20, 21, "Method"], ["manual annotation process", 25, 27, "Task"]], "relations": [[5, 6, 9, 9, "USED-FOR"], [5, 6, 11, 12, "USED-FOR"], [9, 9, 11, 12, "PART-OF"], [20, 21, 5, 6, "USED-FOR"], [20, 21, 25, 27, "USED-FOR"]]},
{"sentence": "Although the system performs well within a limited textual domain , further research is needed to make it effective for open-domain question answering and text summarisation .", "ner": [["system", 2, 2, "Generic"], ["textual domain", 8, 9, "Material"], ["it", 17, 17, "Generic"], ["open-domain question answering", 20, 22, "Task"], ["text summarisation", 24, 25, "Task"]], "relations": [[8, 9, 2, 2, "EVALUATE-FOR"], [8, 9, 20, 22, "COMPARE"], [17, 17, 20, 22, "USED-FOR"], [17, 17, 24, 25, "USED-FOR"], [20, 22, 24, 25, "CONJUNCTION"]]},
{"sentence": "In this paper , we compare the performance of a state-of-the-art statistical parser -LRB- Bikel , 2004 -RRB- in parsing written and spoken language and in generating sub-categorization cues from written and spoken language .", "ner": [["statistical parser", 11, 12, "Method"], ["parsing written and spoken language", 19, 23, "Task"], ["written and spoken language", 20, 23, "Material"], ["generating sub-categorization cues", 26, 28, "Task"], ["written and spoken language", 30, 33, "Material"]], "relations": [[11, 12, 19, 23, "USED-FOR"], [11, 12, 26, 28, "USED-FOR"], [19, 23, 26, 28, "CONJUNCTION"], [30, 33, 26, 28, "USED-FOR"]]},
{"sentence": "Although Bikel 's parser achieves a higher accuracy for parsing written language , it achieves a higher accuracy when extracting subcategorization cues from spoken language .", "ner": [["Bikel 's parser", 1, 3, "Method"], ["accuracy", 7, 7, "Metric"], ["parsing written language", 9, 11, "Task"], ["written language", 10, 11, "Material"], ["it", 13, 13, "Generic"], ["accuracy", 17, 17, "Metric"], ["subcategorization cues", 20, 21, "OtherScientificTerm"], ["spoken language", 23, 24, "Material"]], "relations": [[1, 3, 9, 11, "USED-FOR"], [7, 7, 1, 3, "EVALUATE-FOR"], [13, 13, 20, 21, "USED-FOR"], [17, 17, 13, 13, "EVALUATE-FOR"], [20, 21, 23, 24, "PART-OF"]]},
{"sentence": "Our experiments also show that current technology for extracting subcategorization frames initially designed for written texts works equally well for spoken language .", "ner": [["technology", 6, 6, "Generic"], ["extracting subcategorization frames", 8, 10, "Task"], ["written texts", 14, 15, "Material"], ["spoken language", 20, 21, "Material"]], "relations": [[6, 6, 8, 10, "USED-FOR"], [6, 6, 20, 21, "USED-FOR"], [8, 10, 14, 15, "USED-FOR"], [14, 15, 20, 21, "COMPARE"]]},
{"sentence": "Additionally , we explore the utility of punctuation in helping parsing and extraction of subcategorization cues .", "ner": [["punctuation", 7, 7, "OtherScientificTerm"], ["parsing", 10, 10, "Method"], ["extraction of subcategorization cues", 12, 15, "Task"]], "relations": [[7, 7, 10, 10, "USED-FOR"], [7, 7, 12, 15, "USED-FOR"]]},
{"sentence": "Our experiments show that punctuation is of little help in parsing spoken language and extracting subcategorization cues from spoken language .", "ner": [["punctuation", 4, 4, "OtherScientificTerm"], ["parsing spoken language", 10, 12, "Task"], ["spoken language", 11, 12, "Material"], ["extracting subcategorization cues", 14, 16, "Task"], ["subcategorization cues", 15, 16, "OtherScientificTerm"], ["spoken language", 18, 19, "Material"]], "relations": [[10, 12, 14, 16, "CONJUNCTION"], [15, 16, 18, 19, "PART-OF"], [18, 19, 14, 16, "USED-FOR"]]},
{"sentence": "This indicates that there is no need to add punctuation in transcribing spoken corpora simply in order to help parsers .", "ner": [["punctuation", 9, 9, "OtherScientificTerm"], ["spoken corpora", 12, 13, "Material"], ["parsers", 19, 19, "Method"]], "relations": []},
{"sentence": "This paper proposes an alignment adaptation approach to improve domain-specific -LRB- in-domain -RRB- word alignment .", "ner": [["alignment adaptation approach", 4, 6, "Method"], ["domain-specific -LRB- in-domain -RRB- word alignment", 9, 14, "Task"]], "relations": [[4, 6, 9, 14, "USED-FOR"]]},
{"sentence": "The basic idea of alignment adaptation is to use out-of-domain corpus to improve in-domain word alignment results .", "ner": [["alignment adaptation", 4, 5, "Method"], ["out-of-domain corpus", 9, 10, "Material"], ["in-domain word alignment", 13, 15, "Task"]], "relations": [[4, 5, 13, 15, "USED-FOR"], [9, 10, 4, 5, "USED-FOR"]]},
{"sentence": "In this paper , we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively , and then interpolate these two models to improve the domain-specific word alignment .", "ner": [["statistical word alignment models", 8, 11, "Method"], ["large-scale out-of-domain corpus", 14, 16, "Material"], ["small-scale in-domain corpus", 19, 21, "Material"], ["models", 29, 29, "Generic"], ["domain-specific word alignment", 33, 35, "Task"]], "relations": [[14, 16, 8, 11, "USED-FOR"], [14, 16, 19, 21, "CONJUNCTION"], [19, 21, 8, 11, "USED-FOR"], [29, 29, 33, 35, "USED-FOR"]]},
{"sentence": "Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .", "ner": [["approach", 5, 5, "Generic"], ["domain-specific word alignment", 7, 9, "Task"], ["precision", 14, 14, "Metric"], ["recall", 16, 16, "Metric"], ["relative error rate reduction", 20, 23, "Metric"], ["state-of-the-art technologies", 31, 32, "Generic"]], "relations": [[5, 5, 7, 9, "USED-FOR"], [5, 5, 31, 32, "COMPARE"], [14, 14, 5, 5, "EVALUATE-FOR"], [14, 14, 16, 16, "CONJUNCTION"], [16, 16, 5, 5, "EVALUATE-FOR"], [20, 23, 5, 5, "EVALUATE-FOR"], [20, 23, 31, 32, "EVALUATE-FOR"]]},
{"sentence": "With performance above 97 % accuracy for newspaper text , part of speech -LRB- pos -RRB- tagging might be considered a solved problem .", "ner": [["accuracy", 5, 5, "Metric"], ["newspaper text", 7, 8, "Material"], ["part of speech -LRB- pos -RRB- tagging", 10, 16, "Task"]], "relations": [[5, 5, 10, 16, "EVALUATE-FOR"], [7, 8, 10, 16, "EVALUATE-FOR"]]},
{"sentence": "Previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance .", "ner": [["parser", 7, 7, "Method"], ["pos tag ambiguity", 10, 12, "OtherScientificTerm"]], "relations": [[7, 7, 10, 12, "USED-FOR"]]},
{"sentence": "However , for grammar formalisms which use more fine-grained grammatical categories , for example tag and ccg , tagging accuracy is much lower .", "ner": [["grammar formalisms", 3, 4, "Method"], ["fine-grained grammatical categories", 8, 10, "OtherScientificTerm"], ["tag", 14, 14, "OtherScientificTerm"], ["ccg", 16, 16, "OtherScientificTerm"], ["tagging accuracy", 18, 19, "Metric"]], "relations": [[8, 10, 3, 4, "USED-FOR"], [14, 14, 8, 10, "HYPONYM-OF"], [16, 16, 8, 10, "HYPONYM-OF"], [18, 19, 3, 4, "EVALUATE-FOR"]]},
{"sentence": "In fact , for these formalisms , premature ambiguity resolution makes parsing infeasible .", "ner": [["formalisms", 5, 5, "Generic"], ["premature ambiguity resolution", 7, 9, "OtherScientificTerm"], ["parsing", 11, 11, "Task"]], "relations": [[11, 11, 5, 5, "USED-FOR"]]},
{"sentence": "We describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing .", "ner": [["multi-tagging approach", 3, 4, "Method"], ["lexical category ambiguity", 11, 13, "OtherScientificTerm"], ["ccg parsing", 18, 19, "Task"]], "relations": [[3, 4, 18, 19, "USED-FOR"], [11, 13, 3, 4, "FEATURE-OF"]]},
{"sentence": "We extend this multi-tagging approach to the pos level to overcome errors introduced by automatically assigned pos tags .", "ner": [["multi-tagging approach", 3, 4, "Method"], ["pos level", 7, 8, "OtherScientificTerm"], ["pos tags", 16, 17, "OtherScientificTerm"]], "relations": [[3, 4, 7, 8, "USED-FOR"]]},
{"sentence": "Although pos tagging accuracy seems high , maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging .", "ner": [["pos tagging accuracy", 1, 3, "Metric"], ["pos tag ambiguity", 9, 11, "OtherScientificTerm"], ["language processing pipeline", 14, 16, "Method"], ["ccg supertagging", 21, 22, "Method"]], "relations": [[9, 11, 14, 16, "FEATURE-OF"], [9, 11, 21, 22, "USED-FOR"]]},
{"sentence": "We previously presented a framework for segmentation of complex scenes using multiple physical hypotheses for simple image regions .", "ner": [["framework", 4, 4, "Generic"], ["segmentation of complex scenes", 6, 9, "Task"], ["physical hypotheses", 12, 13, "OtherScientificTerm"], ["simple image regions", 15, 17, "OtherScientificTerm"]], "relations": [[4, 4, 6, 9, "USED-FOR"], [12, 13, 4, 4, "USED-FOR"], [12, 13, 15, 17, "USED-FOR"]]},
{"sentence": "A consequence of that framework was a proposal for a new approach to the segmentation of complex scenes into regions corresponding to coherent surfaces rather than merely regions of similar color .", "ner": [["framework", 4, 4, "Generic"], ["approach", 11, 11, "Generic"], ["segmentation of complex scenes", 14, 17, "Task"], ["coherent surfaces", 22, 23, "OtherScientificTerm"], ["regions of similar color", 27, 30, "OtherScientificTerm"]], "relations": [[4, 4, 11, 11, "USED-FOR"], [11, 11, 14, 17, "USED-FOR"], [22, 23, 27, 30, "COMPARE"]]},
{"sentence": "Herein we present an implementation of this new approach and show example segmentations for scenes containing multi-colored piece-wise uniform objects .", "ner": [["approach", 8, 8, "Generic"], ["segmentations", 12, 12, "OtherScientificTerm"], ["scenes", 14, 14, "Material"], ["multi-colored piece-wise uniform objects", 16, 19, "Material"]], "relations": [[12, 12, 14, 14, "USED-FOR"], [16, 19, 14, 14, "FEATURE-OF"]]},
{"sentence": "Using our approach we are able to intelligently segment scenes with objects of greater complexity than previous physics-based segmentation algorithms .", "ner": [["approach", 2, 2, "Generic"], ["complexity", 14, 14, "Metric"], ["physics-based segmentation algorithms", 17, 19, "Method"]], "relations": [[2, 2, 17, 19, "COMPARE"]]},
{"sentence": "The results show that by using general physical models we obtain segmentations that correspond more closely to coherent surfaces in the scene than segmentations found using only color .", "ner": [["physical models", 7, 8, "Method"], ["coherent surfaces", 17, 18, "OtherScientificTerm"]], "relations": []},
{"sentence": "SmartKom is a multimodal dialog system that combines speech , gesture , and mimics input and output .", "ner": [["SmartKom", 0, 0, "Method"], ["multimodal dialog system", 3, 5, "Method"], ["speech", 8, 8, "Material"], ["gesture", 10, 10, "Material"]], "relations": [[0, 0, 3, 5, "HYPONYM-OF"], [8, 8, 3, 5, "USED-FOR"], [8, 8, 10, 10, "CONJUNCTION"], [10, 10, 3, 5, "USED-FOR"]]},
{"sentence": "Spontaneous speech understanding is combined with the video-based recognition of natural gestures .", "ner": [["Spontaneous speech understanding", 0, 2, "Task"], ["video-based recognition of natural gestures", 7, 11, "Task"]], "relations": [[0, 2, 7, 11, "CONJUNCTION"]]},
{"sentence": "One of the major scientific goals of SmartKom is to design new computational methods for the seamless integration and mutual disambiguation of multimodal input and output on a semantic and pragmatic level .", "ner": [["SmartKom", 7, 7, "Method"], ["computational methods", 12, 13, "Method"], ["integration and mutual disambiguation of multimodal input and output", 17, 25, "Task"], ["semantic and pragmatic level", 28, 31, "OtherScientificTerm"]], "relations": [[7, 7, 12, 13, "USED-FOR"], [12, 13, 17, 25, "USED-FOR"], [28, 31, 17, 25, "FEATURE-OF"]]},
{"sentence": "SmartKom is based on the situated delegation-oriented dialog paradigm , in which the user delegates a task to a virtual communication assistant , visualized as a lifelike character on a graphical display .", "ner": [["SmartKom", 0, 0, "OtherScientificTerm"], ["situated delegation-oriented dialog paradigm", 5, 8, "Method"], ["virtual communication assistant", 19, 21, "OtherScientificTerm"], ["graphical display", 30, 31, "OtherScientificTerm"]], "relations": [[5, 8, 0, 0, "USED-FOR"]]},
{"sentence": "We describe the SmartKom architecture , the use of an XML-based markup language for multimodal content , and some of the distinguishing features of the first fully operational SmartKom demonstrator .", "ner": [["SmartKom architecture", 3, 4, "Method"], ["XML-based markup language", 10, 12, "OtherScientificTerm"], ["multimodal content", 14, 15, "Material"], ["SmartKom demonstrator", 28, 29, "Method"]], "relations": [[10, 12, 14, 15, "USED-FOR"]]},
{"sentence": "We present a single-image highlight removal method that incorporates illumination-based constraints into image in-painting .", "ner": [["single-image highlight removal method", 3, 6, "Method"], ["illumination-based constraints", 9, 10, "OtherScientificTerm"], ["image in-painting", 12, 13, "Task"]], "relations": [[3, 6, 12, 13, "USED-FOR"], [9, 10, 12, 13, "PART-OF"]]},
{"sentence": "Unlike occluded image regions filled by traditional inpainting , highlight pixels contain some useful information for guiding the inpainting process .", "ner": [["occluded image regions", 1, 3, "OtherScientificTerm"], ["inpainting", 7, 7, "OtherScientificTerm"], ["highlight pixels", 9, 10, "OtherScientificTerm"], ["inpainting process", 18, 19, "Task"]], "relations": []},
{"sentence": "Constraints provided by observed pixel colors , highlight color analysis and illumination color uniformity are employed in our method to improve estimation of the underlying diffuse color .", "ner": [["Constraints", 0, 0, "OtherScientificTerm"], ["pixel colors", 4, 5, "OtherScientificTerm"], ["highlight color analysis", 7, 9, "OtherScientificTerm"], ["illumination color uniformity", 11, 13, "OtherScientificTerm"], ["method", 18, 18, "Generic"], ["estimation of the underlying diffuse color", 21, 26, "OtherScientificTerm"]], "relations": [[0, 0, 18, 18, "USED-FOR"], [4, 5, 7, 9, "CONJUNCTION"], [7, 9, 11, 13, "CONJUNCTION"], [18, 18, 21, 26, "USED-FOR"]]},
{"sentence": "The inclusion of these illumination constraints allows for better recovery of shading and textures by inpainting .", "ner": [["illumination constraints", 4, 5, "OtherScientificTerm"], ["recovery of shading and textures", 9, 13, "OtherScientificTerm"], ["inpainting", 15, 15, "Task"]], "relations": [[4, 5, 9, 13, "USED-FOR"], [15, 15, 9, 13, "USED-FOR"]]},
{"sentence": "Experimental results are given to demonstrate the performance of our method .", "ner": [["method", 10, 10, "Method"]], "relations": []},
{"sentence": "In this paper , we propose a novel method , called local non-negative matrix factorization -LRB- LNMF -RRB- , for learning spatially localized , parts-based subspace representation of visual patterns .", "ner": [["method", 8, 8, "Generic"], ["local non-negative matrix factorization -LRB- LNMF -RRB-", 11, 17, "Method"], ["spatially localized , parts-based subspace representation of visual patterns", 21, 29, "Task"]], "relations": [[8, 8, 21, 29, "USED-FOR"]]},
{"sentence": "An objective function is defined to impose lo-calization constraint , in addition to the non-negativity constraint in the standard NMF -LSB- 1 -RSB- .", "ner": [["objective function", 1, 2, "OtherScientificTerm"], ["lo-calization constraint", 7, 8, "OtherScientificTerm"], ["non-negativity constraint", 14, 15, "OtherScientificTerm"], ["NMF", 19, 19, "Method"]], "relations": [[1, 2, 7, 8, "USED-FOR"], [14, 15, 19, 19, "PART-OF"]]},
{"sentence": "This gives a set of bases which not only allows a non-subtractive -LRB- part-based -RRB- representation of images but also manifests localized features .", "ner": [["non-subtractive -LRB- part-based -RRB- representation of images", 11, 17, "Method"], ["localized features", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "An algorithm is presented for the learning of such basis components .", "ner": [["algorithm", 1, 1, "Generic"], ["learning", 6, 6, "Task"]], "relations": [[1, 1, 6, 6, "USED-FOR"]]},
{"sentence": "Experimental results are presented to compare LNMF with the NMF and PCA methods for face representation and recognition , which demonstrates advantages of LNMF .", "ner": [["LNMF", 6, 6, "Method"], ["NMF and PCA methods", 9, 12, "Method"], ["face representation and recognition", 14, 17, "Task"], ["LNMF", 23, 23, "Method"]], "relations": [[6, 6, 9, 12, "COMPARE"], [6, 6, 14, 17, "USED-FOR"], [9, 12, 14, 17, "USED-FOR"], [14, 17, 23, 23, "EVALUATE-FOR"]]},
{"sentence": "Helping end users build and check process models is a challenge for many science and engineering fields .", "ner": [["process models", 6, 7, "Method"], ["science and engineering fields", 13, 16, "Task"]], "relations": []},
{"sentence": "Many AI researchers have investigated useful ways of verifying and validating knowledge bases for ontologies and rules , but it is not easy to directly apply them to checking process models .", "ner": [["knowledge bases", 11, 12, "Material"], ["ontologies", 14, 14, "OtherScientificTerm"], ["rules", 16, 16, "OtherScientificTerm"], ["checking process models", 28, 30, "Task"], ["process models", 29, 30, "Method"]], "relations": [[14, 14, 16, 16, "CONJUNCTION"]]},
{"sentence": "Other techniques developed for checking and refining planning knowledge tend to focus on automated plan generation rather than helping users author process information .", "ner": [["checking and refining planning knowledge", 4, 8, "Task"], ["automated plan generation", 13, 15, "Task"]], "relations": [[4, 8, 13, 15, "USED-FOR"]]},
{"sentence": "In this paper , we propose a complementary approach which helps users author and check process models .", "ner": [["complementary approach", 7, 8, "Generic"], ["process models", 15, 16, "Method"]], "relations": [[7, 8, 15, 16, "USED-FOR"]]},
{"sentence": "Our system , called KANAL , relates pieces of information in process models among themselves and to the existing KB , analyzing how different pieces of input are put together to achieve some effect .", "ner": [["system", 1, 1, "Generic"], ["KANAL", 4, 4, "Method"], ["process models", 11, 12, "Method"], ["KB", 19, 19, "Material"]], "relations": []},
{"sentence": "It builds interdepen-dency models from this analysis and uses them to find errors and propose fixes .", "ner": [["It", 0, 0, "Generic"], ["interdepen-dency models", 2, 3, "Method"], ["them", 9, 9, "Generic"], ["errors", 12, 12, "OtherScientificTerm"], ["fixes", 15, 15, "OtherScientificTerm"]], "relations": [[2, 3, 0, 0, "USED-FOR"], [9, 9, 12, 12, "USED-FOR"], [9, 9, 15, 15, "USED-FOR"]]},
{"sentence": "Our initial evaluation shows that KANAL was able to find most of the errors in the process models and suggest useful fixes including the fixes that directly point to the sources of the errors .", "ner": [["KANAL", 5, 5, "Method"], ["process models", 16, 17, "Method"]], "relations": []},
{"sentence": "In this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque .", "ner": [["machine learning techniques", 9, 11, "Method"], ["comma checker", 15, 16, "Method"], ["grammar checker", 22, 23, "Method"], ["Basque", 25, 25, "Material"]], "relations": [[9, 11, 15, 16, "USED-FOR"], [15, 16, 22, 23, "PART-OF"], [22, 23, 25, 25, "USED-FOR"]]},
{"sentence": "After several experiments , and trained with a little corpus of 100,000 words , the system guesses correctly not placing commas with a precision of 96 % and a recall of 98 % .", "ner": [["system", 15, 15, "Generic"], ["precision", 23, 23, "Metric"], ["recall", 29, 29, "Metric"]], "relations": [[23, 23, 15, 15, "EVALUATE-FOR"], [29, 29, 15, 15, "EVALUATE-FOR"]]},
{"sentence": "It also gets a precision of 70 % and a recall of 49 % in the task of placing commas .", "ner": [["It", 0, 0, "Generic"], ["precision", 4, 4, "Metric"], ["recall", 10, 10, "Metric"], ["placing commas", 18, 19, "Task"]], "relations": [[0, 0, 18, 19, "USED-FOR"], [4, 4, 0, 0, "EVALUATE-FOR"], [10, 10, 0, 0, "EVALUATE-FOR"]]},
{"sentence": "Finally , we have shown that these results can be improved using a bigger and a more homogeneous corpus to train , that is , a bigger corpus written by one unique author .", "ner": [], "relations": []},
{"sentence": "The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the discourse relations in Czech .", "ner": [["language corpus annotation scenario", 11, 14, "Material"], ["discourse relations", 17, 18, "OtherScientificTerm"], ["Czech", 20, 20, "Material"]], "relations": [[11, 14, 17, 18, "USED-FOR"], [20, 20, 17, 18, "FEATURE-OF"]]},
{"sentence": "We primarily focus on the description of the syntactically motivated relations in discourse , basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the Penn Discourse Treebank 2 .", "ner": [["syntactically motivated relations in discourse", 8, 12, "OtherScientificTerm"], ["Prague Dependency Treebank 2.0", 23, 26, "Material"], ["Penn Discourse Treebank 2", 29, 32, "Material"]], "relations": [[23, 26, 8, 12, "USED-FOR"], [23, 26, 29, 32, "CONJUNCTION"], [29, 32, 8, 12, "USED-FOR"]]},
{"sentence": "Our aim is to revisit the present-day syntactico-semantic -LRB- tectogrammatical -RRB- annotation in the Prague Dependency Treebank , extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new , discourse level of annotation .", "ner": [["syntactico-semantic -LRB- tectogrammatical -RRB- annotation", 7, 11, "OtherScientificTerm"], ["Prague Dependency Treebank", 14, 16, "Material"], ["it", 19, 19, "Generic"], ["sentence-boundary-crossing representation", 25, 26, "Task"], ["discourse level of annotation", 34, 37, "Task"]], "relations": [[7, 11, 14, 16, "PART-OF"], [19, 19, 25, 26, "USED-FOR"], [19, 19, 34, 37, "USED-FOR"]]},
{"sentence": "In this paper , we propose a feasible process of such a transfer , comparing the possibilities the Praguian dependency-based approach offers with the Penn discourse annotation based primarily on the analysis and classification of discourse connectives .", "ner": [["Praguian dependency-based approach", 18, 20, "Method"], ["Penn discourse annotation", 24, 26, "Method"], ["analysis and classification of discourse connectives", 31, 36, "Method"]], "relations": [[24, 26, 18, 20, "COMPARE"], [31, 36, 18, 20, "EVALUATE-FOR"], [31, 36, 24, 26, "EVALUATE-FOR"]]},
{"sentence": "Regression-based techniques have shown promising results for people counting in crowded scenes .", "ner": [["Regression-based techniques", 0, 1, "Method"], ["people counting in crowded scenes", 7, 11, "Task"]], "relations": [[0, 1, 7, 11, "USED-FOR"]]},
{"sentence": "However , most existing techniques require expensive and laborious data annotation for model training .", "ner": [["techniques", 4, 4, "Generic"], ["data annotation", 9, 10, "Task"], ["model training", 12, 13, "Task"]], "relations": [[9, 10, 4, 4, "USED-FOR"], [9, 10, 12, 13, "USED-FOR"]]},
{"sentence": "In this study , we propose to address this problem from three perspectives : -LRB- 1 -RRB- Instead of exhaustively annotating every single frame , the most informative frames are selected for annotation automatically and actively .", "ner": [["informative frames", 27, 28, "OtherScientificTerm"], ["annotation", 32, 32, "Task"]], "relations": []},
{"sentence": "-LRB- 2 -RRB- Rather than learning from only labelled data , the abundant unlabelled data are exploited .", "ner": [["labelled data", 8, 9, "Generic"], ["abundant unlabelled data", 12, 14, "Material"]], "relations": [[8, 9, 12, 14, "COMPARE"]]},
{"sentence": "-LRB- 3 -RRB- Labelled data from other scenes are employed to further alleviate the burden for data annotation .", "ner": [["data annotation", 16, 17, "Task"]], "relations": []},
{"sentence": "All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning , by exploiting the underlying geometric structure of crowd patterns via manifold analysis .", "ner": [["unified active and semi-supervised regression framework", 7, 12, "Method"], ["transfer learning", 17, 18, "Method"], ["geometric structure of crowd patterns", 24, 28, "OtherScientificTerm"], ["manifold analysis", 30, 31, "Method"]], "relations": [[7, 12, 17, 18, "USED-FOR"], [24, 28, 7, 12, "USED-FOR"], [30, 31, 24, 28, "USED-FOR"]]},
{"sentence": "Extensive experiments validate the effectiveness of our approach .", "ner": [["approach", 7, 7, "Generic"]], "relations": []},
{"sentence": "Representing images with layers has many important applications , such as video compression , motion analysis , and 3D scene analysis .", "ner": [["Representing images with layers", 0, 3, "Method"], ["applications", 7, 7, "Generic"], ["video compression", 11, 12, "Task"], ["motion analysis", 14, 15, "Task"], ["3D scene analysis", 18, 20, "Task"]], "relations": [[0, 3, 7, 7, "USED-FOR"], [11, 12, 7, 7, "HYPONYM-OF"], [11, 12, 14, 15, "CONJUNCTION"], [14, 15, 7, 7, "HYPONYM-OF"], [14, 15, 18, 20, "CONJUNCTION"], [18, 20, 7, 7, "HYPONYM-OF"]]},
{"sentence": "This paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .", "ner": [["approach", 4, 4, "Generic"], ["layers", 8, 8, "OtherScientificTerm"], ["images", 10, 10, "Material"], ["homographies", 18, 18, "OtherScientificTerm"], ["planar patches", 21, 22, "OtherScientificTerm"], ["scene", 25, 25, "OtherScientificTerm"], ["low dimensional linear subspace", 28, 31, "OtherScientificTerm"]], "relations": [[4, 4, 8, 8, "USED-FOR"], [8, 8, 10, 10, "PART-OF"], [21, 22, 25, 25, "PART-OF"]]},
{"sentence": "Layers in the input images will be mapped in the subspace , where it is proven that they form well-defined clusters and can be reliably identified by a simple mean-shift based clustering algorithm .", "ner": [["Layers", 0, 0, "OtherScientificTerm"], ["images", 4, 4, "Material"], ["subspace", 10, 10, "OtherScientificTerm"], ["clusters", 20, 20, "OtherScientificTerm"], ["mean-shift based clustering algorithm", 29, 32, "Method"]], "relations": [[0, 0, 4, 4, "PART-OF"], [4, 4, 20, 20, "USED-FOR"], [29, 32, 20, 20, "USED-FOR"]]},
{"sentence": "Global optimality is achieved since all valid regions are simultaneously taken into account , and noise can be effectively reduced by enforcing the subspace constraint .", "ner": [["Global optimality", 0, 1, "Task"], ["regions", 7, 7, "OtherScientificTerm"], ["noise", 15, 15, "OtherScientificTerm"], ["subspace constraint", 23, 24, "OtherScientificTerm"]], "relations": [[23, 24, 15, 15, "USED-FOR"]]},
{"sentence": "Good layer descriptions are shown to be extracted in the experimental results .", "ner": [], "relations": []},
{"sentence": "The construction of causal graphs from non-experimental data rests on a set of constraints that the graph structure imposes on all probability distributions compatible with the graph .", "ner": [["construction of causal graphs", 1, 4, "Task"], ["non-experimental data", 6, 7, "Material"], ["constraints", 13, 13, "OtherScientificTerm"], ["graph structure", 16, 17, "OtherScientificTerm"], ["probability distributions", 21, 22, "OtherScientificTerm"], ["graph", 26, 26, "OtherScientificTerm"]], "relations": [[6, 7, 1, 4, "USED-FOR"], [21, 22, 26, 26, "FEATURE-OF"]]},
{"sentence": "These constraints are of two types : conditional inde-pendencies and algebraic constraints , first noted by Verma .", "ner": [["constraints", 1, 1, "Generic"], ["conditional inde-pendencies", 7, 8, "OtherScientificTerm"], ["algebraic constraints", 10, 11, "OtherScientificTerm"]], "relations": [[7, 8, 1, 1, "HYPONYM-OF"], [10, 11, 1, 1, "HYPONYM-OF"]]},
{"sentence": "While conditional independencies are well studied and frequently used in causal induction algorithms , Verma constraints are still poorly understood , and rarely applied .", "ner": [["conditional independencies", 1, 2, "OtherScientificTerm"], ["causal induction algorithms", 10, 12, "Method"], ["Verma constraints", 14, 15, "OtherScientificTerm"]], "relations": [[1, 2, 10, 12, "USED-FOR"], [14, 15, 1, 2, "COMPARE"]]},
{"sentence": "In this paper we examine a special subset of Verma constraints which are easy to understand , easy to identify and easy to apply ; they arise from '' dormant independencies , '' namely , conditional independencies that hold in interventional distributions .", "ner": [["Verma constraints", 9, 10, "OtherScientificTerm"], ["they", 25, 25, "Generic"], ["dormant independencies", 29, 30, "OtherScientificTerm"], ["conditional independencies", 35, 36, "OtherScientificTerm"], ["interventional distributions", 40, 41, "OtherScientificTerm"]], "relations": [[29, 30, 35, 36, "CONJUNCTION"], [35, 36, 40, 41, "FEATURE-OF"]]},
{"sentence": "We give a complete algorithm for determining if a dormant independence between two sets of variables is entailed by the causal graph , such that this independence is identifiable , in other words if it resides in an interventional distribution that can be predicted without resorting to interventions .", "ner": [["algorithm", 4, 4, "Generic"], ["dormant independence", 9, 10, "OtherScientificTerm"], ["variables", 15, 15, "OtherScientificTerm"], ["causal graph", 20, 21, "OtherScientificTerm"], ["independence", 26, 26, "OtherScientificTerm"], ["it", 34, 34, "Generic"], ["interventional distribution", 38, 39, "OtherScientificTerm"], ["interventions", 47, 47, "OtherScientificTerm"]], "relations": [[4, 4, 9, 10, "USED-FOR"], [38, 39, 34, 34, "FEATURE-OF"]]},
{"sentence": "We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .", "ner": [["dormant independencies", 6, 7, "OtherScientificTerm"], ["model testing", 9, 10, "Task"], ["induction", 12, 12, "Task"], ["algorithm", 16, 16, "Generic"], ["constraints", 19, 19, "Generic"], ["dormant independencies", 22, 23, "OtherScientificTerm"], ["extraneous edges", 26, 27, "OtherScientificTerm"], ["causal graph", 31, 32, "OtherScientificTerm"]], "relations": [[6, 7, 9, 10, "USED-FOR"], [6, 7, 12, 12, "USED-FOR"], [9, 10, 12, 12, "CONJUNCTION"], [16, 16, 26, 27, "USED-FOR"], [19, 19, 16, 16, "USED-FOR"], [26, 27, 31, 32, "PART-OF"]]},
{"sentence": "With the recent popularity of animated GIFs on social media , there is need for ways to index them with rich meta-data .", "ner": [["animated GIFs", 5, 6, "Material"], ["social media", 8, 9, "Material"], ["rich meta-data", 20, 21, "Material"]], "relations": [[8, 9, 5, 6, "FEATURE-OF"]]},
{"sentence": "To advance research on animated GIF understanding , we collected a new dataset , Tumblr GIF -LRB- TGIF -RRB- , with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing .", "ner": [["animated GIF understanding", 4, 6, "Task"], ["dataset", 12, 12, "Generic"], ["Tumblr GIF -LRB- TGIF -RRB-", 14, 18, "Material"], ["animated GIFs", 22, 23, "Material"], ["natural language descriptions", 28, 30, "Material"], ["crowdsourcing", 33, 33, "Method"]], "relations": [[12, 12, 4, 6, "USED-FOR"], [28, 30, 22, 23, "CONJUNCTION"], [33, 33, 28, 30, "USED-FOR"]]},
{"sentence": "The motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate natural language descriptions for animated GIFs or video clips .", "ner": [["image sequence description systems", 11, 14, "Task"], ["natural language descriptions", 22, 24, "Material"], ["animated GIFs", 26, 27, "Material"], ["video clips", 29, 30, "Material"]], "relations": [[22, 24, 26, 27, "USED-FOR"], [22, 24, 29, 30, "USED-FOR"], [26, 27, 29, 30, "CONJUNCTION"]]},
{"sentence": "To ensure a high quality dataset , we developed a series of novel quality controls to validate free-form text input from crowd-workers .", "ner": [["quality controls", 13, 14, "Method"], ["free-form text input", 17, 19, "Material"]], "relations": [[13, 14, 17, 19, "USED-FOR"]]},
{"sentence": "We show that there is unambiguous association between visual content and natural language descriptions in our dataset , making it an ideal benchmark for the visual content captioning task .", "ner": [["visual content", 8, 9, "Material"], ["natural language descriptions", 11, 13, "Material"], ["dataset", 16, 16, "Generic"], ["it", 19, 19, "Generic"], ["visual content captioning task", 25, 28, "Task"]], "relations": [[8, 9, 11, 13, "CONJUNCTION"], [8, 9, 16, 16, "PART-OF"], [11, 13, 16, 16, "PART-OF"], [19, 19, 25, 28, "EVALUATE-FOR"]]},
{"sentence": "We perform extensive statistical analyses to compare our dataset to existing image and video description datasets .", "ner": [["statistical analyses", 3, 4, "OtherScientificTerm"], ["dataset", 8, 8, "Generic"], ["image and video description datasets", 11, 15, "Material"]], "relations": [[8, 8, 11, 15, "COMPARE"]]},
{"sentence": "Next , we provide baseline results on the animated GIF description task , using three representative techniques : nearest neighbor , statistical machine translation , and recurrent neural networks .", "ner": [["animated GIF description task", 8, 11, "Task"], ["representative techniques", 15, 16, "Generic"], ["nearest neighbor", 18, 19, "Method"], ["statistical machine translation", 21, 23, "Method"], ["recurrent neural networks", 26, 28, "Method"]], "relations": [[15, 16, 8, 11, "USED-FOR"], [18, 19, 15, 16, "HYPONYM-OF"], [18, 19, 21, 23, "CONJUNCTION"], [21, 23, 15, 16, "HYPONYM-OF"], [21, 23, 26, 28, "CONJUNCTION"], [26, 28, 15, 16, "HYPONYM-OF"]]},
{"sentence": "Finally , we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description .", "ner": [["animated GIF description dataset", 9, 12, "Material"], ["automatic movie description", 17, 19, "Task"]], "relations": [[9, 12, 17, 19, "USED-FOR"]]},
{"sentence": "Systemic grammar has been used for AI text generation work in the past , but the implementations have tended be ad hoc or inefficient .", "ner": [["Systemic grammar", 0, 1, "Method"], ["AI text generation", 6, 8, "Task"], ["implementations", 16, 16, "Generic"]], "relations": [[0, 1, 6, 8, "USED-FOR"]]},
{"sentence": "This paper presents an approach to systemic text generation where AI problem solving techniques are applied directly to an unadulterated systemic grammar .", "ner": [["approach", 4, 4, "Generic"], ["text generation", 7, 8, "Task"], ["AI problem solving techniques", 10, 13, "Method"], ["systemic grammar", 20, 21, "Method"]], "relations": [[4, 4, 7, 8, "USED-FOR"], [10, 13, 20, 21, "USED-FOR"]]},
{"sentence": "This approach is made possible by a special relationship between systemic grammar and problem solving : both are organized primarily as choosing from alternatives .", "ner": [["approach", 1, 1, "Generic"], ["systemic grammar", 10, 11, "Method"], ["problem solving", 13, 14, "Method"]], "relations": [[10, 11, 13, 14, "CONJUNCTION"]]},
{"sentence": "The result is simple , efficient text generation firmly based in a linguistic theory .", "ner": [["text generation", 6, 7, "Task"], ["linguistic theory", 12, 13, "Method"]], "relations": [[12, 13, 6, 7, "USED-FOR"]]},
{"sentence": "In this paper a novel solution to automatic and unsupervised word sense induction -LRB- WSI -RRB- is introduced .", "ner": [["solution", 5, 5, "Generic"], ["automatic and unsupervised word sense induction -LRB- WSI -RRB-", 7, 15, "Task"]], "relations": [[5, 5, 7, 15, "USED-FOR"]]},
{"sentence": "It represents an instantiation of the one sense per collocation observation -LRB- Gale et al. , 1992 -RRB- .", "ner": [["It", 0, 0, "Generic"], ["one sense per collocation observation", 6, 10, "Method"]], "relations": [[0, 0, 6, 10, "HYPONYM-OF"]]},
{"sentence": "Like most existing approaches it utilizes clustering of word co-occurrences .", "ner": [["it", 4, 4, "Generic"], ["clustering of word co-occurrences", 6, 9, "Method"]], "relations": [[6, 9, 4, 4, "USED-FOR"]]},
{"sentence": "This approach differs from other approaches to WSI in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .", "ner": [["approach", 1, 1, "Generic"], ["approaches", 5, 5, "Generic"], ["WSI", 7, 7, "Task"], ["it", 10, 10, "Generic"], ["one sense per collocation observation", 16, 20, "Method"], ["triplets of words", 23, 25, "OtherScientificTerm"]], "relations": [[1, 1, 5, 5, "COMPARE"], [1, 1, 7, 7, "USED-FOR"], [5, 5, 7, 7, "USED-FOR"], [10, 10, 16, 20, "USED-FOR"], [23, 25, 10, 10, "USED-FOR"]]},
{"sentence": "The combination with a two-step clustering process using sentence co-occurrences as features allows for accurate results .", "ner": [["two-step clustering process", 4, 6, "Method"], ["sentence co-occurrences", 8, 9, "OtherScientificTerm"], ["features", 11, 11, "OtherScientificTerm"]], "relations": [[8, 9, 4, 6, "USED-FOR"]]},
{"sentence": "Additionally , a novel and likewise automatic and unsupervised evaluation method inspired by Schutze 's -LRB- 1992 -RRB- idea of evaluation of word sense disambiguation algorithms is employed .", "ner": [["automatic and unsupervised evaluation method", 6, 10, "Metric"], ["word sense disambiguation algorithms", 22, 25, "Method"]], "relations": [[6, 10, 22, 25, "EVALUATE-FOR"]]},
{"sentence": "Offering advantages like reproducability and independency of a given biased gold standard it also enables automatic parameter optimization of the WSI algorithm .", "ner": [["automatic parameter optimization", 15, 17, "Method"], ["WSI algorithm", 20, 21, "Method"]], "relations": [[15, 17, 20, 21, "USED-FOR"]]},
{"sentence": "This abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of it in computer aided second language learning .", "ner": [["natural language system", 4, 6, "Method"], ["ungrammatical input", 11, 12, "OtherScientificTerm"], ["it", 21, 21, "Generic"], ["computer aided second language learning", 23, 27, "Task"]], "relations": [[4, 6, 11, 12, "USED-FOR"], [21, 21, 23, 27, "USED-FOR"]]},
{"sentence": "However , this is not the only area in which the principles of the system might be used , and the aim in building it was simply to demonstrate the workability of the general mechanism , and provide a framework for assessing developments of it .", "ner": [["this", 2, 2, "Generic"], ["system", 14, 14, "Generic"], ["it", 24, 24, "Generic"], ["it", 44, 44, "Generic"]], "relations": [[14, 14, 2, 2, "USED-FOR"]]},
{"sentence": "In a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , electrical system voltage level , and ambient sound .", "ner": [["measurable signals", 8, 9, "Material"], ["frequency components", 11, 12, "Method"], ["rotational speed of the engine", 16, 20, "OtherScientificTerm"], ["vibrations", 27, 27, "OtherScientificTerm"], ["electrical system voltage level", 29, 32, "OtherScientificTerm"], ["ambient sound", 35, 36, "OtherScientificTerm"]], "relations": [[11, 12, 8, 9, "USED-FOR"], [11, 12, 16, 20, "FEATURE-OF"], [27, 27, 8, 9, "HYPONYM-OF"], [27, 27, 29, 32, "CONJUNCTION"], [29, 32, 8, 9, "HYPONYM-OF"], [29, 32, 35, 36, "CONJUNCTION"], [35, 36, 8, 9, "HYPONYM-OF"]]},
{"sentence": "These signals could potentially be used to estimate the speed and related states of the vehicle .", "ner": [["signals", 1, 1, "Generic"], ["speed and related states of the vehicle", 9, 15, "Task"]], "relations": [[1, 1, 9, 15, "USED-FOR"]]},
{"sentence": "Unfortunately , such estimates would typically require the relations -LRB- scale factors -RRB- between the frequency components and the speed for different gears to be known .", "ner": [["frequency components", 15, 16, "Method"], ["speed", 19, 19, "OtherScientificTerm"], ["gears", 22, 22, "OtherScientificTerm"]], "relations": [[15, 16, 19, 19, "CONJUNCTION"], [19, 19, 22, 22, "FEATURE-OF"]]},
{"sentence": "Consequently , in this article we look at the problem of estimating these gear scale factors from training data consisting only of speed measurements and measurements of the signal in question .", "ner": [["gear scale factors", 13, 15, "OtherScientificTerm"], ["training data", 17, 18, "Generic"], ["speed measurements", 22, 23, "OtherScientificTerm"]], "relations": [[17, 18, 13, 15, "USED-FOR"]]},
{"sentence": "The estimation problem is formulated as a maximum likelihood estimation problem and heuristics is used to find initial values for a numerical evaluation of the estimator .", "ner": [["estimation problem", 1, 2, "Task"], ["maximum likelihood estimation problem", 7, 10, "Task"], ["heuristics", 12, 12, "Method"], ["numerical evaluation of the estimator", 21, 25, "Task"], ["estimator", 25, 25, "Generic"]], "relations": [[7, 10, 1, 2, "USED-FOR"], [12, 12, 21, 25, "USED-FOR"]]},
{"sentence": "Finally , a measurement campaign is conducted and the functionality of the estimation method is verified on real data .", "ner": [["estimation method", 12, 13, "Method"], ["real data", 17, 18, "Material"]], "relations": [[17, 18, 12, 13, "EVALUATE-FOR"]]},
{"sentence": "LPC based speech coders operating at bit rates below 3.0 kbits/sec are usually associated with buzzy or metallic artefacts in the synthetic speech .", "ner": [["LPC based speech coders", 0, 3, "Method"], ["bit rates", 6, 7, "OtherScientificTerm"], ["buzzy or metallic artefacts", 15, 18, "OtherScientificTerm"], ["synthetic speech", 21, 22, "Material"]], "relations": [[6, 7, 0, 3, "FEATURE-OF"], [15, 18, 21, 22, "FEATURE-OF"]]},
{"sentence": "These are mainly attributable to the simplifying assumptions made about the excitation source , which are usually required to maintain such low bit rates .", "ner": [["excitation source", 11, 12, "OtherScientificTerm"], ["low bit rates", 21, 23, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper a new LPC vocoder is presented which splits the LPC excitation into two frequency bands using a variable cutoff frequency .", "ner": [["LPC vocoder", 5, 6, "Method"], ["LPC excitation", 12, 13, "OtherScientificTerm"], ["frequency bands", 16, 17, "OtherScientificTerm"], ["variable cutoff frequency", 20, 22, "OtherScientificTerm"]], "relations": [[20, 22, 12, 13, "USED-FOR"], [20, 22, 16, 17, "USED-FOR"]]},
{"sentence": "The lower band is responsible for representing the voiced parts of speech , whilst the upper band represents unvoiced speech .", "ner": [["voiced parts of speech", 8, 11, "Material"], ["unvoiced speech", 18, 19, "Material"]], "relations": []},
{"sentence": "In doing so the coder 's performance during both mixed voicing speech and speech containing acoustic noise is greatly improved , producing soft natural sounding speech .", "ner": [["coder", 4, 4, "Generic"], ["mixed voicing speech", 9, 11, "Material"], ["speech containing acoustic noise", 13, 16, "Material"], ["soft natural sounding speech", 22, 25, "Material"]], "relations": [[4, 4, 22, 25, "USED-FOR"], [9, 11, 4, 4, "USED-FOR"], [13, 16, 4, 4, "USED-FOR"]]},
{"sentence": "The paper also describes new parameter determination and quantisation techniques vital to the operation of this coder at such low bit rates .", "ner": [["parameter determination", 5, 6, "Method"], ["quantisation techniques", 8, 9, "Method"], ["coder", 16, 16, "Generic"], ["low bit rates", 19, 21, "OtherScientificTerm"]], "relations": [[5, 6, 8, 9, "CONJUNCTION"], [5, 6, 16, 16, "USED-FOR"], [8, 9, 16, 16, "USED-FOR"], [19, 21, 16, 16, "FEATURE-OF"]]},
{"sentence": "We consider a problem of blind source separation from a set of instantaneous linear mixtures , where the mixing matrix is unknown .", "ner": [["blind source separation", 5, 7, "Task"], ["instantaneous linear mixtures", 12, 14, "OtherScientificTerm"], ["mixing matrix", 18, 19, "OtherScientificTerm"]], "relations": [[12, 14, 5, 7, "USED-FOR"]]},
{"sentence": "It was discovered recently , that exploiting the sparsity of sources in an appropriate representation according to some signal dictionary , dramatically improves the quality of separation .", "ner": [["sparsity of sources", 8, 10, "OtherScientificTerm"], ["signal dictionary", 18, 19, "OtherScientificTerm"], ["quality of separation", 24, 26, "Metric"]], "relations": [[18, 19, 8, 10, "USED-FOR"], [24, 26, 8, 10, "EVALUATE-FOR"]]},
{"sentence": "In this work we use the property of multi scale transforms , such as wavelet or wavelet packets , to decompose signals into sets of local features with various degrees of sparsity .", "ner": [["multi scale transforms", 8, 10, "OtherScientificTerm"], ["wavelet or wavelet packets", 14, 17, "OtherScientificTerm"], ["local features", 25, 26, "OtherScientificTerm"], ["sparsity", 31, 31, "OtherScientificTerm"]], "relations": [[14, 17, 8, 10, "HYPONYM-OF"]]},
{"sentence": "We use this intrinsic property for selecting the best -LRB- most sparse -RRB- subsets of features for further separation .", "ner": [["features", 15, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "The performance of the algorithm is verified on noise-free and noisy data .", "ner": [["algorithm", 4, 4, "Generic"], ["noise-free and noisy data", 8, 11, "Material"]], "relations": [[8, 11, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "Experiments with simulated signals , musical sounds and images demonstrate significant improvement of separation quality over previously reported results .", "ner": [["simulated signals", 2, 3, "Material"], ["musical sounds", 5, 6, "Material"], ["images", 8, 8, "Material"], ["separation quality", 13, 14, "Metric"]], "relations": [[2, 3, 5, 6, "CONJUNCTION"], [2, 3, 13, 14, "EVALUATE-FOR"], [5, 6, 8, 8, "CONJUNCTION"], [5, 6, 13, 14, "EVALUATE-FOR"], [8, 8, 13, 14, "EVALUATE-FOR"]]},
{"sentence": "In this paper , we explore multilingual feature-level data sharing via Deep Neural Network -LRB- DNN -RRB- stacked bottleneck features .", "ner": [["multilingual feature-level data sharing", 6, 9, "Task"], ["Deep Neural Network -LRB- DNN -RRB- stacked bottleneck features", 11, 19, "Method"]], "relations": [[11, 19, 6, 9, "USED-FOR"]]},
{"sentence": "Given a set of available source languages , we apply language identification to pick the language most similar to the target language , for more efficient use of multilingual resources .", "ner": [["language identification", 10, 11, "Task"], ["multilingual resources", 28, 29, "Material"]], "relations": [[10, 11, 28, 29, "USED-FOR"]]},
{"sentence": "Our experiments with IARPA-Babel languages show that bottleneck features trained on the most similar source language perform better than those trained on all available source languages .", "ner": [["IARPA-Babel languages", 3, 4, "Material"], ["bottleneck features", 7, 8, "OtherScientificTerm"], ["those", 19, 19, "Generic"]], "relations": [[19, 19, 7, 8, "COMPARE"]]},
{"sentence": "Further analysis suggests that only data similar to the target language is useful for multilingual training .", "ner": [["data", 5, 5, "Generic"], ["multilingual training", 14, 15, "Task"]], "relations": [[5, 5, 14, 15, "USED-FOR"]]},
{"sentence": "This article introduces a bidirectional grammar generation system called feature structure-directed generation , developed for a dialogue translation system .", "ner": [["bidirectional grammar generation system", 4, 7, "Method"], ["feature structure-directed generation", 9, 11, "Method"], ["dialogue translation system", 16, 18, "Task"]], "relations": [[4, 7, 16, 18, "USED-FOR"], [9, 11, 4, 7, "HYPONYM-OF"], [9, 11, 16, 18, "USED-FOR"]]},
{"sentence": "The system utilizes typed feature structures to control the top-down derivation in a declarative way .", "ner": [["system", 1, 1, "Generic"], ["typed feature structures", 3, 5, "OtherScientificTerm"], ["top-down derivation", 9, 10, "OtherScientificTerm"]], "relations": [[3, 5, 1, 1, "USED-FOR"], [3, 5, 9, 10, "USED-FOR"]]},
{"sentence": "This generation system also uses disjunctive feature structures to reduce the number of copies of the derivation tree .", "ner": [["generation system", 1, 2, "Method"], ["disjunctive feature structures", 5, 7, "OtherScientificTerm"], ["derivation tree", 16, 17, "OtherScientificTerm"]], "relations": [[5, 7, 1, 2, "USED-FOR"], [5, 7, 16, 17, "USED-FOR"]]},
{"sentence": "The grammar for this generator is designed to properly generate the speaker 's intention in a telephone dialogue .", "ner": [["grammar", 1, 1, "Method"], ["generator", 4, 4, "Generic"], ["speaker 's intention", 11, 13, "OtherScientificTerm"], ["telephone dialogue", 16, 17, "OtherScientificTerm"]], "relations": [[1, 1, 4, 4, "USED-FOR"], [1, 1, 11, 13, "USED-FOR"], [16, 17, 11, 13, "FEATURE-OF"]]},
{"sentence": "Automatic image annotation is a newly developed and promising technique to provide semantic image retrieval via text descriptions .", "ner": [["Automatic image annotation", 0, 2, "Task"], ["semantic image retrieval", 12, 14, "Task"], ["text descriptions", 16, 17, "Material"]], "relations": [[0, 2, 12, 14, "USED-FOR"], [16, 17, 12, 14, "USED-FOR"]]},
{"sentence": "It concerns a process of automatically labeling the image contents with a pre-defined set of keywords which are exploited to represent the image semantics .", "ner": [["automatically labeling the image contents", 5, 9, "Task"], ["keywords", 15, 15, "OtherScientificTerm"], ["image semantics", 22, 23, "OtherScientificTerm"]], "relations": [[15, 15, 5, 9, "USED-FOR"], [15, 15, 22, 23, "USED-FOR"]]},
{"sentence": "A Maximum Entropy Model-based approach to the task of automatic image annotation is proposed in this paper .", "ner": [["Maximum Entropy Model-based approach", 1, 4, "Method"], ["automatic image annotation", 9, 11, "Task"]], "relations": [[1, 4, 9, 11, "USED-FOR"]]},
{"sentence": "In the phase of training , a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first ; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images .", "ner": [["training", 4, 4, "Task"], ["visual vocabulary", 8, 9, "OtherScientificTerm"], ["blob-tokens", 12, 12, "OtherScientificTerm"], ["image content", 16, 17, "OtherScientificTerm"], ["statistical relationship", 25, 26, "Generic"], ["blob-tokens", 31, 31, "OtherScientificTerm"], ["keywords", 33, 33, "OtherScientificTerm"], ["Maximum Entropy Model", 36, 38, "Method"]], "relations": [[8, 9, 16, 17, "USED-FOR"], [12, 12, 8, 9, "PART-OF"], [36, 38, 25, 26, "USED-FOR"]]},
{"sentence": "In the phase of annotation , for an unlabeled image , the most likely associated keywords are predicted in terms of the blob-token set extracted from the given image .", "ner": [["annotation", 4, 4, "Task"], ["keywords", 15, 15, "OtherScientificTerm"], ["blob-token set", 22, 23, "OtherScientificTerm"]], "relations": [[22, 23, 15, 15, "USED-FOR"]]},
{"sentence": "We carried out experiments on a medium-sized image collection with about 5000 images from Corel Photo CDs .", "ner": [["medium-sized image collection", 6, 8, "Material"], ["Corel Photo CDs", 14, 16, "Material"]], "relations": [[14, 16, 6, 8, "USED-FOR"]]},
{"sentence": "The experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 8 % in mean precision , showing a potential of the Maximum Entropy Model in the task of automatic image annotation .", "ner": [["annotation", 6, 6, "Task"], ["method", 10, 10, "Generic"], ["annotation methods", 14, 15, "Method"], ["mean precision", 21, 22, "Metric"], ["Maximum Entropy Model", 29, 31, "Method"], ["automatic image annotation", 36, 38, "Task"]], "relations": [[10, 10, 6, 6, "USED-FOR"], [10, 10, 14, 15, "COMPARE"], [14, 15, 6, 6, "USED-FOR"], [21, 22, 14, 15, "EVALUATE-FOR"], [29, 31, 36, 38, "USED-FOR"]]},
{"sentence": "Recent years have seen increasing research on extracting and using temporal information in natural language applications .", "ner": [["temporal information", 10, 11, "OtherScientificTerm"], ["natural language applications", 13, 15, "Task"]], "relations": []},
{"sentence": "However most of the works found in the literature have focused on identifying and understanding temporal expressions in newswire texts .", "ner": [["temporal expressions", 15, 16, "OtherScientificTerm"], ["temporal expressions in newswire texts", 15, 19, "OtherScientificTerm"], ["newswire texts", 18, 19, "Material"]], "relations": [[18, 19, 15, 16, "FEATURE-OF"]]},
{"sentence": "In this paper we report our work on anchoring temporal expressions in a novel genre , emails .", "ner": [["temporal expressions", 9, 10, "OtherScientificTerm"], ["emails", 16, 16, "Material"]], "relations": [[16, 16, 9, 10, "FEATURE-OF"]]},
{"sentence": "The highly under-specified nature of these expressions fits well with our constraint-based representation of time , Time Calculus for Natural Language -LRB- TCNL -RRB- .", "ner": [["expressions", 6, 6, "Generic"], ["constraint-based representation of time", 11, 14, "OtherScientificTerm"], ["Time Calculus for Natural Language -LRB- TCNL -RRB-", 16, 23, "OtherScientificTerm"]], "relations": [[16, 23, 11, 14, "HYPONYM-OF"]]},
{"sentence": "We have developed and evaluated a Temporal Expression Anchoror -LRB- TEA -RRB- , and the result shows that it performs significantly better than the baseline , and compares favorably with some of the closely related work .", "ner": [["Temporal Expression Anchoror -LRB- TEA -RRB-", 6, 11, "Method"], ["it", 18, 18, "Generic"], ["baseline", 24, 24, "Generic"]], "relations": [[18, 18, 24, 24, "COMPARE"]]},
{"sentence": "We address the problem of populating object category detection datasets with dense , per-object 3D reconstructions , bootstrapped from class labels , ground truth figure-ground segmentations and a small set of keypoint annotations .", "ner": [["object category detection datasets", 6, 9, "Material"], ["per-object 3D reconstructions", 13, 15, "Task"], ["ground truth figure-ground segmentations", 22, 25, "Method"], ["keypoint annotations", 31, 32, "Method"]], "relations": [[6, 9, 13, 15, "USED-FOR"], [22, 25, 13, 15, "USED-FOR"], [22, 25, 31, 32, "CONJUNCTION"], [31, 32, 13, 15, "USED-FOR"]]},
{"sentence": "Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion , then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .", "ner": [["algorithm", 2, 2, "Generic"], ["camera viewpoint", 5, 6, "OtherScientificTerm"], ["rigid structure-from-motion", 8, 9, "OtherScientificTerm"], ["object shapes", 13, 14, "OtherScientificTerm"], ["visual hull proposals", 18, 20, "OtherScientificTerm"], ["loose within-class shape similarity assumptions", 23, 27, "OtherScientificTerm"]], "relations": [[2, 2, 5, 6, "USED-FOR"], [2, 2, 13, 14, "USED-FOR"], [8, 9, 2, 2, "USED-FOR"], [18, 20, 13, 14, "USED-FOR"], [23, 27, 18, 20, "USED-FOR"]]},
{"sentence": "The visual hull sampling process attempts to intersect an object 's projection cone with the cones of minimal subsets of other similar objects among those pictured from certain vantage points .", "ner": [["visual hull sampling process", 1, 4, "Method"], ["projection cone", 11, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "We show that our method is able to produce convincing per-object 3D reconstructions on one of the most challenging existing object-category detection datasets , PASCAL VOC .", "ner": [["method", 4, 4, "Generic"], ["per-object 3D reconstructions", 10, 12, "Task"], ["object-category detection datasets", 20, 22, "Material"], ["PASCAL VOC", 24, 25, "Material"]], "relations": [[4, 4, 10, 12, "USED-FOR"], [20, 22, 4, 4, "USED-FOR"], [24, 25, 20, 22, "HYPONYM-OF"]]},
{"sentence": "Our results may re-stimulate once popular geometry-oriented model-based recognition approaches .", "ner": [["geometry-oriented model-based recognition approaches", 6, 9, "Method"]], "relations": []},
{"sentence": "Probabilistic models have been previously shown to be efficient and effective for modeling and recognition of human motion .", "ner": [["Probabilistic models", 0, 1, "Method"], ["modeling and recognition of human motion", 12, 17, "Task"]], "relations": [[0, 1, 12, 17, "USED-FOR"]]},
{"sentence": "In particular we focus on methods which represent the human motion model as a triangulated graph .", "ner": [["human motion model", 9, 11, "Method"], ["triangulated graph", 14, 15, "Method"]], "relations": [[14, 15, 9, 11, "USED-FOR"]]},
{"sentence": "Previous approaches learned models based just on positions and velocities of the body parts while ignoring their appearance .", "ner": [["models", 3, 3, "Generic"], ["positions", 7, 7, "OtherScientificTerm"], ["velocities", 9, 9, "OtherScientificTerm"], ["appearance", 17, 17, "OtherScientificTerm"]], "relations": [[7, 7, 3, 3, "USED-FOR"], [7, 7, 9, 9, "CONJUNCTION"], [9, 9, 3, 3, "USED-FOR"]]},
{"sentence": "Moreover , a heuristic approach was commonly used to obtain translation invariance .", "ner": [["heuristic approach", 3, 4, "Method"], ["translation invariance", 10, 11, "OtherScientificTerm"]], "relations": [[3, 4, 10, 11, "USED-FOR"]]},
{"sentence": "In this paper we suggest an improved approach for learning such models and using them for human motion recognition .", "ner": [["approach", 7, 7, "Generic"], ["models", 11, 11, "Generic"], ["them", 14, 14, "Generic"], ["human motion recognition", 16, 18, "Task"]], "relations": [[7, 7, 11, 11, "USED-FOR"], [14, 14, 16, 18, "USED-FOR"]]},
{"sentence": "The suggested approach combines multiple cues , i.e. , positions , velocities and appearance into both the learning and detection phases .", "ner": [["approach", 2, 2, "Generic"], ["cues", 5, 5, "Generic"], ["positions", 9, 9, "OtherScientificTerm"], ["velocities", 11, 11, "OtherScientificTerm"], ["appearance", 13, 13, "OtherScientificTerm"], ["learning and detection phases", 17, 20, "Task"]], "relations": [[2, 2, 17, 20, "USED-FOR"], [9, 9, 5, 5, "HYPONYM-OF"], [9, 9, 11, 11, "CONJUNCTION"], [11, 11, 5, 5, "HYPONYM-OF"], [11, 11, 13, 13, "CONJUNCTION"], [13, 13, 5, 5, "HYPONYM-OF"]]},
{"sentence": "Furthermore , we introduce global variables in the model , which can represent global properties such as translation , scale or viewpoint .", "ner": [["global variables", 4, 5, "OtherScientificTerm"], ["model", 8, 8, "Generic"], ["global properties", 13, 14, "OtherScientificTerm"], ["translation", 17, 17, "OtherScientificTerm"], ["scale", 19, 19, "OtherScientificTerm"], ["viewpoint", 21, 21, "OtherScientificTerm"]], "relations": [[4, 5, 8, 8, "USED-FOR"], [4, 5, 13, 14, "USED-FOR"], [17, 17, 13, 14, "HYPONYM-OF"], [17, 17, 19, 19, "CONJUNCTION"], [19, 19, 13, 14, "HYPONYM-OF"], [19, 19, 21, 21, "CONJUNCTION"], [21, 21, 13, 14, "HYPONYM-OF"]]},
{"sentence": "The model is learned in an unsupervised manner from un-labelled data .", "ner": [["model", 1, 1, "Generic"], ["unsupervised manner", 6, 7, "Method"], ["un-labelled data", 9, 10, "Material"]], "relations": [[6, 7, 1, 1, "USED-FOR"], [9, 10, 6, 7, "USED-FOR"]]},
{"sentence": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "ner": [["hybrid proba-bilistic model", 5, 7, "Method"], ["global variables", 11, 12, "OtherScientificTerm"], ["translation", 15, 15, "OtherScientificTerm"], ["local variables", 18, 19, "OtherScientificTerm"], ["relative positions", 22, 23, "OtherScientificTerm"], ["appearances of body parts", 25, 28, "OtherScientificTerm"], ["faster convergence", 37, 38, "Metric"], ["learning phase", 40, 41, "Task"], ["robustness", 46, 46, "Metric"], ["occlusions", 48, 48, "OtherScientificTerm"], ["recognition rate", 56, 57, "Material"]], "relations": [[11, 12, 5, 7, "USED-FOR"], [15, 15, 11, 12, "HYPONYM-OF"], [22, 23, 18, 19, "HYPONYM-OF"], [22, 23, 25, 28, "CONJUNCTION"], [25, 28, 18, 19, "HYPONYM-OF"], [37, 38, 40, 41, "FEATURE-OF"], [37, 38, 46, 46, "CONJUNCTION"], [46, 46, 56, 57, "CONJUNCTION"]]},
{"sentence": "Factor analysis and principal components analysis can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space .", "ner": [["Factor analysis", 0, 1, "Method"], ["principal components analysis", 3, 5, "Method"], ["linear relationships between observed variables", 11, 15, "OtherScientificTerm"], ["high-dimensional data", 19, 20, "Material"], ["lower-dimensional hidden space", 23, 25, "OtherScientificTerm"]], "relations": [[0, 1, 3, 5, "CONJUNCTION"], [0, 1, 11, 15, "USED-FOR"], [3, 5, 11, 15, "USED-FOR"]]},
{"sentence": "In factor analysis , the observations are modeled as a linear combination of normally distributed hidden variables .", "ner": [["factor analysis", 1, 2, "Method"], ["linear combination of normally distributed hidden variables", 10, 16, "Method"]], "relations": []},
{"sentence": "We describe a nonlinear generalization of factor analysis , called `` product analy-sis '' , that models the observed variables as a linear combination of products of normally distributed hidden variables .", "ner": [["nonlinear generalization of factor analysis", 3, 7, "Method"], ["factor analysis", 6, 7, "Method"], ["`` product analy-sis ''", 10, 13, "Method"], ["observed variables", 18, 19, "OtherScientificTerm"], ["linear combination of products of normally distributed hidden variables", 22, 30, "OtherScientificTerm"]], "relations": [[3, 7, 18, 19, "USED-FOR"], [10, 13, 3, 7, "HYPONYM-OF"], [22, 30, 3, 7, "USED-FOR"]]},
{"sentence": "Just as factor analysis can be viewed as unsupervised linear regression on unobserved , normally distributed hidden variables , product analysis can be viewed as unsupervised linear regression on products of unobserved , normally distributed hidden variables .", "ner": [["factor analysis", 2, 3, "Method"], ["unsupervised linear regression", 8, 10, "Method"], ["distributed hidden variables", 15, 17, "OtherScientificTerm"], ["product analysis", 19, 20, "Method"], ["unsupervised linear regression", 25, 27, "Method"], ["distributed hidden variables", 34, 36, "OtherScientificTerm"]], "relations": [[8, 10, 2, 3, "USED-FOR"], [25, 27, 19, 20, "USED-FOR"]]},
{"sentence": "The mapping between the data and the hidden space is nonlinear , so we use an approximate variational technique for inference and learning .", "ner": [["hidden space", 7, 8, "OtherScientificTerm"], ["approximate variational technique", 16, 18, "Method"], ["inference", 20, 20, "Task"], ["learning", 22, 22, "Task"]], "relations": [[16, 18, 20, 20, "USED-FOR"], [16, 18, 22, 22, "USED-FOR"], [20, 20, 22, 22, "CONJUNCTION"]]},
{"sentence": "Since product analysis is a generalization of factor analysis , product analysis always finds a higher data likelihood than factor analysis .", "ner": [["product analysis", 1, 2, "Method"], ["generalization of factor analysis", 5, 8, "Method"], ["factor analysis", 7, 8, "Method"], ["product analysis", 10, 11, "Method"], ["factor analysis", 19, 20, "Method"]], "relations": [[1, 2, 5, 8, "HYPONYM-OF"], [10, 11, 19, 20, "COMPARE"]]},
{"sentence": "We give results on pattern recognition and illumination-invariant image clustering .", "ner": [["pattern recognition", 4, 5, "Task"], ["illumination-invariant image clustering", 7, 9, "Task"]], "relations": [[4, 5, 7, 9, "CONJUNCTION"]]},
{"sentence": "This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query applications .", "ner": [["domain independent strategy", 4, 6, "Method"], ["multimedia articulation of answers", 9, 12, "Task"], ["natural language interface", 16, 18, "OtherScientificTerm"], ["database query applications", 20, 22, "Task"]], "relations": [[4, 6, 9, 12, "USED-FOR"], [9, 12, 16, 18, "USED-FOR"], [16, 18, 20, 22, "USED-FOR"]]},
{"sentence": "Multimedia answers include videodisc images and heuristically-produced complete sentences in text or text-to-speech form .", "ner": [["Multimedia answers", 0, 1, "Material"], ["videodisc images", 3, 4, "Material"], ["text-to-speech form", 12, 13, "OtherScientificTerm"]], "relations": [[3, 4, 0, 1, "PART-OF"]]},
{"sentence": "Deictic reference and feedback about the discourse are enabled .", "ner": [["Deictic reference", 0, 1, "OtherScientificTerm"], ["feedback", 3, 3, "OtherScientificTerm"], ["discourse", 6, 6, "OtherScientificTerm"]], "relations": [[0, 1, 3, 3, "CONJUNCTION"], [0, 1, 6, 6, "FEATURE-OF"], [3, 3, 6, 6, "FEATURE-OF"]]},
{"sentence": "The interface thus presents the application as cooperative and conversational .", "ner": [["interface", 1, 1, "Generic"], ["application", 5, 5, "Generic"]], "relations": []},
{"sentence": "The LOGON MT demonstrator assembles independently valuable general-purpose NLP components into a machine translation pipeline that capitalizes on output quality .", "ner": [["LOGON MT demonstrator", 1, 3, "Method"], ["general-purpose NLP components", 7, 9, "Method"], ["machine translation pipeline", 12, 14, "Method"]], "relations": [[1, 3, 7, 9, "USED-FOR"], [7, 9, 12, 14, "PART-OF"]]},
{"sentence": "The demonstrator embodies an interesting combination of hand-built , symbolic resources and stochastic processes .", "ner": [["demonstrator", 1, 1, "Task"], ["hand-built , symbolic resources", 7, 10, "Material"], ["stochastic processes", 12, 13, "Method"]], "relations": [[7, 10, 1, 1, "PART-OF"], [7, 10, 12, 13, "CONJUNCTION"], [12, 13, 1, 1, "PART-OF"]]},
{"sentence": "In this paper we investigate the simple logical properties of contexts .", "ner": [["logical properties of contexts", 7, 10, "Task"]], "relations": []},
{"sentence": "We describe both the syntax and semantics of a general propositional language of context , and give a Hilbert style proof system for this language .", "ner": [["syntax", 4, 4, "OtherScientificTerm"], ["semantics", 6, 6, "OtherScientificTerm"], ["propositional language of context", 10, 13, "OtherScientificTerm"], ["Hilbert style proof system", 18, 21, "Method"], ["language", 24, 24, "Generic"]], "relations": [[4, 4, 6, 6, "CONJUNCTION"], [4, 4, 10, 13, "FEATURE-OF"], [6, 6, 10, 13, "FEATURE-OF"], [18, 21, 24, 24, "USED-FOR"]]},
{"sentence": "A propositional logic of context extends classical propositional logic in two ways .", "ner": [["propositional logic of context", 1, 4, "OtherScientificTerm"], ["classical propositional logic", 6, 8, "OtherScientificTerm"]], "relations": [[6, 8, 1, 4, "USED-FOR"]]},
{"sentence": "Firstly , a new modality , ist -LRB- ;-RRB- , is introduced .", "ner": [["modality", 4, 4, "OtherScientificTerm"]], "relations": []},
{"sentence": "It is used to express that the sentence , , holds in the context .", "ner": [["It", 0, 0, "Generic"]], "relations": []},
{"sentence": "Secondly , each context has its own vocabulary , i.e. a set of propositional atoms which are deened or meaningful in that context .", "ner": [], "relations": []},
{"sentence": "The main results of this paper are the sound-ness and completeness of this Hilbert style proof system .", "ner": [["Hilbert style proof system", 13, 16, "Method"]], "relations": []},
{"sentence": "We also provide soundness and completeness results -LRB- i.e. correspondence theory -RRB- for various extensions of the general system .", "ner": [["correspondence theory", 9, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "Image matching is a fundamental problem in Computer Vision .", "ner": [["Image matching", 0, 1, "Task"], ["Computer Vision", 7, 8, "Task"]], "relations": [[0, 1, 7, 8, "HYPONYM-OF"]]},
{"sentence": "In the context of feature-based matching , SIFT and its variants have long excelled in a wide array of applications .", "ner": [["feature-based matching", 4, 5, "Task"], ["SIFT", 7, 7, "Method"]], "relations": [[7, 7, 4, 5, "USED-FOR"]]},
{"sentence": "However , for ultra-wide baselines , as in the case of aerial images captured under large camera rotations , the appearance variation goes beyond the reach of SIFT and RANSAC .", "ner": [["ultra-wide baselines", 3, 4, "OtherScientificTerm"], ["aerial images", 11, 12, "Material"], ["large camera rotations", 15, 17, "OtherScientificTerm"], ["appearance variation", 20, 21, "OtherScientificTerm"], ["SIFT", 27, 27, "Method"], ["RANSAC", 29, 29, "Method"]], "relations": [[15, 17, 11, 12, "FEATURE-OF"], [27, 27, 29, 29, "CONJUNCTION"]]},
{"sentence": "In this paper we propose a data-driven , deep learning-based approach that sidesteps local correspondence by framing the problem as a classification task .", "ner": [["data-driven , deep learning-based approach", 6, 10, "Method"], ["local correspondence", 13, 14, "OtherScientificTerm"], ["problem", 18, 18, "Generic"], ["classification task", 21, 22, "Task"]], "relations": [[21, 22, 18, 18, "USED-FOR"]]},
{"sentence": "Furthermore , we demonstrate that local correspondences can still be useful .", "ner": [["local correspondences", 5, 6, "OtherScientificTerm"]], "relations": []},
{"sentence": "To do so we incorporate an attention mechanism to produce a set of probable matches , which allows us to further increase performance .", "ner": [["attention mechanism", 6, 7, "Method"]], "relations": []},
{"sentence": "We train our models on a dataset of urban aerial imagery consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the problem via a human study with annotations from Amazon Mechanical Turk .", "ner": [["models", 3, 3, "Generic"], ["dataset of urban aerial imagery", 6, 10, "Material"], ["problem", 30, 30, "Generic"], ["human study", 33, 34, "Method"], ["annotations from Amazon Mechanical Turk", 36, 40, "Material"]], "relations": [[6, 10, 3, 3, "USED-FOR"], [33, 34, 30, 30, "USED-FOR"], [36, 40, 33, 34, "USED-FOR"]]},
{"sentence": "We demonstrate that our models outperform the state-of-the-art on ultra-wide baseline matching and approach human accuracy .", "ner": [["models", 4, 4, "Generic"], ["state-of-the-art", 7, 7, "Generic"], ["ultra-wide baseline matching", 9, 11, "Task"], ["human accuracy", 14, 15, "Metric"]], "relations": [[4, 4, 7, 7, "COMPARE"], [4, 4, 14, 15, "COMPARE"], [9, 11, 4, 4, "EVALUATE-FOR"], [9, 11, 7, 7, "EVALUATE-FOR"]]},
{"sentence": "This article is devoted to the problem of quantifying noun groups in German .", "ner": [["quantifying noun groups in German", 8, 12, "Task"], ["German", 12, 12, "Material"]], "relations": []},
{"sentence": "After a thorough description of the phenomena , the results of corpus-based investigations are described .", "ner": [], "relations": []},
{"sentence": "Moreover , some examples are given that underline the necessity of integrating some kind of information other than grammar sensu stricto into the treebank .", "ner": [["grammar sensu stricto", 18, 20, "OtherScientificTerm"], ["treebank", 23, 23, "Material"]], "relations": []},
{"sentence": "We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .", "ner": [["fine-grained annotation", 7, 8, "OtherScientificTerm"], ["tree-bank", 11, 11, "Material"], ["stochastic parsers", 18, 19, "Method"], ["tree-bank", 23, 23, "Material"], ["grammars", 26, 26, "Method"], ["treebank", 30, 30, "Material"], ["treebank", 37, 37, "Material"], ["theoretical linguistic investigations", 46, 48, "Task"]], "relations": [[7, 8, 18, 19, "USED-FOR"], [23, 23, 18, 19, "USED-FOR"], [30, 30, 26, 26, "USED-FOR"], [37, 37, 46, 48, "USED-FOR"]]},
{"sentence": "The information gained from corpus research and the analyses that are proposed are realized in the framework of SILVA , a parsing and extraction tool for German text corpora .", "ner": [["SILVA", 18, 18, "Method"], ["parsing and extraction tool", 21, 24, "Method"], ["German text corpora", 26, 28, "Material"]], "relations": [[18, 18, 21, 24, "HYPONYM-OF"], [26, 28, 18, 18, "USED-FOR"]]},
{"sentence": "While paraphrasing is critical both for interpretation and generation of natural language , current systems use manual or semi-automatic methods to collect paraphrases .", "ner": [["paraphrasing", 1, 1, "Method"], ["interpretation and generation of natural language", 6, 11, "Task"], ["systems", 14, 14, "Generic"], ["manual or semi-automatic methods", 16, 19, "Method"], ["paraphrases", 22, 22, "OtherScientificTerm"]], "relations": [[1, 1, 6, 11, "USED-FOR"], [14, 14, 22, 22, "USED-FOR"], [16, 19, 14, 14, "USED-FOR"]]},
{"sentence": "We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text .", "ner": [["unsupervised learning algorithm", 3, 5, "Method"], ["identification of paraphrases", 7, 9, "Task"], ["corpus of multiple English translations", 12, 16, "Material"]], "relations": [[3, 5, 7, 9, "USED-FOR"], [12, 16, 7, 9, "USED-FOR"]]},
{"sentence": "Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases .", "ner": [["approach", 1, 1, "Generic"], ["phrasal and single word lexical paraphrases", 3, 8, "OtherScientificTerm"], ["syntactic paraphrases", 12, 13, "OtherScientificTerm"]], "relations": [[1, 1, 3, 8, "USED-FOR"], [1, 1, 12, 13, "USED-FOR"], [3, 8, 12, 13, "CONJUNCTION"]]},
{"sentence": "An efficient bit-vector-based CKY-style parser for context-free parsing is presented .", "ner": [["bit-vector-based CKY-style parser", 2, 4, "Method"], ["context-free parsing", 6, 7, "Task"]], "relations": [[2, 4, 6, 7, "USED-FOR"]]},
{"sentence": "The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences .", "ner": [["parser", 1, 1, "Method"], ["parse forest representation", 5, 7, "Method"], ["large treebank grammars", 16, 18, "Method"]], "relations": [[1, 1, 5, 7, "USED-FOR"], [5, 7, 16, 18, "USED-FOR"]]},
{"sentence": "The parser uses bit-vector operations to parallelise the basic parsing operations .", "ner": [["parser", 1, 1, "Method"], ["bit-vector operations", 3, 4, "Method"]], "relations": [[3, 4, 1, 1, "USED-FOR"]]},
{"sentence": "The parser is particularly useful when all analyses are needed rather than just the most probable one .", "ner": [["parser", 1, 1, "Method"]], "relations": []},
{"sentence": "In this paper , we propose a partially-blurred-image classification and analysis framework for automatically detecting images containing blurred regions and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring .", "ner": [["partially-blurred-image classification and analysis framework", 7, 11, "Method"], ["automatically detecting images", 13, 15, "Task"], ["images", 15, 15, "Material"], ["blurred regions", 17, 18, "OtherScientificTerm"], ["regions", 26, 26, "Generic"], ["blur kernel estimation", 31, 33, "Task"], ["image deblurring", 35, 36, "Task"]], "relations": [[7, 11, 13, 15, "USED-FOR"], [17, 18, 15, 15, "PART-OF"], [31, 33, 35, 36, "CONJUNCTION"]]},
{"sentence": "We develop several blur features modeled by image color , gradient , and spectrum information , and use feature parameter training to robustly classify blurred images .", "ner": [["blur features", 3, 4, "OtherScientificTerm"], ["image color", 7, 8, "OtherScientificTerm"], ["gradient", 10, 10, "OtherScientificTerm"], ["spectrum information", 13, 14, "OtherScientificTerm"], ["feature parameter training", 18, 20, "Method"], ["blurred images", 24, 25, "Material"]], "relations": [[7, 8, 3, 4, "USED-FOR"], [7, 8, 10, 10, "CONJUNCTION"], [10, 10, 3, 4, "USED-FOR"], [10, 10, 13, 14, "CONJUNCTION"], [13, 14, 3, 4, "USED-FOR"], [18, 20, 24, 25, "USED-FOR"]]},
{"sentence": "Our blur detection is based on image patches , making region-wise training and classification in one image efficient .", "ner": [["blur detection", 1, 2, "Method"], ["image patches", 6, 7, "OtherScientificTerm"], ["region-wise training and classification", 10, 13, "Method"]], "relations": [[6, 7, 1, 2, "USED-FOR"], [10, 13, 1, 2, "USED-FOR"]]},
{"sentence": "Extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several computer vision problems , such as motion analysis and image restoration , using the blur information .", "ner": [["method", 5, 5, "Generic"], ["image data", 10, 11, "Material"], ["computer vision problems", 21, 23, "Task"], ["motion analysis", 27, 28, "Task"], ["image restoration", 30, 31, "Task"], ["blur information", 35, 36, "OtherScientificTerm"]], "relations": [[5, 5, 21, 23, "USED-FOR"], [10, 11, 5, 5, "EVALUATE-FOR"], [27, 28, 21, 23, "HYPONYM-OF"], [27, 28, 30, 31, "CONJUNCTION"], [30, 31, 21, 23, "HYPONYM-OF"], [35, 36, 5, 5, "USED-FOR"]]},
{"sentence": "It is well-known that there are polysemous words like sentence whose meaning or sense depends on the context of use .", "ner": [["polysemous words", 6, 7, "OtherScientificTerm"]], "relations": []},
{"sentence": "We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -LRB- the Canadian Hansards -RRB- and the other trained on monolingual material -LRB- Roget 's Thesaurus and Grolier 's Encyclopedia -RRB- .", "ner": [["word-sense disambiguation systems", 7, 9, "Method"], ["one", 11, 11, "Generic"], ["bilingual material", 14, 15, "Material"], ["Canadian Hansards", 18, 19, "Material"], ["other", 23, 23, "Generic"], ["monolingual material", 26, 27, "Material"], ["Roget 's Thesaurus", 29, 31, "Material"], ["Grolier 's Encyclopedia", 33, 35, "Material"]], "relations": [[11, 11, 7, 9, "HYPONYM-OF"], [11, 11, 23, 23, "CONJUNCTION"], [14, 15, 11, 11, "EVALUATE-FOR"], [23, 23, 7, 9, "HYPONYM-OF"], [26, 27, 23, 23, "USED-FOR"], [29, 31, 26, 27, "HYPONYM-OF"], [29, 31, 33, 35, "CONJUNCTION"], [33, 35, 26, 27, "HYPONYM-OF"]]},
{"sentence": "As this work was nearing completion , we observed a very strong discourse effect .", "ner": [["discourse", 12, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "That is , if a polysemous word such as sentence appears two or more times in a well-written discourse , it is extremely likely that they will all share the same sense .", "ner": [["polysemous word", 5, 6, "OtherScientificTerm"], ["well-written discourse", 17, 18, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper describes an experiment which confirmed this hypothesis and found that the tendency to share sense in the same discourse is extremely strong -LRB- 98 % -RRB- .", "ner": [["discourse", 20, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "This result can be used as an additional source of constraint for improving the performance of the word-sense disambiguation algorithm .", "ner": [["constraint", 10, 10, "OtherScientificTerm"], ["word-sense disambiguation algorithm", 17, 19, "Method"]], "relations": []},
{"sentence": "In addition , it could also be used to help evaluate disambiguation algorithms that did not make use of the discourse constraint .", "ner": [["it", 3, 3, "Generic"], ["disambiguation algorithms", 11, 12, "Method"], ["discourse constraint", 20, 21, "OtherScientificTerm"]], "relations": [[3, 3, 11, 12, "EVALUATE-FOR"]]},
{"sentence": "We study and compare two novel embedding methods for segmenting feature points of piece-wise planar structures from two -LRB- uncalibrated -RRB- perspective images .", "ner": [["embedding methods", 6, 7, "Method"], ["segmenting feature points of piece-wise planar structures", 9, 15, "Task"]], "relations": [[6, 7, 9, 15, "USED-FOR"]]},
{"sentence": "We show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each homography corresponds to either a complex bilinear form or a real quadratic form .", "ner": [["homographies", 7, 7, "OtherScientificTerm"], ["higher-dimensional real or complex space", 16, 20, "OtherScientificTerm"], ["homography", 25, 25, "OtherScientificTerm"], ["complex bilinear form", 30, 32, "OtherScientificTerm"], ["real quadratic form", 35, 37, "OtherScientificTerm"]], "relations": [[16, 20, 7, 7, "FEATURE-OF"], [30, 32, 25, 25, "FEATURE-OF"], [30, 32, 35, 37, "CONJUNCTION"], [35, 37, 25, 25, "FEATURE-OF"]]},
{"sentence": "Each embedding reveals different algebraic properties and relations of homo-graphies .", "ner": [["homo-graphies", 9, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "We give a closed-form segmentation solution for each case by utilizing these properties based on subspace-segmentation methods .", "ner": [["closed-form segmentation solution", 3, 5, "Method"], ["subspace-segmentation methods", 15, 16, "Method"]], "relations": [[15, 16, 3, 5, "USED-FOR"]]},
{"sentence": "These theoretical results show that one can intrinsically segment a piece-wise planar scene from 2-D images without explicitly performing any 3-D reconstruction .", "ner": [["piece-wise planar scene", 10, 12, "OtherScientificTerm"], ["2-D images", 14, 15, "Material"], ["3-D reconstruction", 20, 21, "Method"]], "relations": [[14, 15, 10, 12, "FEATURE-OF"]]},
{"sentence": "The resulting segmentation may make subsequent 3-D reconstruction much better-conditioned .", "ner": [["3-D reconstruction", 6, 7, "Method"]], "relations": []},
{"sentence": "We demonstrate the proposed methods with some convincing experimental results .", "ner": [], "relations": []},
{"sentence": "Background maintenance is a frequent element of video surveillance systems .", "ner": [["Background maintenance", 0, 1, "Task"], ["video surveillance systems", 7, 9, "Task"]], "relations": [[0, 1, 7, 9, "PART-OF"]]},
{"sentence": "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "ner": [["Wallflower", 2, 2, "Method"], ["three-component system", 5, 6, "Generic"], ["background maintenance", 8, 9, "Task"], ["pixel-level component", 12, 13, "Method"], ["Wiener filtering", 15, 16, "Method"], ["probabilistic predictions of the expected background", 19, 24, "Task"], ["region-level component", 27, 28, "Method"], ["homogeneous regions of foreground objects", 31, 35, "OtherScientificTerm"], ["frame-level component", 39, 40, "Method"]], "relations": [[5, 6, 8, 9, "USED-FOR"], [12, 13, 5, 6, "PART-OF"], [12, 13, 27, 28, "CONJUNCTION"], [15, 16, 12, 13, "USED-FOR"], [15, 16, 19, 24, "USED-FOR"], [27, 28, 5, 6, "PART-OF"], [27, 28, 31, 35, "USED-FOR"], [27, 28, 39, 40, "CONJUNCTION"], [39, 40, 5, 6, "PART-OF"]]},
{"sentence": "We compare our system with 8 other background subtraction algorithms .", "ner": [["system", 3, 3, "Generic"], ["background subtraction algorithms", 7, 9, "Method"]], "relations": [[3, 3, 7, 9, "COMPARE"]]},
{"sentence": "Wallflower is shown to outperform previous algorithms by handling a greater set of the difficult situations that can occur .", "ner": [["Wallflower", 0, 0, "Method"], ["algorithms", 6, 6, "Generic"]], "relations": [[0, 0, 6, 6, "COMPARE"]]},
{"sentence": "Finally , we analyze the experimental results and propose normative principles for background maintenance .", "ner": [["normative principles", 9, 10, "Method"], ["background maintenance", 12, 13, "Task"]], "relations": [[9, 10, 12, 13, "USED-FOR"]]},
{"sentence": "Is it possible to use out-of-domain acoustic training data to improve a speech recognizer 's performance on a speciic , independent application ?", "ner": [["out-of-domain acoustic training data", 5, 8, "Material"], ["speech recognizer", 12, 13, "Method"]], "relations": [[5, 8, 12, 13, "USED-FOR"]]},
{"sentence": "In our experiments , we use Wallstreet Journal -LRB- WSJ -RRB- data to train a recognizer , which is adapted and evaluated in the Phonebook domain .", "ner": [["Wallstreet Journal -LRB- WSJ -RRB- data", 6, 11, "Material"], ["recognizer", 15, 15, "Method"], ["Phonebook domain", 24, 25, "Material"]], "relations": [[6, 11, 15, 15, "USED-FOR"], [24, 25, 15, 15, "EVALUATE-FOR"]]},
{"sentence": "Apart from their common language -LRB- US English -RRB- , the two corpora diier in many important respects : microphone vs. telephone channel , continuous speech vs. isolated words , mismatch i n s p e a k i n g r a t e .", "ner": [["common language -LRB- US English -RRB-", 3, 8, "OtherScientificTerm"], ["microphone vs. telephone channel", 19, 22, "OtherScientificTerm"], ["continuous speech", 24, 25, "OtherScientificTerm"], ["isolated words", 27, 28, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper deals with two questions .", "ner": [], "relations": []},
{"sentence": "First , starting from the WSJ-trained recognizer , how much adaptation data -LRB- taken from the Phonebook training corpus -RRB- is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch ?", "ner": [["WSJ-trained recognizer", 5, 6, "Method"], ["adaptation data", 10, 11, "Material"], ["Phonebook training corpus", 16, 18, "Material"], ["recognition", 26, 26, "Task"], ["mismatch", 35, 35, "OtherScientificTerm"]], "relations": [[5, 6, 26, 26, "USED-FOR"], [10, 11, 5, 6, "USED-FOR"], [10, 11, 16, 18, "PART-OF"]]},
{"sentence": "Second , is it possible to improve the recognition performance of a Phonebook-trained baseline acoustic model by using additional out-of-domain training data ?", "ner": [["recognition", 8, 8, "Task"], ["Phonebook-trained baseline acoustic model", 12, 15, "Method"], ["out-of-domain training data", 19, 21, "Material"]], "relations": [[12, 15, 8, 8, "USED-FOR"], [19, 21, 12, 15, "USED-FOR"]]},
{"sentence": "The paper describes the adaptation and normalization techniques used to bridge the mismatch b e-tween the two corpora .", "ner": [["adaptation and normalization techniques", 4, 7, "Method"]], "relations": []},
{"sentence": "This paper proposes an approach to full parsing suitable for Information Extraction from texts .", "ner": [["approach", 4, 4, "Generic"], ["full parsing", 6, 7, "Task"], ["Information Extraction", 10, 11, "Task"]], "relations": [[4, 4, 6, 7, "USED-FOR"], [6, 7, 10, 11, "USED-FOR"]]},
{"sentence": "Sequences of cascades of rules deterministically analyze the text , building unambiguous structures .", "ner": [["rules", 4, 4, "OtherScientificTerm"], ["unambiguous structures", 11, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "Initially basic chunks are analyzed ; then argumental relations are recognized ; finally modifier attachment is performed and the global parse tree is built .", "ner": [["argumental relations", 7, 8, "OtherScientificTerm"], ["modifier attachment", 13, 14, "OtherScientificTerm"], ["global parse tree", 19, 21, "OtherScientificTerm"]], "relations": []},
{"sentence": "The approach was proven to work for three languages and different domains .", "ner": [["approach", 1, 1, "Generic"]], "relations": []},
{"sentence": "It was implemented in the IE module of FACILE , a EU project for multilingual text classification and IE .", "ner": [["It", 0, 0, "Generic"], ["IE module", 5, 6, "Method"], ["FACILE , a EU project for multilingual text classification and IE", 8, 18, "Method"]], "relations": [[0, 0, 5, 6, "USED-FOR"], [5, 6, 8, 18, "PART-OF"]]},
{"sentence": "This paper presents a corpus study that explores the extent to which captions contribute to recognizing the intended message of an information graphic .", "ner": [["corpus study", 4, 5, "Task"], ["information graphic", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals , and an evaluation study showing that evidence obtained from shallow processing of the graphic 's caption has a significant impact on the system 's success .", "ner": [["graphic interpretation system", 5, 7, "Method"], ["communicative signals", 15, 16, "Material"], ["shallow processing", 27, 28, "Method"], ["graphic 's caption", 31, 33, "Material"], ["system", 40, 40, "Generic"]], "relations": [[15, 16, 5, 7, "USED-FOR"], [27, 28, 40, 40, "USED-FOR"], [31, 33, 27, 28, "USED-FOR"]]},
{"sentence": "This work is part of a larger project whose goal is to provide sight-impaired users with effective access to information graphics .", "ner": [["sight-impaired users", 13, 14, "OtherScientificTerm"], ["information graphics", 19, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "Graphical models such as Bayesian Networks -LRB- BNs -RRB- are being increasingly applied to various computer vision problems .", "ner": [["Graphical models", 0, 1, "Method"], ["Bayesian Networks -LRB- BNs -RRB-", 4, 8, "Method"], ["computer vision problems", 15, 17, "Task"]], "relations": [[0, 1, 15, 17, "USED-FOR"], [4, 8, 0, 1, "HYPONYM-OF"]]},
{"sentence": "One bottleneck in using BN is that learning the BN model parameters often requires a large amount of reliable and representative training data , which proves to be difficult to acquire for many computer vision tasks .", "ner": [["BN", 4, 4, "Method"], ["BN model parameters", 9, 11, "OtherScientificTerm"], ["representative training data", 20, 22, "Material"], ["computer vision tasks", 33, 35, "Task"]], "relations": [[20, 22, 9, 11, "USED-FOR"], [20, 22, 33, 35, "USED-FOR"]]},
{"sentence": "On the other hand , there is often available qualitative prior knowledge about the model .", "ner": [["qualitative prior knowledge", 9, 11, "OtherScientificTerm"], ["model", 14, 14, "Generic"]], "relations": [[9, 11, 14, 14, "FEATURE-OF"]]},
{"sentence": "Such knowledge comes either from domain experts based on their experience or from various physical or geometric constraints that govern the objects we try to model .", "ner": [["knowledge", 1, 1, "Generic"], ["domain experts", 5, 6, "OtherScientificTerm"], ["physical or geometric constraints", 14, 17, "OtherScientificTerm"]], "relations": [[5, 6, 1, 1, "USED-FOR"], [5, 6, 14, 17, "CONJUNCTION"], [14, 17, 1, 1, "USED-FOR"]]},
{"sentence": "Unlike the quantitative prior , the qualitative prior is often ignored due to the difficulty of incorporating them into the model learning process .", "ner": [["quantitative prior", 2, 3, "OtherScientificTerm"], ["qualitative prior", 6, 7, "OtherScientificTerm"], ["them", 17, 17, "Generic"], ["model learning process", 20, 22, "Task"]], "relations": [[2, 3, 6, 7, "COMPARE"], [17, 17, 20, 22, "PART-OF"]]},
{"sentence": "In this paper , we introduce a closed-form solution to systematically combine the limited training data with some generic qualitative knowledge for BN parameter learning .", "ner": [["closed-form solution", 7, 8, "Method"], ["limited training data", 13, 15, "Material"], ["qualitative knowledge", 19, 20, "OtherScientificTerm"], ["BN parameter learning", 22, 24, "Method"]], "relations": [[13, 15, 19, 20, "CONJUNCTION"], [13, 15, 22, 24, "USED-FOR"], [19, 20, 22, 24, "USED-FOR"], [22, 24, 7, 8, "USED-FOR"]]},
{"sentence": "To validate our method , we compare it with the Maximum Likelihood -LRB- ML -RRB- estimation method under sparse data and with the Expectation Maximization -LRB- EM -RRB- algorithm under incomplete data respectively .", "ner": [["method", 3, 3, "Generic"], ["it", 7, 7, "Generic"], ["Maximum Likelihood -LRB- ML -RRB- estimation method", 10, 16, "Method"], ["sparse data", 18, 19, "Material"], ["Expectation Maximization -LRB- EM -RRB- algorithm", 23, 28, "Method"], ["incomplete data", 30, 31, "Material"]], "relations": [[7, 7, 10, 16, "COMPARE"], [7, 7, 23, 28, "COMPARE"], [18, 19, 7, 7, "USED-FOR"], [18, 19, 10, 16, "USED-FOR"], [30, 31, 7, 7, "USED-FOR"], [30, 31, 23, 28, "USED-FOR"]]},
{"sentence": "To further demonstrate its applications for computer vision , we apply it to learn a BN model for facial Action Unit -LRB- AU -RRB- recognition from real image data .", "ner": [["computer vision", 6, 7, "Task"], ["it", 11, 11, "Generic"], ["BN model", 15, 16, "Method"], ["facial Action Unit -LRB- AU -RRB- recognition", 18, 24, "Task"], ["real image data", 26, 28, "Material"]], "relations": [[11, 11, 6, 7, "USED-FOR"], [11, 11, 15, 16, "USED-FOR"], [15, 16, 18, 24, "USED-FOR"], [26, 28, 18, 24, "USED-FOR"]]},
{"sentence": "The experimental results show that with simple and generic qualitative constraints and using only a small amount of training data , our method can robustly and accurately estimate the BN model parameters .", "ner": [["generic qualitative constraints", 8, 10, "OtherScientificTerm"], ["training data", 18, 19, "Material"], ["method", 22, 22, "Generic"], ["BN model parameters", 29, 31, "OtherScientificTerm"]], "relations": [[8, 10, 18, 19, "CONJUNCTION"], [8, 10, 22, 22, "USED-FOR"], [18, 19, 22, 22, "USED-FOR"], [22, 22, 29, 31, "USED-FOR"]]},
{"sentence": "In this paper we introduce a modal language LT for imposing constraints on trees , and an extension LT -LRB- LF -RRB- for imposing constraints on trees decorated with feature structures .", "ner": [["modal language LT", 6, 8, "Method"], ["constraints on trees", 11, 13, "OtherScientificTerm"], ["extension LT -LRB- LF -RRB-", 17, 21, "Method"], ["constraints on trees decorated with feature structures", 24, 30, "OtherScientificTerm"]], "relations": [[6, 8, 11, 13, "USED-FOR"], [17, 21, 24, 30, "USED-FOR"]]},
{"sentence": "The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously , and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT -LRB- LF -RRB- .", "ner": [["languages", 5, 5, "Generic"], ["grammatical frameworks", 12, 13, "Method"], ["GPSG", 28, 28, "Method"], ["LT -LRB- LF -RRB-", 33, 36, "Method"]], "relations": [[5, 5, 12, 13, "USED-FOR"], [28, 28, 33, 36, "USED-FOR"]]},
{"sentence": "In addition , the role of modal languages -LRB- and in particular , what we have called as constraint formalisms for linguistic theorising is discussed in some detail .", "ner": [["modal languages", 6, 7, "Method"], ["constraint formalisms", 18, 19, "Method"]], "relations": []},
{"sentence": "Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data .", "ner": [["clustering", 7, 7, "Method"], ["inducing semantic verb classes", 9, 12, "Task"], ["undisambiguated corpus data", 14, 16, "Material"]], "relations": [[7, 7, 9, 12, "USED-FOR"], [14, 16, 7, 7, "USED-FOR"]]},
{"sentence": "We describe a new approach which involves clustering subcategorization frame -LRB- SCF -RRB- distributions using the Information Bottleneck and nearest neighbour methods .", "ner": [["approach", 4, 4, "Generic"], ["clustering subcategorization frame -LRB- SCF -RRB- distributions", 7, 13, "Task"], ["Information Bottleneck and nearest neighbour methods", 16, 21, "Method"]], "relations": [[7, 13, 4, 4, "PART-OF"], [16, 21, 7, 13, "USED-FOR"]]},
{"sentence": "In contrast to previous work , we particularly focus on clustering polysemic verbs .", "ner": [["clustering polysemic verbs", 10, 12, "Task"], ["polysemic verbs", 11, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data .", "ner": [["evaluation scheme", 2, 3, "Generic"], ["polysemy", 12, 12, "OtherScientificTerm"], ["clusters", 15, 15, "OtherScientificTerm"], ["semantically classifying undisambiguated SCF data", 28, 32, "Task"]], "relations": [[2, 3, 12, 12, "USED-FOR"], [2, 3, 28, 32, "EVALUATE-FOR"], [12, 12, 15, 15, "FEATURE-OF"]]},
{"sentence": "Due to the capacity of pan-tilt-zoom -LRB- PTZ -RRB- cameras to simultaneously cover a panoramic area and maintain high resolution imagery , researches in automated surveillance systems with multiple PTZ cameras have become increasingly important .", "ner": [["pan-tilt-zoom -LRB- PTZ -RRB- cameras", 5, 9, "OtherScientificTerm"], ["panoramic area", 14, 15, "OtherScientificTerm"], ["high resolution imagery", 18, 20, "OtherScientificTerm"], ["automated surveillance systems", 24, 26, "Task"], ["PTZ cameras", 29, 30, "OtherScientificTerm"]], "relations": [[5, 9, 14, 15, "USED-FOR"], [5, 9, 18, 20, "USED-FOR"], [29, 30, 24, 26, "FEATURE-OF"]]},
{"sentence": "Most existing algorithms require the prior knowledge of intrinsic parameters of the PTZ camera to infer the relative positioning and orientation among multiple PTZ cameras .", "ner": [["algorithms", 2, 2, "Generic"], ["prior knowledge of intrinsic parameters of the PTZ camera", 5, 13, "OtherScientificTerm"], ["relative positioning", 17, 18, "OtherScientificTerm"], ["orientation", 20, 20, "OtherScientificTerm"], ["PTZ cameras", 23, 24, "OtherScientificTerm"]], "relations": [[2, 2, 17, 18, "USED-FOR"], [2, 2, 20, 20, "USED-FOR"], [5, 13, 2, 2, "USED-FOR"], [17, 18, 20, 20, "CONJUNCTION"], [17, 18, 23, 24, "FEATURE-OF"], [20, 20, 23, 24, "FEATURE-OF"]]},
{"sentence": "To overcome this limitation , we propose a novel mapping algorithm that derives the relative positioning and orientation between two PTZ cameras based on a unified polynomial model .", "ner": [["mapping algorithm", 9, 10, "Method"], ["relative positioning", 14, 15, "OtherScientificTerm"], ["orientation", 17, 17, "OtherScientificTerm"], ["PTZ cameras", 20, 21, "OtherScientificTerm"], ["unified polynomial model", 25, 27, "Method"]], "relations": [[9, 10, 14, 15, "USED-FOR"], [9, 10, 17, 17, "USED-FOR"], [14, 15, 17, 17, "CONJUNCTION"], [14, 15, 20, 21, "FEATURE-OF"], [17, 17, 20, 21, "FEATURE-OF"], [25, 27, 9, 10, "USED-FOR"]]},
{"sentence": "This reduces the dependence on the knowledge of intrinsic parameters of PTZ camera and relative positions .", "ner": [["PTZ camera", 11, 12, "OtherScientificTerm"], ["relative positions", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "Experimental results demonstrate that our proposed algorithm presents substantially reduced computational complexity and improved flexibility at the cost of slightly decreased pixel accuracy , as compared with the work of Chen and Wang .", "ner": [["algorithm", 6, 6, "Generic"], ["computational complexity", 10, 11, "Metric"], ["flexibility", 14, 14, "Metric"], ["pixel accuracy", 21, 22, "Metric"]], "relations": [[10, 11, 6, 6, "EVALUATE-FOR"], [14, 14, 6, 6, "EVALUATE-FOR"], [21, 22, 6, 6, "EVALUATE-FOR"]]},
{"sentence": "This slightly decreased pixel accuracy can be compensated by consistent labeling approaches without added cost for the application of automated surveillance systems along with changing configurations and a larger number of PTZ cameras .", "ner": [["pixel accuracy", 3, 4, "Metric"], ["consistent labeling approaches", 9, 11, "Method"], ["automated surveillance systems", 19, 21, "Task"], ["PTZ cameras", 31, 32, "OtherScientificTerm"]], "relations": [[9, 11, 3, 4, "USED-FOR"]]},
{"sentence": "This paper presents a new two-pass algorithm for Extra Large -LRB- more than 1M words -RRB- Vocabulary COntinuous Speech recognition based on the Information Retrieval -LRB- ELVIRCOS -RRB- .", "ner": [["two-pass algorithm", 5, 6, "Generic"], ["Extra Large -LRB- more than 1M words -RRB- Vocabulary COntinuous Speech recognition", 8, 19, "Task"], ["Information Retrieval -LRB- ELVIRCOS -RRB-", 23, 27, "Task"]], "relations": [[5, 6, 8, 19, "USED-FOR"], [23, 27, 5, 6, "USED-FOR"]]},
{"sentence": "The principle of this approach is to decompose a recognition process into two passes where the first pass builds the words subset for the second pass recognition by using information retrieval procedure .", "ner": [["approach", 4, 4, "Generic"], ["recognition process", 9, 10, "Method"], ["passes", 13, 13, "Generic"], ["first pass", 16, 17, "Generic"], ["words subset", 20, 21, "OtherScientificTerm"], ["second pass recognition", 24, 26, "Generic"], ["information retrieval procedure", 29, 31, "Method"]], "relations": [[16, 17, 13, 13, "HYPONYM-OF"], [24, 26, 13, 13, "HYPONYM-OF"], [29, 31, 24, 26, "USED-FOR"]]},
{"sentence": "Word graph composition for continuous speech is presented .", "ner": [["Word graph composition", 0, 2, "Method"], ["continuous speech", 4, 5, "Material"]], "relations": [[0, 2, 4, 5, "USED-FOR"]]},
{"sentence": "With this approach a high performances for large vocabulary speech recognition can be obtained .", "ner": [["approach", 2, 2, "Generic"], ["large vocabulary speech recognition", 7, 10, "Task"]], "relations": [[2, 2, 7, 10, "USED-FOR"]]},
{"sentence": "This paper describes our work on classification of outdoor scenes .", "ner": [["classification of outdoor scenes", 6, 9, "Task"]], "relations": []},
{"sentence": "First , images are partitioned into regions using one-class classification and patch-based clustering algorithms where one-class classifiers model the regions with relatively uniform color and texture properties , and clustering of patches aims to detect structures in the remaining regions .", "ner": [["one-class classification", 8, 9, "Method"], ["patch-based clustering algorithms", 11, 13, "Method"], ["one-class classifiers", 15, 16, "Method"], ["uniform color and texture properties", 22, 26, "OtherScientificTerm"], ["clustering of patches", 29, 31, "Method"]], "relations": [[11, 13, 8, 9, "CONJUNCTION"], [22, 26, 15, 16, "USED-FOR"]]},
{"sentence": "Next , the resulting regions are clustered to obtain a codebook of region types , and two models are constructed for scene representation : a '' bag of individual regions '' representation where each region is regarded separately , and a '' bag of region pairs '' representation where regions with particular spatial relationships are considered together .", "ner": [["codebook of region types", 10, 13, "OtherScientificTerm"], ["models", 17, 17, "Generic"], ["scene representation", 21, 22, "Method"], ["spatial relationships", 52, 53, "OtherScientificTerm"]], "relations": [[17, 17, 21, 22, "USED-FOR"]]},
{"sentence": "Given these representations , scene classification is done using Bayesian classifiers .", "ner": [["scene classification", 4, 5, "Task"], ["Bayesian classifiers", 9, 10, "Method"]], "relations": [[9, 10, 4, 5, "USED-FOR"]]},
{"sentence": "We also propose a novel region selection algorithm that identifies region types that are frequently found in a particular class of scenes but rarely exist in other classes , and also consistently occur together in the same class of scenes .", "ner": [["region selection algorithm", 5, 7, "Method"]], "relations": []},
{"sentence": "Experiments on the LabelMe data set showed that the proposed models significantly out-perform a baseline global feature-based approach .", "ner": [["LabelMe data set", 3, 5, "Material"], ["models", 10, 10, "Generic"], ["baseline global feature-based approach", 14, 17, "Method"]], "relations": [[3, 5, 10, 10, "EVALUATE-FOR"], [3, 5, 14, 17, "EVALUATE-FOR"], [10, 10, 14, 17, "COMPARE"]]},
{"sentence": "In this paper , we introduce a generative probabilistic optical character recognition -LRB- OCR -RRB- model that describes an end-to-end process in the noisy channel framework , progressing from generation of true text through its transformation into the noisy output of an OCR system .", "ner": [["generative probabilistic optical character recognition -LRB- OCR -RRB- model", 7, 15, "Method"], ["noisy channel framework", 23, 25, "Method"], ["OCR system", 42, 43, "OtherScientificTerm"]], "relations": []},
{"sentence": "The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .", "ner": [["model", 1, 1, "Generic"], ["error correction", 7, 8, "Task"], ["post-processing", 14, 14, "Task"], ["output of black-box OCR systems", 16, 20, "OtherScientificTerm"], ["OCR systems", 19, 20, "OtherScientificTerm"], ["it", 25, 25, "Generic"], ["NLP tasks", 29, 30, "Task"]], "relations": [[1, 1, 7, 8, "USED-FOR"], [1, 1, 14, 14, "USED-FOR"], [14, 14, 7, 8, "PART-OF"], [25, 25, 29, 30, "USED-FOR"]]},
{"sentence": "We present an implementation of the model based on finite-state models , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .", "ner": [["model", 6, 6, "Generic"], ["finite-state models", 9, 10, "Method"], ["model", 14, 14, "Generic"], ["character and word error rate", 20, 24, "Metric"], ["automatic extraction of translation lexicons", 31, 35, "Task"], ["printed text", 37, 38, "Material"]], "relations": [[9, 10, 6, 6, "USED-FOR"], [20, 24, 14, 14, "EVALUATE-FOR"], [31, 35, 14, 14, "EVALUATE-FOR"], [37, 38, 31, 35, "USED-FOR"]]},
{"sentence": "We present a framework for word alignment based on log-linear models .", "ner": [["framework", 3, 3, "Generic"], ["word alignment", 5, 6, "Task"], ["log-linear models", 9, 10, "Method"]], "relations": [[3, 3, 5, 6, "USED-FOR"], [9, 10, 3, 3, "USED-FOR"]]},
{"sentence": "All knowledge sources are treated as feature functions , which depend on the source langauge sentence , the target language sentence and possible additional variables .", "ner": [["knowledge sources", 1, 2, "Material"], ["feature functions", 6, 7, "OtherScientificTerm"]], "relations": [[1, 2, 6, 7, "USED-FOR"]]},
{"sentence": "Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information .", "ner": [["Log-linear models", 0, 1, "Method"], ["statistical alignment models", 3, 5, "Method"], ["syntactic information", 12, 13, "OtherScientificTerm"]], "relations": [[0, 1, 3, 5, "USED-FOR"], [12, 13, 0, 1, "USED-FOR"]]},
{"sentence": "In this paper , we use IBM Model 3 alignment probabilities , POS correspondence , and bilingual dictionary coverage as features .", "ner": [["IBM Model 3 alignment probabilities", 6, 10, "OtherScientificTerm"], ["POS correspondence", 12, 13, "OtherScientificTerm"], ["bilingual dictionary coverage", 16, 18, "OtherScientificTerm"], ["features", 20, 20, "OtherScientificTerm"]], "relations": [[6, 10, 12, 13, "CONJUNCTION"], [6, 10, 20, 20, "USED-FOR"], [12, 13, 16, 18, "CONJUNCTION"], [12, 13, 20, 20, "USED-FOR"], [16, 18, 20, 20, "USED-FOR"]]},
{"sentence": "Our experiments show that log-linear models significantly outperform IBM translation models .", "ner": [["log-linear models", 4, 5, "Method"], ["IBM translation models", 8, 10, "Method"]], "relations": [[4, 5, 8, 10, "COMPARE"]]},
{"sentence": "Hough voting in a geometric transformation space allows us to realize spatial verification , but remains sensitive to feature detection errors because of the inflexible quan-tization of single feature correspondences .", "ner": [["Hough voting", 0, 1, "Method"], ["geometric transformation space", 4, 6, "OtherScientificTerm"], ["spatial verification", 11, 12, "Task"], ["feature detection errors", 18, 20, "OtherScientificTerm"], ["inflexible quan-tization of single feature correspondences", 24, 29, "OtherScientificTerm"]], "relations": [[0, 1, 11, 12, "USED-FOR"], [4, 6, 0, 1, "FEATURE-OF"]]},
{"sentence": "To handle this problem , we propose a new method , called adaptive dither voting , for robust spatial verification .", "ner": [["method", 9, 9, "Generic"], ["adaptive dither voting", 12, 14, "Method"], ["robust spatial verification", 17, 19, "Task"]], "relations": [[9, 9, 17, 19, "USED-FOR"]]},
{"sentence": "For each correspondence , instead of hard-mapping it to a single transformation , the method augments its description by using multiple dithered transformations that are deterministically generated by the other correspondences .", "ner": [["method", 14, 14, "Generic"], ["multiple dithered transformations", 20, 22, "OtherScientificTerm"]], "relations": [[20, 22, 14, 14, "USED-FOR"]]},
{"sentence": "The method reduces the probability of losing correspondences during transformation quantization , and provides high robustness as regards mismatches by imposing three geometric constraints on the dithering process .", "ner": [["method", 1, 1, "Generic"], ["transformation quantization", 9, 10, "OtherScientificTerm"], ["regards mismatches", 17, 18, "OtherScientificTerm"], ["geometric constraints", 22, 23, "OtherScientificTerm"], ["dithering process", 26, 27, "OtherScientificTerm"]], "relations": []},
{"sentence": "We also propose exploiting the non-uniformity of a Hough histogram as the spatial similarity to handle multiple matching surfaces .", "ner": [["non-uniformity", 5, 5, "OtherScientificTerm"], ["Hough histogram", 8, 9, "Method"], ["spatial similarity", 12, 13, "OtherScientificTerm"], ["multiple matching surfaces", 16, 18, "OtherScientificTerm"]], "relations": [[5, 5, 8, 9, "FEATURE-OF"], [5, 5, 16, 18, "USED-FOR"]]},
{"sentence": "Extensive experiments conducted on four datasets show the superiority of our method .", "ner": [["method", 11, 11, "Generic"]], "relations": []},
{"sentence": "The method outperforms its state-of-the-art counterparts in both accuracy and scalability , especially when it comes to the retrieval of small , rotated objects .", "ner": [["method", 1, 1, "Generic"], ["counterparts", 5, 5, "Generic"], ["accuracy", 8, 8, "Metric"], ["scalability", 10, 10, "Metric"], ["retrieval of small , rotated objects", 18, 23, "Task"]], "relations": [[1, 1, 18, 23, "USED-FOR"], [5, 5, 1, 1, "COMPARE"], [8, 8, 1, 1, "EVALUATE-FOR"], [8, 8, 5, 5, "EVALUATE-FOR"], [10, 10, 1, 1, "EVALUATE-FOR"], [10, 10, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "We propose a novel technique called bispectral photo-metric stereo that makes effective use of fluorescence for shape reconstruction .", "ner": [["technique", 4, 4, "Generic"], ["bispectral photo-metric stereo", 6, 8, "Method"], ["fluorescence", 14, 14, "OtherScientificTerm"], ["shape reconstruction", 16, 17, "Task"]], "relations": [[6, 8, 16, 17, "USED-FOR"], [14, 14, 6, 8, "USED-FOR"]]},
{"sentence": "Fluorescence is a common phenomenon occurring in many objects from natural gems and corals , to fluorescent dyes used in clothing .", "ner": [["Fluorescence", 0, 0, "OtherScientificTerm"], ["natural gems", 10, 11, "OtherScientificTerm"], ["fluorescent dyes", 16, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "One of the important characteristics of fluorescence is its wavelength-shifting behavior : fluorescent materials absorb light at a certain wavelength and then reemit it at longer wavelengths .", "ner": [["fluorescence", 6, 6, "OtherScientificTerm"], ["fluorescent materials", 12, 13, "OtherScientificTerm"]], "relations": []},
{"sentence": "Due to the complexity of its emission process , fluo-rescence tends to be excluded from most algorithms in computer vision and image processing .", "ner": [["complexity", 3, 3, "Metric"], ["emission process", 6, 7, "Method"], ["fluo-rescence", 9, 9, "OtherScientificTerm"], ["algorithms", 16, 16, "Generic"], ["computer vision", 18, 19, "Task"], ["image processing", 21, 22, "Task"]], "relations": [[3, 3, 6, 7, "EVALUATE-FOR"], [16, 16, 18, 19, "USED-FOR"], [16, 16, 21, 22, "USED-FOR"], [18, 19, 21, 22, "CONJUNCTION"]]},
{"sentence": "In this paper , we show that there is a strong similarity between fluorescence and ideal diffuse reflection and that fluorescence can provide distinct clues on how to estimate an object 's shape .", "ner": [["similarity", 11, 11, "OtherScientificTerm"], ["fluorescence", 13, 13, "OtherScientificTerm"], ["diffuse reflection", 16, 17, "OtherScientificTerm"], ["fluorescence", 20, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "Moreover , fluorescence 's wavelength-shifting property enables us to estimate the shape of an object by applying photomet-ric stereo to emission-only images without suffering from specular reflection .", "ner": [["fluorescence 's wavelength-shifting property", 2, 5, "OtherScientificTerm"], ["shape", 11, 11, "OtherScientificTerm"], ["photomet-ric stereo", 17, 18, "Method"], ["emission-only images", 20, 21, "Material"], ["specular reflection", 25, 26, "OtherScientificTerm"]], "relations": [[2, 5, 11, 11, "USED-FOR"], [17, 18, 11, 11, "USED-FOR"], [20, 21, 17, 18, "USED-FOR"]]},
{"sentence": "This is the significant advantage of the fluorescence-based method over previous methods based on reflection .", "ner": [["fluorescence-based method", 7, 8, "Method"], ["methods", 11, 11, "Generic"], ["reflection", 14, 14, "OtherScientificTerm"]], "relations": [[11, 11, 7, 8, "COMPARE"]]},
{"sentence": "In this paper , we present an approach for learning a visual representation from the raw spatiotemporal signals in videos .", "ner": [["approach", 7, 7, "Generic"], ["visual representation", 11, 12, "Method"], ["raw spatiotemporal signals in videos", 15, 19, "Material"]], "relations": [[7, 7, 11, 12, "USED-FOR"], [15, 19, 11, 12, "USED-FOR"]]},
{"sentence": "Our representation is learned without supervision from semantic labels .", "ner": [["representation", 1, 1, "Generic"], ["supervision from semantic labels", 5, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "We formulate our method as an unsupervised sequential verification task , i.e. , we determine whether a sequence of frames from a video is in the correct temporal order .", "ner": [["method", 3, 3, "Generic"], ["unsupervised sequential verification task", 6, 9, "Task"], ["temporal order", 27, 28, "OtherScientificTerm"]], "relations": [[6, 9, 3, 3, "USED-FOR"]]},
{"sentence": "With this simple task and no semantic labels , we learn a powerful visual representation using a Convolutional Neural Network -LRB- CNN -RRB- .", "ner": [["task", 3, 3, "Generic"], ["semantic labels", 6, 7, "OtherScientificTerm"], ["visual representation", 13, 14, "Method"], ["Convolutional Neural Network -LRB- CNN -RRB-", 17, 22, "Method"]], "relations": [[3, 3, 13, 14, "USED-FOR"], [17, 22, 13, 14, "USED-FOR"]]},
{"sentence": "The representation contains complementary information to that learned from supervised image datasets like ImageNet .", "ner": [["representation", 1, 1, "Generic"], ["complementary information", 3, 4, "OtherScientificTerm"], ["supervised image datasets", 9, 11, "Material"], ["ImageNet", 13, 13, "Material"]], "relations": [[3, 4, 1, 1, "PART-OF"], [9, 11, 3, 4, "USED-FOR"], [13, 13, 9, 11, "HYPONYM-OF"]]},
{"sentence": "Qualitative results show that our method captures information that is temporally varying , such as human pose .", "ner": [["method", 5, 5, "Generic"], ["human pose", 15, 16, "OtherScientificTerm"]], "relations": [[5, 5, 15, 16, "USED-FOR"]]},
{"sentence": "When used as pre-training for action recognition , our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51 .", "ner": [["pre-training", 3, 3, "Method"], ["action recognition", 5, 6, "Task"], ["our method", 8, 9, "Method"], ["learning without external data", 14, 17, "Method"], ["benchmark datasets", 19, 20, "Generic"], ["UCF101", 22, 22, "Material"], ["HMDB51", 24, 24, "Material"]], "relations": [[3, 3, 5, 6, "USED-FOR"], [8, 9, 3, 3, "USED-FOR"], [8, 9, 14, 17, "COMPARE"], [19, 20, 8, 9, "EVALUATE-FOR"], [19, 20, 14, 17, "EVALUATE-FOR"], [22, 22, 19, 20, "HYPONYM-OF"], [22, 22, 24, 24, "CONJUNCTION"], [24, 24, 19, 20, "HYPONYM-OF"]]},
{"sentence": "To demonstrate its sensitivity to human pose , we show results for pose estimation on the FLIC and MPII datasets that are competitive , or better than approaches using significantly more supervision .", "ner": [["human pose", 5, 6, "OtherScientificTerm"], ["pose estimation", 12, 13, "Task"], ["FLIC and MPII datasets", 16, 19, "Material"], ["approaches", 27, 27, "Generic"], ["supervision", 31, 31, "OtherScientificTerm"]], "relations": [[16, 19, 12, 13, "EVALUATE-FOR"], [31, 31, 27, 27, "USED-FOR"]]},
{"sentence": "Our method can be combined with supervised representations to provide an additional boost in accuracy .", "ner": [["Our method", 0, 1, "OtherScientificTerm"], ["supervised representations", 6, 7, "Method"], ["accuracy", 14, 14, "Metric"]], "relations": [[6, 7, 0, 1, "CONJUNCTION"], [14, 14, 0, 1, "EVALUATE-FOR"]]},
{"sentence": "`` To explain complex phenomena , an explanation system must be able to select information from a formal representation of domain knowledge , organize the selected information into multisentential discourse plans , and realize the discourse plans in text .", "ner": [["explanation system", 7, 8, "Method"], ["multisentential discourse plans", 28, 30, "OtherScientificTerm"], ["discourse plans", 35, 36, "OtherScientificTerm"]], "relations": [[7, 8, 35, 36, "USED-FOR"]]},
{"sentence": "Although recent years have witnessed significant progress in the development of sophisticated computational mechanisms for explanation , empirical results have been limited .", "ner": [], "relations": []},
{"sentence": "This paper reports on a seven-year effort to empirically study explanation generation from semantically rich , large-scale knowledge bases .", "ner": [["explanation generation", 10, 11, "Task"], ["semantically rich , large-scale knowledge bases", 13, 18, "Material"]], "relations": [[13, 18, 10, 11, "USED-FOR"]]},
{"sentence": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of botanical anatomy , physiology , and development .", "ner": [["robust explanation system", 6, 8, "Method"], ["multisentential and multi-paragraph explanations", 11, 14, "OtherScientificTerm"], ["large-scale knowledge base", 18, 20, "Material"], ["botanical anatomy", 25, 26, "Material"], ["physiology", 28, 28, "Material"], ["development", 31, 31, "Material"]], "relations": [[6, 8, 11, 14, "USED-FOR"], [18, 20, 6, 8, "USED-FOR"], [25, 26, 18, 20, "FEATURE-OF"], [25, 26, 28, 28, "CONJUNCTION"], [28, 28, 18, 20, "FEATURE-OF"], [28, 28, 31, 31, "CONJUNCTION"], [31, 31, 18, 20, "FEATURE-OF"]]},
{"sentence": "We introduce the evaluation methodology and describe how performance was assessed with this methodology in the most extensive empirical evaluation conducted on an explanation system .", "ner": [["evaluation methodology", 3, 4, "Generic"], ["methodology", 13, 13, "Generic"], ["explanation system", 23, 24, "Method"]], "relations": [[13, 13, 23, 24, "EVALUATE-FOR"]]},
{"sentence": "In this evaluation , scored within '' `` half a grade '' '' of domain experts , and its performance exceeded that of one of the domain experts . ''", "ner": [["evaluation", 2, 2, "Generic"]], "relations": []},
{"sentence": "We present an operable definition of focus which is argued to be of a cognito-pragmatic nature and explore how it is determined in discourse in a formalized manner .", "ner": [["operable definition", 3, 4, "Generic"], ["cognito-pragmatic nature", 14, 15, "OtherScientificTerm"], ["it", 19, 19, "Generic"], ["discourse", 23, 23, "OtherScientificTerm"]], "relations": [[14, 15, 3, 4, "FEATURE-OF"]]},
{"sentence": "For this purpose , a file card model of discourse model and knowledge store is introduced enabling the decomposition and formal representation of its determination process as a programmable algorithm -LRB- FDA -RRB- .", "ner": [["a file card model of discourse model and knowledge store", 4, 13, "Method"], ["discourse model", 9, 10, "Method"], ["knowledge store", 12, 13, "OtherScientificTerm"], ["determination process", 24, 25, "OtherScientificTerm"], ["programmable algorithm", 28, 29, "Method"], ["FDA", 31, 31, "Method"]], "relations": [[31, 31, 28, 29, "HYPONYM-OF"]]},
{"sentence": "Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse-level construct into speech synthesis systems , in particular , concept-to-speech systems , is also briefly discussed .", "ner": [["social and cognitive psychology", 3, 6, "OtherScientificTerm"], ["FDA", 18, 18, "OtherScientificTerm"], ["discourse-level construct", 21, 22, "OtherScientificTerm"], ["speech synthesis systems", 24, 26, "Method"], ["concept-to-speech systems", 31, 32, "Method"]], "relations": [[18, 18, 21, 22, "USED-FOR"], [21, 22, 24, 26, "PART-OF"], [31, 32, 24, 26, "HYPONYM-OF"]]},
{"sentence": "Conditional Random Field models have proved effective for several low-level computer vision problems .", "ner": [["Conditional Random Field models", 0, 3, "Method"], ["low-level computer vision problems", 9, 12, "Task"]], "relations": []},
{"sentence": "Inference in these models involves solving a combinatorial optimization problem , with methods such as graph cuts , belief propagation .", "ner": [["Inference", 0, 0, "Task"], ["models", 3, 3, "Generic"], ["combinatorial optimization problem", 7, 9, "Task"], ["methods", 12, 12, "Generic"], ["graph cuts", 15, 16, "Method"], ["belief propagation", 18, 19, "Method"]], "relations": [[0, 0, 3, 3, "USED-FOR"], [7, 9, 0, 0, "PART-OF"], [12, 12, 7, 9, "USED-FOR"], [15, 16, 12, 12, "USED-FOR"], [15, 16, 18, 19, "CONJUNCTION"], [18, 19, 12, 12, "USED-FOR"]]},
{"sentence": "Although several methods have been proposed to learn the model parameters from training data , they suffer from various drawbacks .", "ner": [["model parameters", 9, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "Learning these parameters involves computing the partition function , which is intractable .", "ner": [["parameters", 2, 2, "Generic"], ["partition function", 6, 7, "OtherScientificTerm"]], "relations": []},
{"sentence": "To overcome this , state-of-the-art structured learning methods frame the problem as one of large margin estimation .", "ner": [["structured learning methods", 5, 7, "Method"], ["problem", 10, 10, "Generic"], ["large margin estimation", 14, 16, "Task"]], "relations": [[5, 7, 10, 10, "USED-FOR"], [14, 16, 10, 10, "USED-FOR"]]},
{"sentence": "Iterative solutions have been proposed to solve the resulting convex optimization problem .", "ner": [["Iterative solutions", 0, 1, "Method"], ["convex optimization problem", 9, 11, "Task"]], "relations": [[0, 1, 9, 11, "USED-FOR"]]},
{"sentence": "Each iteration involves solving an inference problem over all the labels , which limits the efficiency of these structured methods .", "ner": [["inference problem", 5, 6, "Task"], ["structured methods", 18, 19, "Generic"]], "relations": []},
{"sentence": "In this paper we present an efficient large margin piece-wise learning method which is widely applicable .", "ner": [["large margin piece-wise learning method", 7, 11, "Method"]], "relations": []},
{"sentence": "We show how the resulting optimization problem can be reduced to an equivalent convex problem with a small number of constraints , and solve it using an efficient scheme .", "ner": [["optimization problem", 5, 6, "Task"], ["convex problem", 13, 14, "Task"]], "relations": [[13, 14, 5, 6, "USED-FOR"]]},
{"sentence": "Our method is both memory and computationally efficient .", "ner": [["method", 1, 1, "Generic"]], "relations": []},
{"sentence": "We show results on publicly available standard datasets .", "ner": [], "relations": []},
{"sentence": "Interpreting metaphors is an integral and inescapable process in human understanding of natural language .", "ner": [["Interpreting metaphors", 0, 1, "Task"], ["human understanding of natural language", 9, 13, "Task"]], "relations": [[0, 1, 9, 13, "HYPONYM-OF"]]},
{"sentence": "This paper discusses a method of analyzing metaphors based on the existence of a small number of generalized metaphor mappings .", "ner": [["method", 4, 4, "Generic"], ["analyzing metaphors", 6, 7, "Task"], ["generalized metaphor mappings", 17, 19, "Method"]], "relations": [[4, 4, 6, 7, "USED-FOR"], [17, 19, 6, 7, "USED-FOR"]]},
{"sentence": "Each generalized metaphor contains a recognition network , a basic mapping , additional transfer mappings , and an implicit intention component .", "ner": [["generalized metaphor", 1, 2, "OtherScientificTerm"], ["recognition network", 5, 6, "Method"], ["basic mapping", 9, 10, "Method"], ["transfer mappings", 13, 14, "Method"], ["implicit intention component", 18, 20, "Method"]], "relations": [[5, 6, 1, 2, "PART-OF"], [5, 6, 9, 10, "CONJUNCTION"], [9, 10, 1, 2, "PART-OF"], [13, 14, 1, 2, "PART-OF"], [13, 14, 9, 10, "CONJUNCTION"], [13, 14, 18, 20, "CONJUNCTION"], [18, 20, 1, 2, "PART-OF"]]},
{"sentence": "It is argued that the method reduces metaphor interpretation from a reconstruction to a recognition task .", "ner": [["method", 5, 5, "Generic"], ["metaphor interpretation", 7, 8, "Task"], ["reconstruction", 11, 11, "Task"], ["recognition task", 14, 15, "Task"]], "relations": [[5, 5, 7, 8, "USED-FOR"], [14, 15, 7, 8, "USED-FOR"]]},
{"sentence": "Implications towards automating certain aspects of language learning are also discussed .", "ner": [["language learning", 6, 7, "Task"]], "relations": []},
{"sentence": "This study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .", "ner": [["method to automatically acquire paraphrases", 4, 8, "Method"], ["bilingual corpora", 10, 11, "Material"], ["bilingual dependency relations", 16, 18, "OtherScientificTerm"], ["monolingual dependency parse", 23, 25, "OtherScientificTerm"], ["statistical alignment techniques", 33, 35, "Method"]], "relations": [[10, 11, 4, 8, "USED-FOR"], [16, 18, 4, 8, "USED-FOR"], [23, 25, 16, 18, "USED-FOR"], [33, 35, 16, 18, "USED-FOR"]]},
{"sentence": "Since the paraphrasing method is capable of clearly disambiguating the sense of an original phrase using the bilingual context of dependency relation , it would be possible to obtain interchangeable paraphrases under a given context .", "ner": [["paraphrasing method", 2, 3, "Method"], ["bilingual context of dependency relation", 17, 21, "OtherScientificTerm"], ["paraphrases", 30, 30, "OtherScientificTerm"]], "relations": [[17, 21, 2, 3, "USED-FOR"]]},
{"sentence": "Also , we provide an advanced method to acquire generalized translation knowledge using the extracted paraphrases .", "ner": [["method", 6, 6, "Generic"], ["generalized translation knowledge", 9, 11, "OtherScientificTerm"], ["paraphrases", 15, 15, "OtherScientificTerm"]], "relations": [[6, 6, 9, 11, "USED-FOR"], [15, 15, 6, 6, "USED-FOR"]]},
{"sentence": "We applied the method to acquire the generalized translation knowledge for Korean-English translation .", "ner": [["method", 3, 3, "Generic"], ["generalized translation knowledge", 7, 9, "OtherScientificTerm"], ["Korean-English translation", 11, 12, "Task"]], "relations": [[3, 3, 7, 9, "USED-FOR"], [7, 9, 11, 12, "USED-FOR"]]},
{"sentence": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .", "ner": [["parallel corpora of a Korean and English language pairs", 3, 11, "Material"], ["paraphrasing method", 17, 18, "Method"], ["paraphrases", 21, 21, "OtherScientificTerm"], ["precision", 24, 24, "Metric"], ["Korean", 33, 33, "Material"], ["English", 35, 35, "Material"], ["translation knowledge", 39, 40, "OtherScientificTerm"], ["bilingual corpora", 44, 45, "Material"], ["paraphrases", 52, 52, "OtherScientificTerm"], ["compression ratio", 57, 58, "Metric"]], "relations": [[17, 18, 21, 21, "USED-FOR"], [24, 24, 17, 18, "EVALUATE-FOR"], [33, 33, 35, 35, "CONJUNCTION"], [44, 45, 39, 40, "USED-FOR"], [52, 52, 39, 40, "USED-FOR"], [57, 58, 39, 40, "EVALUATE-FOR"]]},
{"sentence": "We provide a logical definition of Minimalist grammars , that are Stabler 's formalization of Chomsky 's minimalist program .", "ner": [["logical definition of Minimalist grammars", 3, 7, "Method"], ["Stabler 's formalization of Chomsky 's minimalist program", 11, 18, "Method"]], "relations": [[11, 18, 3, 7, "HYPONYM-OF"]]},
{"sentence": "Our logical definition leads to a neat relation to categorial grammar , -LRB- yielding a treatment of Montague semantics -RRB- , a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data -LRB- based on a typing-algorithm and type-unification -RRB- .", "ner": [["logical definition", 1, 2, "Method"], ["categorial grammar", 9, 10, "Method"], ["Montague semantics", 17, 18, "OtherScientificTerm"], ["parsing-as-deduction", 22, 22, "Method"], ["resource sensitive logic", 25, 27, "OtherScientificTerm"], ["learning algorithm", 31, 32, "Method"], ["structured data", 34, 35, "Material"], ["typing-algorithm", 40, 40, "Method"], ["type-unification", 42, 42, "Method"]], "relations": [[1, 2, 17, 18, "USED-FOR"], [22, 22, 25, 27, "USED-FOR"], [34, 35, 31, 32, "USED-FOR"], [40, 40, 31, 32, "USED-FOR"], [40, 40, 42, 42, "CONJUNCTION"], [42, 42, 31, 32, "USED-FOR"]]},
{"sentence": "Here we emphasize the connection to Montague semantics which can be viewed as a formal computation of the logical form .", "ner": [["Montague semantics", 6, 7, "OtherScientificTerm"], ["formal computation of the logical form", 14, 19, "OtherScientificTerm"], ["logical form", 18, 19, "OtherScientificTerm"]], "relations": []},
{"sentence": "There are several approaches that model information extraction as a token classification task , using various tagging strategies to combine multiple tokens .", "ner": [["approaches", 3, 3, "Generic"], ["information extraction", 6, 7, "Task"], ["token classification task", 10, 12, "Task"], ["tagging strategies", 16, 17, "Method"]], "relations": [[3, 3, 6, 7, "USED-FOR"], [6, 7, 10, 12, "HYPONYM-OF"], [16, 17, 10, 12, "USED-FOR"]]},
{"sentence": "We describe the tagging strategies that can be found in the literature and evaluate their relative performances .", "ner": [["tagging strategies", 3, 4, "Method"]], "relations": []},
{"sentence": "We also introduce a new strategy , called Begin/After tagging or BIA , and show that it is competitive to the best other strategies .", "ner": [["strategy", 5, 5, "Generic"], ["Begin/After tagging", 8, 9, "Method"], ["BIA", 11, 11, "Method"], ["it", 16, 16, "Generic"], ["strategies", 23, 23, "Generic"]], "relations": [[16, 16, 23, 23, "COMPARE"]]},
{"sentence": "The PRC Adaptive Knowledge-based Text Understanding System -LRB- PAKTUS -RRB- has been under development as an Independent Research and Development project at PRC since 1984 .", "ner": [["PRC Adaptive Knowledge-based Text Understanding System -LRB- PAKTUS -RRB-", 1, 9, "Method"]], "relations": []},
{"sentence": "The objective is a generic system of tools , including a core English lexicon , grammar , and concept representations , for building natural language processing -LRB- NLP -RRB- systems for text understanding .", "ner": [["system", 5, 5, "Generic"], ["core English lexicon", 11, 13, "OtherScientificTerm"], ["grammar", 15, 15, "Method"], ["concept representations", 18, 19, "Method"], ["natural language processing -LRB- NLP -RRB- systems", 23, 29, "Method"], ["text understanding", 31, 32, "Task"]], "relations": [[5, 5, 23, 29, "USED-FOR"], [11, 13, 5, 5, "PART-OF"], [15, 15, 5, 5, "PART-OF"], [18, 19, 5, 5, "PART-OF"], [23, 29, 31, 32, "USED-FOR"]]},
{"sentence": "Systems built with PAKTUS are intended to generate input to knowledge based systems ordata base systems .", "ner": [["PAKTUS", 3, 3, "Method"], ["knowledge based systems", 10, 12, "Task"]], "relations": [[3, 3, 10, 12, "USED-FOR"]]},
{"sentence": "Input to the NLP system is typically derived from an existing electronic message stream , such as a news wire .", "ner": [["NLP system", 3, 4, "Method"], ["electronic message stream", 11, 13, "Material"], ["news wire", 18, 19, "Material"]], "relations": [[11, 13, 3, 4, "USED-FOR"], [18, 19, 11, 13, "HYPONYM-OF"]]},
{"sentence": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "ner": [["PAKTUS", 0, 0, "Method"], ["JINTACCS messages", 14, 15, "Material"], ["RAINFORM messages", 17, 18, "Material"], ["news reports", 20, 21, "OtherScientificTerm"], ["event", 27, 27, "OtherScientificTerm"], ["financial transfers", 31, 32, "Material"], ["terrorist acts", 34, 35, "Material"], ["sublanguage and domain-specific grammar", 41, 44, "Method"], ["words", 46, 46, "Generic"], ["conceptual mappings", 48, 49, "OtherScientificTerm"], ["discourse patterns", 52, 53, "OtherScientificTerm"]], "relations": [[0, 0, 14, 15, "USED-FOR"], [0, 0, 17, 18, "USED-FOR"], [0, 0, 20, 21, "USED-FOR"], [14, 15, 17, 18, "CONJUNCTION"], [17, 18, 20, 21, "CONJUNCTION"], [27, 27, 20, 21, "FEATURE-OF"], [31, 32, 27, 27, "HYPONYM-OF"], [31, 32, 34, 35, "CONJUNCTION"], [34, 35, 27, 27, "HYPONYM-OF"], [41, 44, 0, 0, "USED-FOR"], [41, 44, 46, 46, "CONJUNCTION"], [46, 46, 0, 0, "USED-FOR"], [46, 46, 48, 49, "CONJUNCTION"], [48, 49, 0, 0, "USED-FOR"], [48, 49, 52, 53, "CONJUNCTION"], [52, 53, 0, 0, "USED-FOR"]]},
{"sentence": "The long-term goal is a system that can support the processing of relatively long discourses in domains that are fairly broad with a high rate of success .", "ner": [], "relations": []},
{"sentence": "A major axis of research at LIMSI is directed at multilingual , speaker-independent , large vocabulary speech dictation .", "ner": [["multilingual , speaker-independent , large vocabulary speech dictation", 10, 17, "Task"]], "relations": []},
{"sentence": "In this paper the LIMSI recognizer which was evaluated in the ARPA NOV93 CSR test is described , and experimental results on the WSJ and BREF corpora under closely matched conditions are reported .", "ner": [["LIMSI recognizer", 4, 5, "Method"], ["ARPA NOV93 CSR test", 11, 14, "Material"], ["WSJ and BREF corpora", 23, 26, "Material"]], "relations": [[11, 14, 4, 5, "EVALUATE-FOR"], [23, 26, 4, 5, "EVALUATE-FOR"]]},
{"sentence": "For both corpora word recognition experiments were carried out with vocabularies containing up to 20k words .", "ner": [["corpora", 2, 2, "Generic"], ["word recognition", 3, 4, "Task"]], "relations": [[2, 2, 3, 4, "EVALUATE-FOR"]]},
{"sentence": "The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .", "ner": [["recognizer", 1, 1, "Generic"], ["continuous density HMM", 5, 7, "Method"], ["Gaussian mixture", 9, 10, "Method"], ["acoustic modeling", 12, 13, "Task"], ["n-gram statistics", 15, 16, "Method"], ["newspaper texts", 20, 21, "Material"], ["language modeling", 23, 24, "Task"]], "relations": [[5, 7, 1, 1, "USED-FOR"], [5, 7, 9, 10, "CONJUNCTION"], [5, 7, 12, 13, "USED-FOR"], [5, 7, 15, 16, "CONJUNCTION"], [9, 10, 12, 13, "USED-FOR"], [15, 16, 1, 1, "USED-FOR"], [15, 16, 23, 24, "USED-FOR"], [20, 21, 15, 16, "EVALUATE-FOR"]]},
{"sentence": "The recognizer uses a time-synchronous graph-search strategy which is shown to still be viable with a 20k-word vocabulary when used with bigram back-off language models .", "ner": [["recognizer", 1, 1, "Generic"], ["time-synchronous graph-search strategy", 4, 6, "Method"], ["bigram back-off language models", 21, 24, "Method"]], "relations": [[4, 6, 1, 1, "USED-FOR"], [21, 24, 1, 1, "USED-FOR"], [21, 24, 4, 6, "CONJUNCTION"]]},
{"sentence": "A second forward pass , which makes use of a word graph generated with the bigram , incorporates a trigram language model .", "ner": [["word graph", 10, 11, "OtherScientificTerm"], ["bigram", 15, 15, "Method"], ["trigram language model", 19, 21, "Method"]], "relations": [[15, 15, 10, 11, "USED-FOR"], [19, 21, 10, 11, "CONJUNCTION"]]},
{"sentence": "Acoustic modeling uses cepstrum-based features , context-dependent phone models -LRB- intra and interword -RRB- , phone duration models , and sex-dependent models .", "ner": [["Acoustic modeling", 0, 1, "Task"], ["cepstrum-based features", 3, 4, "OtherScientificTerm"], ["context-dependent phone models -LRB- intra and interword -RRB-", 6, 13, "Method"], ["phone duration models", 15, 17, "Method"], ["sex-dependent models", 20, 21, "Method"]], "relations": [[3, 4, 0, 1, "USED-FOR"], [3, 4, 6, 13, "CONJUNCTION"], [6, 13, 0, 1, "USED-FOR"], [6, 13, 15, 17, "CONJUNCTION"], [15, 17, 0, 1, "USED-FOR"], [15, 17, 20, 21, "CONJUNCTION"], [20, 21, 0, 1, "USED-FOR"]]},
{"sentence": "The co-occurrence pattern , a combination of binary or local features , is more discriminative than individual features and has shown its advantages in object , scene , and action recognition .", "ner": [["co-occurrence pattern", 1, 2, "OtherScientificTerm"], ["binary or local features", 7, 10, "OtherScientificTerm"], ["object , scene , and action recognition", 24, 30, "Task"]], "relations": [[1, 2, 24, 30, "USED-FOR"], [7, 10, 1, 2, "PART-OF"]]},
{"sentence": "We discuss two types of co-occurrence patterns that are complementary to each other , the conjunction -LRB- AND -RRB- and disjunction -LRB- OR -RRB- of binary features .", "ner": [["co-occurrence patterns", 5, 6, "OtherScientificTerm"], ["conjunction -LRB- AND -RRB- and disjunction -LRB- OR -RRB- of binary features", 15, 26, "OtherScientificTerm"]], "relations": []},
{"sentence": "The necessary condition of identifying discriminative co-occurrence patterns is firstly provided .", "ner": [["identifying discriminative co-occurrence patterns", 4, 7, "Task"], ["discriminative co-occurrence patterns", 5, 7, "OtherScientificTerm"]], "relations": []},
{"sentence": "Then we propose a novel data mining method to efficiently discover the optimal co-occurrence pattern with minimum empirical error , despite the noisy training dataset .", "ner": [["data mining method", 5, 7, "Method"], ["optimal co-occurrence pattern", 12, 14, "OtherScientificTerm"], ["minimum empirical error", 16, 18, "OtherScientificTerm"], ["noisy training dataset", 22, 24, "Material"]], "relations": [[5, 7, 12, 14, "USED-FOR"], [16, 18, 12, 14, "FEATURE-OF"], [22, 24, 5, 7, "USED-FOR"]]},
{"sentence": "This mining procedure of AND and OR patterns is readily integrated to boosting , which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps .", "ner": [["mining procedure", 1, 2, "Generic"], ["AND and OR patterns", 4, 7, "OtherScientificTerm"], ["boosting", 12, 12, "Method"], ["generalization ability", 17, 18, "Metric"], ["boosting decision trees", 22, 24, "Method"], ["boosting decision stumps", 26, 28, "Method"]], "relations": [[1, 2, 4, 7, "USED-FOR"], [4, 7, 12, 12, "PART-OF"], [12, 12, 22, 24, "COMPARE"], [12, 12, 26, 28, "COMPARE"], [17, 18, 12, 12, "EVALUATE-FOR"], [17, 18, 22, 24, "EVALUATE-FOR"], [17, 18, 26, 28, "EVALUATE-FOR"], [22, 24, 26, 28, "CONJUNCTION"]]},
{"sentence": "Our versatile experiments on object , scene , and action cat-egorization validate the advantages of the discovered dis-criminative co-occurrence patterns .", "ner": [["object , scene , and action cat-egorization", 4, 10, "Task"], ["dis-criminative co-occurrence patterns", 17, 19, "OtherScientificTerm"]], "relations": [[4, 10, 17, 19, "EVALUATE-FOR"]]},
{"sentence": "Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers , boosting and SVMs are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "ner": [["classifiers", 12, 12, "Method"], ["maximum entropy classifiers", 15, 17, "Method"], ["boosting", 19, 19, "Method"], ["SVMs", 21, 21, "Method"], ["language processing tasks", 25, 27, "Task"]], "relations": [[12, 12, 25, 27, "USED-FOR"], [15, 17, 12, 12, "HYPONYM-OF"], [15, 17, 19, 19, "CONJUNCTION"], [19, 19, 12, 12, "HYPONYM-OF"], [19, 19, 21, 21, "CONJUNCTION"], [21, 21, 12, 12, "HYPONYM-OF"]]},
{"sentence": "To further improve performance , various error correction mechanisms have been developed , but in practice , most of them can not be relied on to predictably improve performance on unseen data ; indeed , depending upon the test set , they are as likely to degrade accuracy as to improve it .", "ner": [["error correction mechanisms", 6, 8, "Method"]], "relations": []},
{"sentence": "This problem is especially severe if the base classifier has already been finely tuned .", "ner": [["base classifier", 7, 8, "Generic"]], "relations": []},
{"sentence": "In recent work , we introduced N-fold Templated Piped Correction , or NTPC -LRB- `` nitpick '' -RRB- , an intriguing error corrector that is designed to work in these extreme operating conditions .", "ner": [["N-fold Templated Piped Correction , or NTPC -LRB- `` nitpick '' -RRB-", 6, 17, "Method"], ["error corrector", 21, 22, "Method"]], "relations": [[6, 17, 21, 22, "HYPONYM-OF"]]},
{"sentence": "Despite its simplicity , it consistently and robustly improves the accuracy of existing highly accurate base models .", "ner": [["it", 4, 4, "Generic"], ["base models", 15, 16, "Generic"]], "relations": [[4, 4, 15, 16, "COMPARE"]]},
{"sentence": "This paper investigates some of the more surprising claims made by NTPC , and presents experiments supporting an Occam 's Razor argument that more complex models are damaging or unnecessary in practice .", "ner": [["NTPC", 11, 11, "Method"], ["Occam 's Razor argument", 18, 21, "OtherScientificTerm"]], "relations": []},
{"sentence": "A flexible parser can deal with input that deviates from its grammar , in addition to input that conforms to it .", "ner": [["flexible parser", 1, 2, "Method"], ["it", 20, 20, "Generic"]], "relations": []},
{"sentence": "Ideally , such a parser will correct the deviant input : sometimes , it will be unable to correct it at all ; at other times , correction will be possible , but only to within a range of ambiguous possibilities .", "ner": [["parser", 4, 4, "Method"]], "relations": []},
{"sentence": "This paper is concerned with such ambiguous situations , and with making it as easy as possible for the ambiguity to be resolved through consultation with the user of the parser - we presume interactive use .", "ner": [["ambiguity", 19, 19, "OtherScientificTerm"], ["parser", 30, 30, "Method"]], "relations": []},
{"sentence": "We show the importance of asking the user for clarification in as focused a way as possible .", "ner": [], "relations": []},
{"sentence": "Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with specialized parsing techniques for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .", "ner": [["construction-specific approach", 9, 10, "Method"], ["flexible parsing", 12, 13, "Task"], ["specialized parsing techniques", 16, 18, "Method"], ["construction", 23, 23, "Task"], ["ambiguity representations", 27, 28, "Method"], ["ambiguity", 33, 33, "OtherScientificTerm"], ["construction", 37, 37, "Task"]], "relations": [[9, 10, 12, 13, "USED-FOR"], [9, 10, 16, 18, "CONJUNCTION"], [16, 18, 23, 23, "USED-FOR"], [16, 18, 27, 28, "CONJUNCTION"], [27, 28, 33, 33, "USED-FOR"]]},
{"sentence": "A construction-specific approach also aids in task-specific language development by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism , thus greatly speeding the testing of changes to the language definition .", "ner": [["construction-specific approach", 1, 2, "Method"], ["task-specific language development", 6, 8, "Task"], ["uniform grammar formalism", 31, 33, "Method"]], "relations": [[1, 2, 6, 8, "USED-FOR"]]},
{"sentence": "A proposal to deal with French tenses in the framework of Discourse Representation Theory is presented , as it has been implemented for a fragment at the IMS .", "ner": [["proposal", 1, 1, "Generic"], ["French tenses", 5, 6, "OtherScientificTerm"], ["Discourse Representation Theory", 11, 13, "Method"], ["it", 18, 18, "Generic"], ["IMS", 27, 27, "Task"]], "relations": [[11, 13, 5, 6, "USED-FOR"], [18, 18, 27, 27, "USED-FOR"]]},
{"sentence": "It is based on the theory of tenses of H. Kamp and Ch .", "ner": [["It", 0, 0, "Generic"], ["theory of tenses", 5, 7, "OtherScientificTerm"]], "relations": [[5, 7, 0, 0, "USED-FOR"]]},
{"sentence": "Rohrer .", "ner": [], "relations": []},
{"sentence": "Instead of using operators to express the meaning of the tenses the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .", "ner": [["operators", 3, 3, "Generic"], ["meaning of the tenses", 7, 10, "OtherScientificTerm"], ["event structure", 50, 51, "OtherScientificTerm"]], "relations": [[3, 3, 7, 10, "USED-FOR"]]},
{"sentence": "Thereby a system of relevant times provided by the preceeding text and by the temporal adverbials of the sentence being processed is used .", "ner": [["system of relevant times", 2, 5, "Method"], ["preceeding text", 9, 10, "OtherScientificTerm"], ["temporal adverbials", 14, 15, "OtherScientificTerm"]], "relations": [[9, 10, 2, 5, "USED-FOR"], [9, 10, 14, 15, "CONJUNCTION"], [14, 15, 2, 5, "USED-FOR"]]},
{"sentence": "This system consists of one or more reference times and temporal perspective times , the speech time and the location time .", "ner": [["system", 1, 1, "Generic"], ["reference times", 7, 8, "OtherScientificTerm"], ["temporal perspective times", 10, 12, "OtherScientificTerm"], ["speech time", 15, 16, "OtherScientificTerm"], ["location time", 19, 20, "OtherScientificTerm"]], "relations": [[7, 8, 1, 1, "PART-OF"], [7, 8, 10, 12, "CONJUNCTION"], [10, 12, 1, 1, "PART-OF"], [10, 12, 15, 16, "CONJUNCTION"], [15, 16, 1, 1, "PART-OF"], [15, 16, 19, 20, "CONJUNCTION"], [19, 20, 1, 1, "PART-OF"]]},
{"sentence": "The special interest of our proposal is to establish a plausible choice of anchors for the new event out of the system of relevant times and to update this system of temporal coordinates correctly .", "ner": [["system of relevant times", 21, 24, "Method"], ["system of temporal coordinates", 29, 32, "Method"]], "relations": []},
{"sentence": "The problem of choice is largely neglected in the literature .", "ner": [], "relations": []},
{"sentence": "In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the resolution component and not in the process of syntactic analysis .", "ner": [["meaning of the tenses", 11, 14, "OtherScientificTerm"], ["resolution component", 19, 20, "Method"], ["syntactic analysis", 27, 28, "Method"]], "relations": [[19, 20, 11, 14, "USED-FOR"], [19, 20, 27, 28, "COMPARE"]]},
{"sentence": "The work presented in this paper is the first step in a project which aims to cluster and summarise electronic discussions in the context of help-desk applications .", "ner": [["electronic discussions", 19, 20, "Task"], ["help-desk applications", 25, 26, "Task"]], "relations": [[19, 20, 25, 26, "PART-OF"]]},
{"sentence": "The eventual objective of this project is to use these summaries to assist help-desk users and operators .", "ner": [], "relations": []},
{"sentence": "In this paper , we identify features of electronic discussions that influence the clustering process , and offer a filtering mechanism that removes undesirable influences .", "ner": [["features", 6, 6, "OtherScientificTerm"], ["electronic discussions", 8, 9, "Task"], ["clustering process", 13, 14, "Method"], ["filtering mechanism", 19, 20, "Method"]], "relations": [[6, 6, 8, 9, "FEATURE-OF"]]},
{"sentence": "We tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval .", "ner": [["clustering and filtering processes", 3, 6, "Method"], ["electronic newsgroup discussions", 8, 10, "Material"], ["experiments", 20, 20, "Generic"], ["coarse-level clustering", 22, 23, "Task"], ["information retrieval", 25, 26, "Task"]], "relations": [[8, 10, 3, 6, "EVALUATE-FOR"], [20, 20, 3, 6, "EVALUATE-FOR"], [20, 20, 3, 6, "EVALUATE-FOR"], [22, 23, 20, 20, "HYPONYM-OF"], [25, 26, 20, 20, "HYPONYM-OF"]]},
{"sentence": "The paper presents a method for word sense disambiguation based on parallel corpora .", "ner": [["method", 4, 4, "Generic"], ["word sense disambiguation", 6, 8, "Task"], ["parallel corpora", 11, 12, "Material"]], "relations": [[4, 4, 6, 8, "USED-FOR"], [11, 12, 4, 4, "USED-FOR"]]},
{"sentence": "The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .", "ner": [["method", 1, 1, "Generic"], ["word alignment", 6, 7, "Task"], ["word clustering", 9, 10, "Task"], ["automatic extraction of translation equivalents", 13, 17, "Task"], ["aligned wordnets", 23, 24, "Material"]], "relations": [[1, 1, 6, 7, "USED-FOR"], [1, 1, 9, 10, "USED-FOR"], [6, 7, 9, 10, "CONJUNCTION"], [13, 17, 1, 1, "USED-FOR"], [23, 24, 1, 1, "USED-FOR"]]},
{"sentence": "The wordnets are aligned to the Princeton Wordnet , according to the principles established by EuroWordNet .", "ner": [["wordnets", 1, 1, "Material"], ["Princeton Wordnet", 6, 7, "Material"], ["EuroWordNet", 15, 15, "Material"]], "relations": []},
{"sentence": "The evaluation of the WSD system , implementing the method described herein showed very encouraging results .", "ner": [["WSD system", 4, 5, "Method"], ["method", 9, 9, "Generic"]], "relations": []},
{"sentence": "The same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet .", "ner": [["system", 2, 2, "Generic"], ["alignment errors in multilingually aligned wordnets", 16, 21, "OtherScientificTerm"], ["multilingually aligned wordnets", 19, 21, "Material"], ["BalkaNet", 23, 23, "Material"], ["EuroWordNet", 25, 25, "Material"]], "relations": [[2, 2, 16, 21, "USED-FOR"], [23, 23, 19, 21, "HYPONYM-OF"], [23, 23, 25, 25, "CONJUNCTION"], [25, 25, 19, 21, "HYPONYM-OF"]]},
{"sentence": "This paper investigates critical configurations for projective reconstruction from multiple images taken by a camera moving in a straight line .", "ner": [["projective reconstruction", 6, 7, "Task"], ["images", 10, 10, "Material"]], "relations": [[10, 10, 6, 7, "USED-FOR"]]},
{"sentence": "Projective reconstruction refers to a determination of the 3D geometrical configuration of a set of 3D points and cameras , given only correspondences between points in the images .", "ner": [["Projective reconstruction", 0, 1, "Task"], ["3D geometrical configuration", 8, 10, "OtherScientificTerm"], ["3D points and cameras", 15, 18, "OtherScientificTerm"]], "relations": [[8, 10, 15, 18, "FEATURE-OF"]]},
{"sentence": "A configuration of points and cameras is critical if it can not be determined uniquely -LRB- up to a projective transform -RRB- from the image coordinates of the points .", "ner": [["image coordinates", 24, 25, "OtherScientificTerm"]], "relations": []},
{"sentence": "It is shown that a configuration consisting of any number of cameras lying on a straight line , and any number of points lying on a twisted cubic constitutes a critical configuration .", "ner": [], "relations": []},
{"sentence": "An alternative configuration consisting of a set of points and cameras all lying on a rational quartic curve exists .", "ner": [["rational quartic curve", 15, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "Porting a Natural Language Processing -LRB- NLP -RRB- system to a new domain remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracies of the new sublanguage .", "ner": [["Natural Language Processing -LRB- NLP -RRB- system", 2, 8, "Task"], ["new domain", 11, 12, "Material"], ["syntactic parsing", 19, 20, "Task"], ["grammar", 41, 41, "Method"], ["idiosyncracies of the new sublanguage", 44, 48, "OtherScientificTerm"]], "relations": [[2, 8, 11, 12, "USED-FOR"], [41, 41, 44, 48, "USED-FOR"]]},
{"sentence": "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional knowledge-based techniques with a corpus-based approach .", "ner": [["lexicalized grammar", 9, 10, "Method"], ["hybrid system", 24, 25, "Method"], ["knowledge-based techniques", 29, 30, "Method"], ["corpus-based approach", 33, 34, "Method"]], "relations": [[29, 30, 24, 25, "PART-OF"], [29, 30, 33, 34, "CONJUNCTION"], [33, 34, 24, 25, "PART-OF"]]},
{"sentence": "Chart parsing is directional in the sense that it works from the starting point -LRB- usually the beginning of the sentence -RRB- extending its activity usually in a rightward manner .", "ner": [], "relations": []},
{"sentence": "We shall introduce the concept of a chart that works outward from islands and makes sense of as much of the sentence as it is actually possible , and after that will lead to predictions of missing fragments .", "ner": [], "relations": []},
{"sentence": "So , for any place where the easily identifiable fragments occur in the sentence , the process will extend to both the left and the right of the islands , until possibly completely missing fragments are reached .", "ner": [], "relations": []},
{"sentence": "At that point , by virtue of the fact that both a left and a right context were found , heuristics can be introduced that predict the nature of the missing fragments .", "ner": [], "relations": []},
{"sentence": "Unification is often the appropriate method for expressing relations between representations in the form of feature structures ; however , there are circumstances in which a different approach is desirable .", "ner": [["Unification", 0, 0, "Method"], ["method", 5, 5, "Generic"], ["relations between representations", 8, 10, "Task"], ["feature structures", 15, 16, "OtherScientificTerm"], ["approach", 27, 27, "Generic"]], "relations": [[5, 5, 8, 10, "USED-FOR"], [15, 16, 8, 10, "USED-FOR"], [27, 27, 5, 5, "COMPARE"]]},
{"sentence": "A declarative formalism is presented which permits direct mappings of one feature structure into another , and illustrative examples are given of its application to areas of current interest .", "ner": [["declarative formalism", 1, 2, "Method"], ["direct mappings of one feature structure into another", 7, 14, "OtherScientificTerm"]], "relations": [[7, 14, 1, 2, "FEATURE-OF"]]},
{"sentence": "To support engaging human users in robust , mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program -LSB- 1 -RSB- is funding the development of a distributed message-passing infrastructure for dialogue systems which all Communicator participants are using .", "ner": [["mixed-initiative speech dialogue interactions", 8, 11, "Method"], ["dialogue systems", 18, 19, "Method"], ["distributed message-passing infrastructure", 34, 36, "Method"], ["dialogue systems", 38, 39, "Method"]], "relations": [[34, 36, 38, 39, "USED-FOR"]]},
{"sentence": "In this presentation , we describe the features of and requirements for a genuinely useful software infrastructure for this purpose .", "ner": [], "relations": []},
{"sentence": "We propose a novel limited-memory stochastic block BFGS update for incorporating enriched curvature information in stochastic approximation methods .", "ner": [["limited-memory stochastic block BFGS update", 4, 8, "Method"], ["incorporating enriched curvature information in stochastic approximation methods", 10, 17, "Task"]], "relations": [[4, 8, 10, 17, "USED-FOR"]]},
{"sentence": "In our method , the estimate of the inverse Hessian matrix that is maintained by it , is updated at each iteration using a sketch of the Hessian , i.e. , a randomly generated compressed form of the Hessian .", "ner": [["method", 2, 2, "Generic"], ["inverse Hessian matrix", 8, 10, "OtherScientificTerm"], ["it", 15, 15, "Generic"], ["Hessian", 27, 27, "OtherScientificTerm"], ["randomly generated compressed form of the Hessian", 32, 38, "OtherScientificTerm"]], "relations": [[15, 15, 8, 10, "USED-FOR"], [27, 27, 15, 15, "USED-FOR"], [32, 38, 27, 27, "HYPONYM-OF"]]},
{"sentence": "We propose several sketching strategies , present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients , and prove linear convergence of the resulting method .", "ner": [["sketching strategies", 3, 4, "Method"], ["quasi-Newton method", 9, 10, "Method"], ["stochastic block BFGS updates", 13, 16, "Method"], ["variance reduction approach SVRG", 20, 23, "Method"], ["batch stochastic gradients", 26, 28, "OtherScientificTerm"], ["linear convergence", 32, 33, "OtherScientificTerm"], ["method", 37, 37, "Generic"]], "relations": [[9, 10, 26, 28, "USED-FOR"], [13, 16, 9, 10, "USED-FOR"], [13, 16, 20, 23, "CONJUNCTION"], [20, 23, 9, 10, "USED-FOR"], [32, 33, 37, 37, "FEATURE-OF"]]},
{"sentence": "Numerical tests on large-scale logistic regression problems reveal that our method is more robust and substantially outperforms current state-of-the-art methods .", "ner": [["large-scale logistic regression problems", 3, 6, "Task"], ["method", 10, 10, "Generic"], ["state-of-the-art methods", 18, 19, "Generic"]], "relations": [[3, 6, 10, 10, "EVALUATE-FOR"], [3, 6, 18, 19, "EVALUATE-FOR"], [10, 10, 18, 19, "COMPARE"]]},
{"sentence": "The goal of this research is to develop a spoken language system that will demonstrate the usefulness of voice input for interactive problem solving .", "ner": [["spoken language system", 9, 11, "Method"], ["voice input", 18, 19, "Material"], ["interactive problem solving", 21, 23, "Task"]], "relations": [[9, 11, 21, 23, "USED-FOR"], [18, 19, 21, 23, "USED-FOR"]]},
{"sentence": "The system will accept continuous speech , and will handle multiple speakers without explicit speaker enrollment .", "ner": [["continuous speech", 4, 5, "Material"]], "relations": []},
{"sentence": "Combining speech recognition and natural language processing to achieve speech understanding , the system will be demonstrated in an application domain relevant to the DoD .", "ner": [["speech recognition", 1, 2, "Task"], ["natural language processing", 4, 6, "Task"], ["speech understanding", 9, 10, "Task"], ["system", 13, 13, "Generic"]], "relations": [[1, 2, 4, 6, "CONJUNCTION"], [1, 2, 9, 10, "USED-FOR"], [4, 6, 9, 10, "USED-FOR"]]},
{"sentence": "The objective of this project is to develop a robust and high-performance speech recognition system using a segment-based approach to phonetic recognition .", "ner": [["robust and high-performance speech recognition system", 9, 14, "Method"], ["segment-based approach", 17, 18, "Method"], ["phonetic recognition", 20, 21, "Task"]], "relations": [[17, 18, 9, 14, "USED-FOR"], [17, 18, 20, 21, "USED-FOR"], [20, 21, 9, 14, "USED-FOR"]]},
{"sentence": "The recognition system will eventually be integrated with natural language processing to achieve spoken language understanding .", "ner": [["recognition system", 1, 2, "Method"], ["natural language processing", 8, 10, "Task"], ["spoken language understanding", 13, 15, "Task"]], "relations": [[1, 2, 13, 15, "USED-FOR"], [8, 10, 1, 2, "CONJUNCTION"], [8, 10, 13, 15, "USED-FOR"]]},
{"sentence": "Spelling-checkers have become an integral part of most text processing software .", "ner": [["Spelling-checkers", 0, 0, "Task"], ["text processing software", 8, 10, "Task"]], "relations": [[0, 0, 8, 10, "PART-OF"]]},
{"sentence": "From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words .", "ner": [["they", 10, 10, "Generic"], ["dictionaries of word forms", 15, 18, "OtherScientificTerm"]], "relations": [[15, 18, 10, 10, "USED-FOR"]]},
{"sentence": "This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .", "ner": [["approach", 1, 1, "Generic"], ["languages", 5, 5, "Material"], ["inflection", 8, 8, "OtherScientificTerm"], ["English", 11, 11, "Material"], ["highly inflective languages", 16, 18, "Material"], ["Czech", 21, 21, "Material"], ["Russian", 23, 23, "Material"], ["Slovak", 25, 25, "Material"], ["Slavonic languages", 28, 29, "Material"]], "relations": [[5, 5, 1, 1, "USED-FOR"], [8, 8, 5, 5, "FEATURE-OF"], [11, 11, 5, 5, "HYPONYM-OF"], [21, 21, 16, 18, "HYPONYM-OF"], [21, 21, 23, 23, "CONJUNCTION"], [23, 23, 16, 18, "HYPONYM-OF"], [23, 23, 25, 25, "CONJUNCTION"], [25, 25, 16, 18, "HYPONYM-OF"], [25, 25, 28, 29, "CONJUNCTION"], [28, 29, 16, 18, "HYPONYM-OF"]]},
{"sentence": "We have developed a special method for describing inflection for the purpose of building spelling-checkers for such languages .", "ner": [["method", 5, 5, "Generic"], ["inflection", 8, 8, "OtherScientificTerm"], ["spelling-checkers", 14, 14, "Task"], ["languages", 17, 17, "Generic"]], "relations": [[5, 5, 8, 8, "USED-FOR"], [5, 5, 14, 14, "USED-FOR"], [14, 14, 17, 17, "USED-FOR"]]},
{"sentence": "The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million -LRB- for Czech -RRB- .", "ner": [["program", 5, 5, "Generic"], ["spelling-checkers", 16, 16, "Method"], ["English", 18, 18, "Material"], ["Czech", 42, 42, "Material"]], "relations": [[18, 18, 16, 16, "USED-FOR"]]},
{"sentence": "Further , a special method has been developed for easy word classification .", "ner": [["method", 4, 4, "Generic"], ["word classification", 10, 11, "Task"]], "relations": [[4, 4, 10, 11, "USED-FOR"]]},
{"sentence": "We present a new HMM tagger that exploits context on both sides of a word to be tagged , and evaluate it in both the unsupervised and supervised case .", "ner": [["HMM tagger", 4, 5, "Method"], ["it", 21, 21, "Generic"], ["unsupervised and supervised case", 25, 28, "Task"]], "relations": [[25, 28, 21, 21, "EVALUATE-FOR"]]},
{"sentence": "Along the way , we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging , noting that published results to date have not been comparable across corpora or lexicons .", "ner": [["unsupervised methods", 11, 12, "Method"], ["part-of-speech tagging", 14, 15, "Task"]], "relations": [[11, 12, 14, 15, "USED-FOR"]]},
{"sentence": "Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms , we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable .", "ner": [["accuracy", 10, 10, "Metric"], ["algorithms", 17, 17, "Generic"], ["HMM training", 24, 25, "Method"], ["lexical probabilities", 32, 33, "OtherScientificTerm"]], "relations": [[10, 10, 17, 17, "EVALUATE-FOR"]]},
{"sentence": "Finally , we show how this new tagger achieves state-of-the-art results in a supervised , non-training intensive framework .", "ner": [["tagger", 7, 7, "Generic"], ["supervised , non-training intensive framework", 13, 17, "Task"]], "relations": [[13, 17, 7, 7, "EVALUATE-FOR"]]},
{"sentence": "We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- and Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- .", "ner": [["non-uniform sampling strategies", 5, 7, "Method"], ["stochastic optimization algorithms", 15, 17, "Method"], ["linear convergence", 19, 20, "OtherScientificTerm"], ["Stochastic Variance Reduced Gradient -LRB- SVRG -RRB-", 22, 28, "Method"], ["Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB-", 30, 36, "Method"]], "relations": [[5, 7, 15, 17, "USED-FOR"], [19, 20, 15, 17, "FEATURE-OF"], [22, 28, 15, 17, "HYPONYM-OF"], [22, 28, 30, 36, "CONJUNCTION"], [30, 36, 15, 17, "HYPONYM-OF"]]},
{"sentence": "For a large family of penalized empirical risk minimization problems , our methods exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .", "ner": [["penalized empirical risk minimization problems", 5, 9, "Task"], ["methods", 12, 12, "Generic"], ["data dependent local smoothness", 14, 17, "OtherScientificTerm"], ["loss functions", 20, 21, "OtherScientificTerm"], ["optimum", 24, 24, "OtherScientificTerm"], ["convergence guarantees", 28, 29, "OtherScientificTerm"]], "relations": [[12, 12, 5, 9, "USED-FOR"], [14, 17, 12, 12, "USED-FOR"], [14, 17, 20, 21, "FEATURE-OF"]]},
{"sentence": "Our bounds are the first to quantify the advantage gained from local smoothness which are significant for some problems significantly better .", "ner": [["local smoothness", 11, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "Empirically , we provide thorough numerical results to back up our theory .", "ner": [["theory", 11, 11, "Generic"]], "relations": []},
{"sentence": "Additionally we present algorithms exploiting local smoothness in more aggressive ways , which perform even better in practice .", "ner": [["algorithms", 3, 3, "Generic"], ["local smoothness", 5, 6, "OtherScientificTerm"]], "relations": [[5, 6, 3, 3, "USED-FOR"]]},
{"sentence": "Statistical language modeling remains a challenging task , in particular for morphologically rich languages .", "ner": [["Statistical language modeling", 0, 2, "Method"], ["task", 6, 6, "Generic"], ["morphologically rich languages", 11, 13, "Material"]], "relations": [[6, 6, 11, 13, "USED-FOR"]]},
{"sentence": "Recently , new approaches based on factored language models have been developed to address this problem .", "ner": [["approaches", 3, 3, "Generic"], ["factored language models", 6, 8, "Method"]], "relations": [[6, 8, 3, 3, "USED-FOR"]]},
{"sentence": "These models provide principled ways of including additional conditioning variables other than the preceding words , such as morphological or syntactic features .", "ner": [["models", 1, 1, "Generic"], ["conditioning variables", 8, 9, "OtherScientificTerm"], ["morphological or syntactic features", 18, 21, "OtherScientificTerm"]], "relations": [[18, 21, 8, 9, "HYPONYM-OF"]]},
{"sentence": "However , the number of possible choices for model parameters creates a large space of models that can not be searched exhaustively .", "ner": [["model parameters", 8, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks -LRB- Arabic and Turkish -RRB- .", "ner": [["entirely data-driven model selection procedure", 4, 8, "Method"], ["genetic search", 11, 12, "Method"], ["knowledge-based and random selection procedures", 20, 24, "Method"], ["language modeling tasks", 28, 30, "Task"], ["Arabic", 32, 32, "Material"], ["Turkish", 34, 34, "Material"]], "relations": [[4, 8, 20, 24, "COMPARE"], [11, 12, 4, 8, "USED-FOR"], [20, 24, 28, 30, "USED-FOR"], [32, 32, 28, 30, "HYPONYM-OF"], [32, 32, 34, 34, "CONJUNCTION"], [34, 34, 28, 30, "HYPONYM-OF"]]},
{"sentence": "We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems .", "ner": [["user modeling", 3, 4, "Method"], ["cooperative responses", 9, 10, "OtherScientificTerm"], ["spoken dialogue systems", 15, 17, "Task"]], "relations": [[3, 4, 9, 10, "USED-FOR"], [3, 4, 15, 17, "PART-OF"]]},
{"sentence": "Unlike previous studies that focus on user 's knowledge or typical kinds of users , the user model we propose is more comprehensive .", "ner": [["studies", 2, 2, "Generic"], ["user model", 16, 17, "Method"]], "relations": [[2, 2, 16, 17, "COMPARE"]]},
{"sentence": "Specifically , we set up three dimensions of user models : skill level to the system , knowledge level on the target domain and the degree of hastiness .", "ner": [["user models", 8, 9, "Method"]], "relations": []},
{"sentence": "Moreover , the models are automatically derived by decision tree learning using real dialogue data collected by the system .", "ner": [["models", 3, 3, "Generic"], ["decision tree learning", 8, 10, "Method"], ["real dialogue data", 12, 14, "Material"], ["system", 18, 18, "Generic"]], "relations": [[8, 10, 3, 3, "USED-FOR"], [12, 14, 8, 10, "USED-FOR"], [18, 18, 12, 14, "USED-FOR"]]},
{"sentence": "We obtained reasonable classification accuracy for all dimensions .", "ner": [["classification accuracy", 3, 4, "Metric"]], "relations": []},
{"sentence": "Dialogue strategies based on the user modeling are implemented in Kyoto city bus information system that has been developed at our laboratory .", "ner": [["Dialogue strategies", 0, 1, "Method"], ["user modeling", 5, 6, "Method"], ["Kyoto city bus information system", 10, 14, "Method"]], "relations": [[0, 1, 10, 14, "USED-FOR"], [5, 6, 0, 1, "USED-FOR"]]},
{"sentence": "Experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users .", "ner": [["cooperative responses", 5, 6, "OtherScientificTerm"], ["dialogue duration", 21, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper proposes a novel method of building polarity-tagged corpus from HTML documents .", "ner": [["method", 5, 5, "Generic"], ["building polarity-tagged corpus", 7, 9, "Task"], ["HTML documents", 11, 12, "Material"]], "relations": [[5, 5, 7, 9, "USED-FOR"], [11, 12, 5, 5, "USED-FOR"]]},
{"sentence": "The characteristics of this method is that it is fully automatic and can be applied to arbitrary HTML documents .", "ner": [["method", 4, 4, "Generic"], ["it", 7, 7, "Generic"], ["HTML documents", 17, 18, "Material"]], "relations": [[7, 7, 17, 18, "USED-FOR"]]},
{"sentence": "The idea behind our method is to utilize certain layout structures and linguistic pattern .", "ner": [["method", 4, 4, "Generic"], ["layout structures", 9, 10, "OtherScientificTerm"], ["linguistic pattern", 12, 13, "OtherScientificTerm"]], "relations": [[9, 10, 4, 4, "USED-FOR"], [9, 10, 12, 13, "CONJUNCTION"], [12, 13, 4, 4, "USED-FOR"]]},
{"sentence": "By using them , we can automatically extract such sentences that express opinion .", "ner": [["them", 2, 2, "Generic"]], "relations": []},
{"sentence": "In our experiment , the method could construct a corpus consisting of 126,610 sentences .", "ner": [["method", 5, 5, "Generic"]], "relations": []},
{"sentence": "Previous work has used monolingual parallel corpora to extract and generate paraphrases .", "ner": [["monolingual parallel corpora", 4, 6, "Material"], ["paraphrases", 11, 11, "OtherScientificTerm"]], "relations": [[4, 6, 11, 11, "USED-FOR"]]},
{"sentence": "We show that this task can be done using bilingual parallel corpora , a much more commonly available resource .", "ner": [["task", 4, 4, "Generic"], ["bilingual parallel corpora", 9, 11, "Material"]], "relations": [[9, 11, 4, 4, "USED-FOR"]]},
{"sentence": "Using alignment techniques from phrase-based statistical machine translation , we show how paraphrases in one language can be identified using a phrase in another language as a pivot .", "ner": [["alignment techniques", 1, 2, "Method"], ["phrase-based statistical machine translation", 4, 7, "Task"], ["paraphrases", 12, 12, "OtherScientificTerm"]], "relations": [[1, 2, 4, 7, "USED-FOR"]]},
{"sentence": "We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .", "ner": [["paraphrase probability", 3, 4, "OtherScientificTerm"], ["paraphrases", 7, 7, "OtherScientificTerm"], ["bilingual parallel corpus", 11, 13, "Material"], ["translation probabilities", 18, 19, "OtherScientificTerm"], ["it", 24, 24, "Generic"], ["contextual information", 30, 31, "OtherScientificTerm"]], "relations": [[7, 7, 11, 13, "PART-OF"], [18, 19, 7, 7, "USED-FOR"], [30, 31, 24, 24, "USED-FOR"]]},
{"sentence": "We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments .", "ner": [["paraphrase extraction and ranking methods", 3, 7, "Method"], ["manual word alignments", 12, 14, "Material"], ["quality", 19, 19, "Metric"], ["paraphrases", 21, 21, "OtherScientificTerm"], ["automatic alignments", 24, 25, "OtherScientificTerm"]], "relations": [[12, 14, 3, 7, "EVALUATE-FOR"], [19, 19, 21, 21, "EVALUATE-FOR"], [21, 21, 24, 25, "PART-OF"]]},
{"sentence": "This paper proposes an automatic , essentially domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB- which combines software we have developed for that purpose -LRB- the '' Comparator '' -RRB- and a set of specifications for answer expressions -LRB- the '' Common Answer Specification '' , or CAS -RRB- .", "ner": [["domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB-", 7, 16, "Method"], ["software", 19, 19, "Generic"], ["Comparator", 29, 29, "Method"], ["specifications", 36, 36, "Generic"], ["answer expressions", 38, 39, "OtherScientificTerm"], ["Common Answer Specification", 43, 45, "Method"], ["CAS", 49, 49, "Method"]], "relations": [[19, 19, 7, 16, "PART-OF"], [19, 19, 36, 36, "CONJUNCTION"], [36, 36, 7, 16, "PART-OF"], [36, 36, 38, 39, "USED-FOR"]]},
{"sentence": "The Comparator checks whether the answer provided by a SLS accords with a canonical answer , returning either true or false .", "ner": [["Comparator", 1, 1, "Method"], ["SLS", 9, 9, "Method"]], "relations": []},
{"sentence": "The Common Answer Specification determines the syntax of answer expressions , the minimal content that must be included in them , the data to be included in and excluded from test corpora , and the procedures used by the Comparator .", "ner": [["Common Answer Specification", 1, 3, "Method"], ["syntax of answer expressions", 6, 9, "OtherScientificTerm"], ["Comparator", 39, 39, "Method"]], "relations": [[1, 3, 6, 9, "USED-FOR"]]},
{"sentence": "Though some details of the CAS are particular to individual domains , the Comparator software is domain-independent , as is the CAS approach .", "ner": [["CAS", 5, 5, "OtherScientificTerm"], ["Comparator software", 13, 14, "Method"], ["CAS approach", 21, 22, "Method"]], "relations": []},
{"sentence": "This paper describes an unsupervised learning method for associative relationships between verb phrases , which is important in developing reliable Q&A systems .", "ner": [["unsupervised learning method", 4, 6, "Method"], ["associative relationships between verb phrases", 8, 12, "OtherScientificTerm"], ["Q&A systems", 20, 21, "Task"]], "relations": [[4, 6, 8, 12, "USED-FOR"], [8, 12, 20, 21, "USED-FOR"]]},
{"sentence": "Consider the situation that a user gives a query `` How much petrol was imported to Japan from Saudi Arabia ? ''", "ner": [], "relations": []},
{"sentence": "to a Q&A system , but the text given to the system includes only the description `` X tonnes of petrol was conveyed to Japan from Saudi Arabia '' .", "ner": [["Q&A system", 2, 3, "Task"]], "relations": []},
{"sentence": "We think that the description is a good clue to find the answer for our query , `` X tonnes '' .", "ner": [], "relations": []},
{"sentence": "But there is no large-scale database that provides the associative relationship between `` imported '' and `` conveyed '' .", "ner": [["large-scale database", 4, 5, "Material"], ["associative relationship", 9, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "Our aim is to develop an unsupervised learning method that can obtain such an associative relationship , which we call scenario consistency .", "ner": [["unsupervised learning method", 6, 8, "Method"], ["associative relationship", 14, 15, "OtherScientificTerm"], ["scenario consistency", 20, 21, "OtherScientificTerm"]], "relations": [[6, 8, 14, 15, "USED-FOR"]]},
{"sentence": "The method we are currently working on uses an expectation-maximization -LRB- EM -RRB- based word-clustering algorithm , and we have evaluated the effectiveness of this method using Japanese verb phrases .", "ner": [["method", 1, 1, "Generic"], ["expectation-maximization -LRB- EM -RRB- based word-clustering algorithm", 9, 15, "Method"], ["method", 25, 25, "Generic"], ["Japanese verb phrases", 27, 29, "Material"]], "relations": [[9, 15, 1, 1, "USED-FOR"], [27, 29, 25, 25, "USED-FOR"]]},
{"sentence": "We describe the use of text data scraped from the web to augment language models for Automatic Speech Recognition and Keyword Search for Low Resource Languages .", "ner": [["text data", 5, 6, "Material"], ["web", 10, 10, "Material"], ["language models", 13, 14, "Method"], ["Automatic Speech Recognition", 16, 18, "Task"], ["Keyword Search", 20, 21, "Task"], ["Low Resource Languages", 23, 25, "Material"]], "relations": [[5, 6, 13, 14, "USED-FOR"], [10, 10, 5, 6, "FEATURE-OF"], [13, 14, 16, 18, "USED-FOR"], [13, 14, 20, 21, "USED-FOR"], [16, 18, 20, 21, "CONJUNCTION"], [23, 25, 16, 18, "USED-FOR"], [23, 25, 20, 21, "USED-FOR"]]},
{"sentence": "We scrape text from multiple genres including blogs , online news , translated TED talks , and subtitles .", "ner": [["genres", 5, 5, "Generic"], ["blogs", 7, 7, "Material"], ["online news", 9, 10, "Material"], ["translated TED talks", 12, 14, "Material"], ["subtitles", 17, 17, "Material"]], "relations": [[7, 7, 5, 5, "HYPONYM-OF"], [7, 7, 9, 10, "CONJUNCTION"], [9, 10, 5, 5, "HYPONYM-OF"], [9, 10, 12, 14, "CONJUNCTION"], [12, 14, 5, 5, "HYPONYM-OF"], [12, 14, 17, 17, "CONJUNCTION"], [17, 17, 5, 5, "HYPONYM-OF"]]},
{"sentence": "Using linearly interpolated language models , we find that blogs and movie subtitles are more relevant for language modeling of conversational telephone speech and obtain large reductions in out-of-vocabulary keywords .", "ner": [["linearly interpolated language models", 1, 4, "Method"], ["blogs", 9, 9, "Material"], ["movie subtitles", 11, 12, "Material"], ["language modeling of conversational telephone speech", 17, 22, "Method"], ["out-of-vocabulary keywords", 28, 29, "OtherScientificTerm"]], "relations": [[1, 4, 17, 22, "USED-FOR"], [9, 9, 11, 12, "CONJUNCTION"], [9, 9, 17, 22, "USED-FOR"], [11, 12, 17, 22, "USED-FOR"]]},
{"sentence": "Furthermore , we show that the web data can improve Term Error Rate Performance by 3.8 % absolute and Maximum Term-Weighted Value in Keyword Search by 0.0076-0 .1059 absolute points .", "ner": [["web data", 6, 7, "Material"], ["Term Error Rate Performance", 10, 13, "Metric"], ["Maximum Term-Weighted Value", 19, 21, "Metric"], ["Keyword Search", 23, 24, "Task"]], "relations": [[6, 7, 23, 24, "USED-FOR"], [10, 13, 23, 24, "EVALUATE-FOR"], [19, 21, 23, 24, "EVALUATE-FOR"]]},
{"sentence": "Much of the gain comes from the reduction of out-of-vocabulary items .", "ner": [["reduction of out-of-vocabulary items", 7, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions , lexical choice , and revision .", "ner": [["Pipelined Natural Language Generation -LRB- NLG -RRB- systems", 0, 7, "Method"], ["architectural modules", 13, 14, "Method"], ["language functionalities", 19, 20, "OtherScientificTerm"], ["referring expressions", 23, 24, "OtherScientificTerm"], ["lexical choice", 26, 27, "OtherScientificTerm"], ["revision", 30, 30, "OtherScientificTerm"]], "relations": [[13, 14, 19, 20, "USED-FOR"], [23, 24, 19, 20, "HYPONYM-OF"], [23, 24, 26, 27, "CONJUNCTION"], [26, 27, 19, 20, "HYPONYM-OF"], [26, 27, 30, 30, "CONJUNCTION"], [30, 30, 19, 20, "HYPONYM-OF"]]},
{"sentence": "This has given rise to discussions about the relative placement of these new modules in the overall architecture .", "ner": [["modules", 13, 13, "Generic"], ["overall architecture", 16, 17, "Generic"]], "relations": [[13, 13, 16, 17, "PART-OF"]]},
{"sentence": "Recent work on another aspect of multi-paragraph text , discourse markers , indicates it is time to consider where a discourse marker insertion algorithm fits in .", "ner": [["multi-paragraph text", 6, 7, "Material"], ["discourse markers", 9, 10, "OtherScientificTerm"], ["discourse marker insertion algorithm", 20, 23, "Method"]], "relations": []},
{"sentence": "We present examples which suggest that in a pipelined NLG architecture , the best approach is to strongly tie it to a revision component .", "ner": [["pipelined NLG architecture", 8, 10, "Method"], ["approach", 14, 14, "Generic"], ["it", 19, 19, "Generic"], ["revision component", 22, 23, "Method"]], "relations": [[19, 19, 22, 23, "CONJUNCTION"], [22, 23, 8, 10, "PART-OF"]]},
{"sentence": "Finally , we evaluate the approach in a working multi-page system .", "ner": [["approach", 5, 5, "Generic"], ["multi-page system", 9, 10, "Task"]], "relations": [[9, 10, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "In this paper a system which understands and conceptualizes scenes descriptions in natural language is presented .", "ner": [["system", 4, 4, "Generic"], ["scenes descriptions in natural language", 9, 13, "Task"]], "relations": [[4, 4, 9, 13, "USED-FOR"]]},
{"sentence": "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .", "ner": [["components", 4, 4, "Generic"], ["system", 7, 7, "Generic"], ["syntactic analyzer", 12, 13, "Method"], ["Procedural Systemic Grammar", 18, 20, "Method"], ["semantic analyzer", 23, 24, "Method"], ["Conceptual Dependency Theory", 28, 30, "Method"], ["dictionary", 34, 34, "OtherScientificTerm"]], "relations": [[4, 4, 7, 7, "PART-OF"], [12, 13, 4, 4, "PART-OF"], [12, 13, 23, 24, "CONJUNCTION"], [18, 20, 12, 13, "USED-FOR"], [23, 24, 4, 4, "PART-OF"], [23, 24, 34, 34, "CONJUNCTION"], [28, 30, 23, 24, "USED-FOR"], [34, 34, 4, 4, "PART-OF"]]},
{"sentence": "This article considers approaches which rerank the output of an existing probabilistic parser .", "ner": [["approaches", 3, 3, "Generic"], ["probabilistic parser", 11, 12, "Method"]], "relations": []},
{"sentence": "The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .", "ner": [["parser", 2, 2, "Method"], ["candidate parses", 7, 8, "OtherScientificTerm"], ["ranking", 21, 21, "OtherScientificTerm"], ["parses", 24, 24, "Generic"]], "relations": [[21, 21, 24, 24, "FEATURE-OF"]]},
{"sentence": "A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .", "ner": [["model", 2, 2, "Generic"], ["ranking", 10, 10, "OtherScientificTerm"], ["features", 14, 14, "OtherScientificTerm"], ["tree", 17, 17, "OtherScientificTerm"]], "relations": [[2, 2, 10, 10, "USED-FOR"], [14, 14, 2, 2, "USED-FOR"]]},
{"sentence": "The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .", "ner": [["approach", 4, 4, "Generic"], ["it", 7, 7, "Generic"], ["tree", 10, 10, "OtherScientificTerm"], ["features", 19, 19, "OtherScientificTerm"], ["features", 26, 26, "OtherScientificTerm"], ["derivation", 37, 37, "OtherScientificTerm"], ["generative model", 40, 41, "Method"], ["features", 45, 45, "OtherScientificTerm"]], "relations": [[45, 45, 40, 41, "USED-FOR"]]},
{"sentence": "We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al. -LRB- 1998 -RRB- .", "ner": [["method", 4, 4, "Generic"], ["reranking task", 7, 8, "Task"], ["boosting approach", 13, 14, "Method"], ["ranking problems", 16, 17, "Task"]], "relations": [[4, 4, 7, 8, "USED-FOR"], [13, 14, 4, 4, "USED-FOR"], [13, 14, 16, 17, "USED-FOR"]]},
{"sentence": "We apply the boosting method to parsing the Wall Street Journal treebank .", "ner": [["boosting method", 3, 4, "Method"], ["parsing", 6, 6, "Task"], ["Wall Street Journal treebank", 8, 11, "Material"]], "relations": [[3, 4, 6, 6, "USED-FOR"], [8, 11, 3, 4, "USED-FOR"]]},
{"sentence": "The method combined the log-likelihood under a baseline model -LRB- that of Collins -LSB- 1999 -RSB- -RRB- with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "ner": [["method", 1, 1, "Generic"], ["log-likelihood", 4, 4, "OtherScientificTerm"], ["baseline model", 7, 8, "Method"], ["features", 23, 23, "OtherScientificTerm"], ["parse trees", 25, 26, "OtherScientificTerm"], ["model", 34, 34, "Generic"]], "relations": [[4, 4, 1, 1, "PART-OF"], [4, 4, 7, 8, "CONJUNCTION"]]},
{"sentence": "The new model achieved 89.75 % F-measure , a 13 % relative decrease in F-measure error over the baseline model 's score of 88.2 % .", "ner": [["model", 2, 2, "Generic"], ["F-measure", 6, 6, "Metric"], ["F-measure", 14, 14, "Metric"], ["baseline model", 18, 19, "Generic"]], "relations": [[6, 6, 2, 2, "EVALUATE-FOR"], [14, 14, 18, 19, "EVALUATE-FOR"], [18, 19, 2, 2, "COMPARE"]]},
{"sentence": "The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .", "ner": [["algorithm", 6, 6, "Generic"], ["boosting approach", 9, 10, "Method"], ["sparsity of the feature space", 16, 20, "OtherScientificTerm"], ["parsing data", 23, 24, "Material"]], "relations": [[6, 6, 9, 10, "USED-FOR"], [16, 20, 6, 6, "USED-FOR"], [16, 20, 23, 24, "FEATURE-OF"]]},
{"sentence": "Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .", "ner": [["algorithm", 8, 8, "Generic"], ["boosting approach", 15, 16, "Method"]], "relations": [[8, 8, 15, 16, "COMPARE"]]},
{"sentence": "We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on feature selection methods within log-linear -LRB- maximum-entropy -RRB- models .", "ner": [["method", 4, 4, "Generic"], ["feature selection methods", 21, 23, "Method"], ["log-linear -LRB- maximum-entropy -RRB- models", 25, 29, "Method"]], "relations": [[21, 23, 25, 29, "PART-OF"]]},
{"sentence": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation .", "ner": [["natural language parsing -LRB- NLP -RRB-", 8, 13, "Task"], ["NLP problems", 23, 24, "Task"], ["ranking tasks", 30, 31, "Task"], ["speech recognition", 36, 37, "Task"], ["machine translation", 39, 40, "Task"], ["natural language generation", 43, 45, "Task"]], "relations": [[30, 31, 23, 24, "USED-FOR"], [36, 37, 23, 24, "HYPONYM-OF"], [36, 37, 30, 31, "HYPONYM-OF"], [36, 37, 39, 40, "CONJUNCTION"], [39, 40, 23, 24, "HYPONYM-OF"], [39, 40, 30, 31, "HYPONYM-OF"], [39, 40, 43, 45, "CONJUNCTION"], [43, 45, 23, 24, "HYPONYM-OF"], [43, 45, 30, 31, "HYPONYM-OF"]]},
{"sentence": "A model is presented to characterize the class of languages obtained by adding reduplication to context-free languages .", "ner": [["model", 1, 1, "Generic"], ["class of languages", 7, 9, "OtherScientificTerm"], ["reduplication", 13, 13, "OtherScientificTerm"], ["context-free languages", 15, 16, "Material"]], "relations": [[1, 1, 7, 9, "USED-FOR"], [13, 13, 15, 16, "USED-FOR"]]},
{"sentence": "The model is a pushdown automaton augmented with the ability to check reduplication by using the stack in a new way .", "ner": [["model", 1, 1, "Method"], ["pushdown automaton", 4, 5, "Method"], ["reduplication", 12, 12, "OtherScientificTerm"], ["stack", 16, 16, "OtherScientificTerm"]], "relations": [[1, 1, 4, 5, "HYPONYM-OF"], [16, 16, 4, 5, "USED-FOR"], [16, 16, 12, 12, "USED-FOR"]]},
{"sentence": "The class of languages generated is shown to lie strictly between the context-free languages and the indexed languages .", "ner": [["class of languages", 1, 3, "OtherScientificTerm"], ["context-free languages", 12, 13, "Material"], ["indexed languages", 16, 17, "Material"]], "relations": [[12, 13, 16, 17, "CONJUNCTION"]]},
{"sentence": "The model appears capable of accommodating the sort of reduplications that have been observed to occur in natural languages , but it excludes many of the unnatural constructions that other formal models have permitted .", "ner": [["model", 1, 1, "Generic"], ["reduplications", 9, 9, "OtherScientificTerm"], ["natural languages", 17, 18, "Material"], ["it", 21, 21, "Generic"], ["formal models", 30, 31, "Method"]], "relations": [[1, 1, 9, 9, "USED-FOR"]]},
{"sentence": "We present an image set classification algorithm based on unsupervised clustering of labeled training and unla-beled test data where labels are only used in the stopping criterion .", "ner": [["image set classification algorithm", 3, 6, "Method"], ["unsupervised clustering", 9, 10, "Method"], ["labeled training and unla-beled test data", 12, 17, "Material"], ["stopping criterion", 25, 26, "OtherScientificTerm"]], "relations": [[9, 10, 3, 6, "USED-FOR"], [12, 17, 9, 10, "USED-FOR"]]},
{"sentence": "The probability distribution of each class over the set of clusters is used to define a true set based similarity measure .", "ner": [["probability distribution", 1, 2, "OtherScientificTerm"], ["clusters", 10, 10, "OtherScientificTerm"], ["set based similarity measure", 17, 20, "Metric"]], "relations": [[1, 2, 17, 20, "USED-FOR"]]},
{"sentence": "To this end , we propose an iterative sparse spectral clustering algorithm .", "ner": [["iterative sparse spectral clustering algorithm", 7, 11, "Method"]], "relations": []},
{"sentence": "In each iteration , a proximity matrix is efficiently recomputed to better represent the local subspace structure .", "ner": [["proximity matrix", 5, 6, "OtherScientificTerm"], ["local subspace structure", 14, 16, "OtherScientificTerm"]], "relations": [[5, 6, 14, 16, "USED-FOR"]]},
{"sentence": "Initial clusters capture the global data structure and finer clusters at the later stages capture the subtle class differences not visible at the global scale .", "ner": [["Initial clusters", 0, 1, "OtherScientificTerm"], ["global data structure", 4, 6, "OtherScientificTerm"], ["finer clusters", 8, 9, "OtherScientificTerm"], ["subtle class differences", 16, 18, "OtherScientificTerm"], ["global scale", 23, 24, "OtherScientificTerm"]], "relations": [[0, 1, 4, 6, "USED-FOR"], [8, 9, 16, 18, "USED-FOR"]]},
{"sentence": "Image sets are compactly represented with multiple Grass-mannian manifolds which are subsequently embedded in Euclidean space with the proposed spectral clustering algorithm .", "ner": [["Image sets", 0, 1, "Material"], ["Grass-mannian manifolds", 7, 8, "Method"], ["Euclidean space", 14, 15, "OtherScientificTerm"], ["spectral clustering algorithm", 19, 21, "Method"]], "relations": [[7, 8, 0, 1, "USED-FOR"], [19, 21, 14, 15, "USED-FOR"]]},
{"sentence": "We also propose an efficient eigenvector solver which not only reduces the computational cost of spectral clustering by many folds but also improves the clustering quality and final classification results .", "ner": [["eigenvector solver", 5, 6, "Method"], ["computational cost", 12, 13, "Metric"], ["spectral clustering", 15, 16, "Method"], ["clustering quality", 24, 25, "Metric"], ["classification results", 28, 29, "Metric"]], "relations": [[5, 6, 15, 16, "USED-FOR"], [12, 13, 15, 16, "EVALUATE-FOR"], [24, 25, 15, 16, "EVALUATE-FOR"], [28, 29, 15, 16, "EVALUATE-FOR"]]},
{"sentence": "Experiments on five standard datasets and comparison with seven existing techniques show the efficacy of our algorithm .", "ner": [["datasets", 4, 4, "Material"], ["algorithm", 16, 16, "Generic"]], "relations": []},
{"sentence": "This paper investigates some computational problems associated with probabilistic translation models that have recently been adopted in the literature on machine translation .", "ner": [["computational problems", 4, 5, "OtherScientificTerm"], ["probabilistic translation models", 8, 10, "Method"], ["machine translation", 20, 21, "Task"]], "relations": [[4, 5, 8, 10, "FEATURE-OF"], [8, 10, 20, 21, "USED-FOR"]]},
{"sentence": "These models can be viewed as pairs of probabilistic context-free grammars working in a ` synchronous ' way .", "ner": [["models", 1, 1, "Generic"], ["probabilistic context-free grammars", 8, 10, "Method"]], "relations": [[8, 10, 1, 1, "FEATURE-OF"]]},
{"sentence": "Two hardness results for the class NP are reported , along with an exponential time lower-bound for certain classes of algorithms that are currently used in the literature .", "ner": [["exponential time lower-bound", 13, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "Active shape models are a powerful and widely used tool to interpret complex image data .", "ner": [["Active shape models", 0, 2, "Method"], ["complex image data", 12, 14, "Material"]], "relations": [[0, 2, 12, 14, "USED-FOR"]]},
{"sentence": "By building models of shape variation they enable search algorithms to use a pri-ori knowledge in an efficient and gainful way .", "ner": [["models of shape variation", 2, 5, "Method"], ["search algorithms", 8, 9, "Method"], ["pri-ori knowledge", 13, 14, "OtherScientificTerm"]], "relations": [[8, 9, 2, 5, "USED-FOR"], [13, 14, 8, 9, "USED-FOR"]]},
{"sentence": "However , due to the linearity of PCA , non-linearities like rotations or independently moving sub-parts in the data can deteriorate the resulting model considerably .", "ner": [["linearity", 5, 5, "OtherScientificTerm"], ["PCA", 7, 7, "Method"], ["non-linearities", 9, 9, "OtherScientificTerm"], ["rotations", 11, 11, "OtherScientificTerm"]], "relations": [[5, 5, 7, 7, "FEATURE-OF"], [11, 11, 9, 9, "HYPONYM-OF"]]},
{"sentence": "Although non-linear extensions of active shape models have been proposed and application specific solutions have been used , they still need a certain amount of user interaction during model building .", "ner": [["non-linear extensions of active shape models", 1, 6, "Method"], ["active shape models", 4, 6, "Method"], ["user interaction", 25, 26, "OtherScientificTerm"]], "relations": [[25, 26, 1, 6, "USED-FOR"]]},
{"sentence": "In this paper the task of build-ing/choosing optimal models is tackled in a more generic information theoretic fashion .", "ner": [["build-ing/choosing optimal models", 6, 8, "Task"]], "relations": []},
{"sentence": "In particular , we propose an algorithm based on the minimum description length principle to find an optimal subdivision of the data into sub-parts , each adequate for linear modeling .", "ner": [["algorithm", 6, 6, "Generic"], ["minimum description length principle", 10, 13, "Method"], ["linear modeling", 28, 29, "Method"]], "relations": [[10, 13, 6, 6, "USED-FOR"]]},
{"sentence": "This results in an overall more compact model configuration .", "ner": [], "relations": []},
{"sentence": "Which in turn leads to a better model in terms of modes of variations .", "ner": [["model", 7, 7, "Generic"], ["modes of variations", 11, 13, "OtherScientificTerm"]], "relations": [[11, 13, 7, 7, "FEATURE-OF"]]},
{"sentence": "The proposed method is evaluated on synthetic data , medical images and hand contours .", "ner": [["method", 2, 2, "Generic"], ["synthetic data", 6, 7, "Material"], ["medical images", 9, 10, "Material"], ["hand contours", 12, 13, "Material"]], "relations": [[6, 7, 2, 2, "EVALUATE-FOR"], [6, 7, 9, 10, "CONJUNCTION"], [9, 10, 2, 2, "EVALUATE-FOR"], [9, 10, 12, 13, "CONJUNCTION"], [12, 13, 2, 2, "EVALUATE-FOR"]]},
{"sentence": "We describe a set of experiments to explore statistical techniques for ranking and selecting the best translations in a graph of translation hypotheses .", "ner": [["statistical techniques", 8, 9, "Method"], ["ranking", 11, 11, "Task"], ["graph of translation hypotheses", 19, 22, "OtherScientificTerm"]], "relations": [[8, 9, 11, 11, "USED-FOR"]]},
{"sentence": "In a previous paper -LRB- Carl , 2007 -RRB- we have described how the hypotheses graph is generated through shallow mapping and permutation rules .", "ner": [["hypotheses graph", 14, 15, "OtherScientificTerm"], ["shallow mapping", 19, 20, "Method"], ["permutation rules", 22, 23, "Method"]], "relations": [[19, 20, 14, 15, "USED-FOR"], [19, 20, 22, 23, "CONJUNCTION"], [22, 23, 14, 15, "USED-FOR"]]},
{"sentence": "We have given examples of its nodes consisting of vectors representing morpho-syntactic properties of words and phrases .", "ner": [["nodes", 6, 6, "OtherScientificTerm"], ["vectors representing morpho-syntactic properties", 9, 12, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper describes a number of methods for elaborating statistical feature functions from some of the vector components .", "ner": [["methods", 6, 6, "Generic"], ["statistical feature functions", 9, 11, "OtherScientificTerm"], ["vector components", 16, 17, "Method"]], "relations": [[6, 6, 9, 11, "USED-FOR"], [16, 17, 6, 6, "USED-FOR"]]},
{"sentence": "The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M translation paths in the graph .", "ner": [["feature functions", 1, 2, "OtherScientificTerm"], ["log-linear combination", 13, 14, "OtherScientificTerm"], ["translation paths", 23, 24, "OtherScientificTerm"], ["graph", 27, 27, "OtherScientificTerm"]], "relations": [[13, 14, 23, 24, "USED-FOR"], [23, 24, 27, 27, "PART-OF"]]},
{"sentence": "We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 -RRB- word-lemma based feature function models produce better results than token-based models , 2 -RRB- adding a PoS-tag feature function to the word-lemma model improves the output and 3 -RRB- weights for lexical translations are suitable if the training material is similar to the texts to be translated .", "ner": [["language modelling toolkits", 3, 5, "Method"], ["CMU and the SRI toolkit", 8, 12, "Method"], ["word-lemma based feature function models", 21, 25, "Method"], ["token-based models", 30, 31, "Method"], ["PoS-tag feature function", 37, 39, "OtherScientificTerm"], ["word-lemma model", 42, 43, "Method"], ["lexical translations", 52, 53, "Task"]], "relations": [[8, 12, 3, 5, "HYPONYM-OF"], [21, 25, 30, 31, "COMPARE"], [37, 39, 42, 43, "PART-OF"]]},
{"sentence": "This paper presents a specialized editor for a highly structured dictionary .", "ner": [["editor", 5, 5, "Method"], ["structured dictionary", 9, 10, "Material"]], "relations": [[9, 10, 5, 5, "USED-FOR"]]},
{"sentence": "The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent dictionary on the basis of a linguistic theory .", "ner": [["editor", 6, 6, "Method"], ["dictionary", 21, 21, "Generic"], ["linguistic theory", 27, 28, "Method"]], "relations": [[6, 6, 21, 21, "USED-FOR"], [27, 28, 21, 21, "USED-FOR"]]},
{"sentence": "If we want valuable lexicons and grammars to achieve complex natural language processing , we must provide very powerful tools to help create and ensure the validity of such complex linguistic databases .", "ner": [["natural language processing", 10, 12, "Task"], ["linguistic databases", 30, 31, "Material"]], "relations": []},
{"sentence": "Our most important task in building the editor was to define a set of coherence rules that could be computationally applied to ensure the validity of lexical entries .", "ner": [["editor", 7, 7, "Method"], ["coherence rules", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "A customized interface for browsing and editing was also designed and implemented .", "ner": [], "relations": []},
{"sentence": "Existing techniques extract term candidates by looking for internal and contextual information associated with domain specific terms .", "ner": [["internal and contextual information", 8, 11, "OtherScientificTerm"], ["domain specific terms", 14, 16, "OtherScientificTerm"]], "relations": [[14, 16, 8, 11, "FEATURE-OF"]]},
{"sentence": "The algorithms always face the dilemma that fewer features are not enough to distinguish terms from non-terms whereas more features lead to more conflicts among selected features .", "ner": [["features", 8, 8, "OtherScientificTerm"], ["features", 19, 19, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper presents a novel approach for term extraction based on delimiters which are much more stable and domain independent .", "ner": [["approach", 5, 5, "Generic"], ["term extraction", 7, 8, "Task"], ["delimiters", 11, 11, "OtherScientificTerm"]], "relations": [[5, 5, 7, 8, "USED-FOR"], [11, 11, 5, 5, "USED-FOR"]]},
{"sentence": "The proposed approach is not as sensitive to term frequency as that of previous works .", "ner": [["approach", 2, 2, "Generic"], ["term frequency", 8, 9, "OtherScientificTerm"]], "relations": [[2, 2, 8, 9, "COMPARE"]]},
{"sentence": "This approach has no strict limit or hard rules and thus they can deal with all kinds of terms .", "ner": [["approach", 1, 1, "Generic"], ["hard rules", 7, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "It also requires no prior domain knowledge and no additional training to adapt to new domains .", "ner": [["domain knowledge", 5, 6, "OtherScientificTerm"]], "relations": []},
{"sentence": "Consequently , the proposed approach can be applied to different domains easily and it is especially useful for resource-limited domains .", "ner": [["approach", 4, 4, "Generic"], ["it", 13, 13, "Generic"], ["resource-limited domains", 18, 19, "Material"]], "relations": [[13, 13, 18, 19, "USED-FOR"]]},
{"sentence": "Evaluations conducted on two different domains for Chinese term extraction show significant improvements over existing techniques which verifies its efficiency and domain independent nature .", "ner": [["Evaluations", 0, 0, "Generic"], ["Chinese term extraction", 7, 9, "Task"]], "relations": [[0, 0, 7, 9, "EVALUATE-FOR"]]},
{"sentence": "Experiments on new term extraction indicate that the proposed approach can also serve as an effective tool for domain lexicon expansion .", "ner": [["new term extraction", 2, 4, "Task"], ["approach", 9, 9, "Generic"], ["tool", 16, 16, "Generic"], ["domain lexicon expansion", 18, 20, "Task"]], "relations": [[9, 9, 18, 20, "USED-FOR"]]},
{"sentence": "We describe a method for identifying systematic patterns in translation data using part-of-speech tag sequences .", "ner": [["method", 3, 3, "Generic"], ["systematic patterns in translation data", 6, 10, "OtherScientificTerm"], ["part-of-speech tag sequences", 12, 14, "Material"]], "relations": [[3, 3, 6, 10, "USED-FOR"], [12, 14, 3, 3, "USED-FOR"]]},
{"sentence": "We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .", "ner": [["analysis", 3, 3, "Generic"], ["diagnostic tool", 6, 7, "Method"], ["machine translation systems", 12, 14, "Method"], ["application", 20, 20, "Generic"], ["patterns in machine translation output", 28, 32, "OtherScientificTerm"]], "relations": [[3, 3, 6, 7, "PART-OF"], [6, 7, 12, 14, "USED-FOR"], [20, 20, 28, 32, "USED-FOR"]]},
{"sentence": "We study the number of hidden layers required by a multilayer neu-ral network with threshold units to compute a function f from n d to -LCB- O , I -RCB- .", "ner": [["number of hidden layers", 3, 6, "OtherScientificTerm"], ["multilayer neu-ral network", 10, 12, "Method"], ["threshold units", 14, 15, "OtherScientificTerm"]], "relations": [[3, 6, 10, 12, "USED-FOR"], [14, 15, 3, 6, "USED-FOR"]]},
{"sentence": "In dimension d = 2 , Gibson characterized the functions computable with just one hidden layer , under the assumption that there is no `` multiple intersection point '' and that f is only defined on a compact set .", "ner": [["hidden layer", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "We consider the restriction of f to the neighborhood of a multiple intersection point or of infinity , and give necessary and sufficient conditions for it to be locally computable with one hidden layer .", "ner": [["hidden layer", 32, 33, "OtherScientificTerm"]], "relations": []},
{"sentence": "We show that adding these conditions to Gib-son 's assumptions is not sufficient to ensure global computability with one hidden layer , by exhibiting a new non-local configuration , the `` critical cycle '' , which implies that f is not computable with one hidden layer .", "ner": [["global computability", 15, 16, "OtherScientificTerm"], ["hidden layer", 19, 20, "OtherScientificTerm"], ["non-local configuration", 26, 27, "OtherScientificTerm"], ["`` critical cycle ''", 30, 33, "OtherScientificTerm"], ["hidden layer", 44, 45, "OtherScientificTerm"]], "relations": [[30, 33, 26, 27, "HYPONYM-OF"]]},
{"sentence": "This paper presents an approach to estimate the intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes from multiple view acquisition under unknown illumination conditions .", "ner": [["approach", 4, 4, "Generic"], ["intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes", 8, 19, "Task"], ["multiple view acquisition", 21, 23, "Task"], ["unknown illumination conditions", 25, 27, "OtherScientificTerm"]], "relations": [[4, 4, 8, 19, "USED-FOR"], [21, 23, 8, 19, "USED-FOR"], [25, 27, 21, 23, "FEATURE-OF"]]},
{"sentence": "We introduce the concept of intrinsic textures , which are pixel-resolution surface textures representing the intrinsic appearance parameters of a scene .", "ner": [["intrinsic textures", 5, 6, "OtherScientificTerm"], ["pixel-resolution surface textures", 10, 12, "OtherScientificTerm"], ["intrinsic appearance parameters", 15, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "Unlike previous video relighting methods , the approach does not assume regions of uniform albedo , which makes it applicable to richly textured scenes .", "ner": [["video relighting methods", 2, 4, "Method"], ["approach", 7, 7, "Generic"], ["uniform albedo", 13, 14, "OtherScientificTerm"], ["it", 18, 18, "Generic"], ["richly textured scenes", 21, 23, "Material"]], "relations": [[7, 7, 2, 4, "COMPARE"], [18, 18, 21, 23, "USED-FOR"]]},
{"sentence": "We show that intrinsic image methods can be used to refine an initial , low-frequency shading estimate based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading .", "ner": [["intrinsic image methods", 3, 5, "Method"], ["initial , low-frequency shading estimate", 12, 16, "Task"], ["global lighting reconstruction", 20, 22, "OtherScientificTerm"], ["texture and coarse scene geometry", 26, 30, "OtherScientificTerm"], ["inherent global ambiguity in shading", 36, 40, "OtherScientificTerm"]], "relations": [[3, 5, 12, 16, "USED-FOR"], [12, 16, 36, 40, "USED-FOR"], [20, 22, 3, 5, "USED-FOR"], [26, 30, 20, 22, "FEATURE-OF"]]},
{"sentence": "The method is applied to relight-ing of free-viewpoint rendering from multiple view video capture .", "ner": [["method", 1, 1, "Generic"], ["relight-ing of free-viewpoint rendering", 5, 8, "Task"], ["multiple view video capture", 10, 13, "OtherScientificTerm"]], "relations": [[1, 1, 5, 8, "USED-FOR"], [10, 13, 5, 8, "USED-FOR"]]},
{"sentence": "This demonstrates relighting with reproduction of fine surface detail .", "ner": [["relighting", 2, 2, "Task"], ["reproduction of fine surface detail", 4, 8, "OtherScientificTerm"]], "relations": [[4, 8, 2, 2, "FEATURE-OF"]]},
{"sentence": "Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach , implemented in a measure called POURPRE , for automatically evaluating answers to definition questions .", "ner": [["automatic evaluation", 5, 6, "Task"], ["automatic evaluation of machine translation and document summarization", 5, 12, "Task"], ["machine translation", 8, 9, "Task"], ["document summarization", 11, 12, "Task"], ["approach", 18, 18, "Generic"], ["measure", 23, 23, "Generic"], ["POURPRE", 25, 25, "Metric"], ["automatically evaluating answers to definition questions", 28, 33, "Task"]], "relations": [[5, 6, 8, 9, "EVALUATE-FOR"], [5, 6, 11, 12, "EVALUATE-FOR"], [8, 9, 11, 12, "CONJUNCTION"], [23, 23, 18, 18, "USED-FOR"], [23, 23, 28, 33, "USED-FOR"]]},
{"sentence": "Until now , the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system 's response .", "ner": [], "relations": []},
{"sentence": "The lack of automatic methods for scoring system output is an impediment to progress in the field , which we address with this work .", "ner": [], "relations": []},
{"sentence": "Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics .", "ner": [["TREC 2003 and TREC 2004 QA tracks", 3, 9, "Material"], ["rankings", 12, 12, "OtherScientificTerm"], ["metric", 16, 16, "Generic"], ["official rankings", 20, 21, "OtherScientificTerm"], ["POURPRE", 25, 25, "Metric"], ["metrics", 31, 31, "Generic"]], "relations": [[3, 9, 16, 16, "EVALUATE-FOR"], [3, 9, 25, 25, "EVALUATE-FOR"], [3, 9, 31, 31, "EVALUATE-FOR"], [16, 16, 12, 12, "USED-FOR"], [25, 25, 31, 31, "COMPARE"]]},
{"sentence": "Recent advances in Automatic Speech Recognition technology have put the goal of naturally sounding dialog systems within reach .", "ner": [["Automatic Speech Recognition technology", 3, 6, "Method"], ["dialog systems", 14, 15, "Task"]], "relations": [[3, 6, 14, 15, "USED-FOR"]]},
{"sentence": "However , the improved speech recognition has brought to light a new problem : as dialog systems understand more of what the user tells them , they need to be more sophisticated at responding to the user .", "ner": [["speech recognition", 4, 5, "Task"], ["dialog systems", 15, 16, "Task"], ["they", 26, 26, "Generic"]], "relations": []},
{"sentence": "The issue of system response to users has been extensively studied by the natural language generation community , though rarely in the context of dialog systems .", "ner": [["system response", 3, 4, "OtherScientificTerm"], ["natural language generation community", 13, 16, "Task"], ["dialog systems", 24, 25, "Task"]], "relations": [[3, 4, 13, 16, "PART-OF"], [13, 16, 24, 25, "COMPARE"]]},
{"sentence": "We show how research in generation can be adapted to dialog systems , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .", "ner": [["generation", 5, 5, "Task"], ["dialog systems", 10, 11, "Task"], ["hand-crafting knowledge-based generation systems", 19, 22, "Method"], ["machine learning techniques", 28, 30, "Method"]], "relations": [[5, 5, 10, 11, "USED-FOR"], [28, 30, 19, 22, "USED-FOR"]]},
{"sentence": "We present a tool , called ILIMP , which takes as input a raw text in French and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag -LSB- ANA -RSB- for anaphoric or -LSB- IMP -RSB- for impersonal or expletive .", "ner": [["tool", 3, 3, "Generic"], ["ILIMP", 6, 6, "Method"], ["raw text in French", 13, 16, "Material"]], "relations": [[13, 16, 3, 3, "USED-FOR"]]},
{"sentence": "This tool is therefore designed to distinguish between the anaphoric occurrences of il , for which an anaphora resolution system has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent .", "ner": [["tool", 1, 1, "Generic"], ["anaphoric occurrences of il", 9, 12, "Task"], ["anaphora resolution system", 17, 19, "Method"]], "relations": [[1, 1, 9, 12, "USED-FOR"], [17, 19, 9, 12, "USED-FOR"]]},
{"sentence": "The precision rate for ILIMP is 97,5 % .", "ner": [["precision rate", 1, 2, "Metric"], ["ILIMP", 4, 4, "Method"]], "relations": [[1, 2, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "The few errors are analyzed in detail .", "ner": [], "relations": []},
{"sentence": "Other tasks using the method developed for ILIMP are described briefly , as well as the use of ILIMP in a modular syntactic analysis system .", "ner": [["tasks", 1, 1, "Generic"], ["method", 4, 4, "Generic"], ["ILIMP", 7, 7, "Method"], ["ILIMP", 18, 18, "Method"], ["modular syntactic analysis system", 21, 24, "Method"]], "relations": [[4, 4, 1, 1, "USED-FOR"], [4, 4, 7, 7, "USED-FOR"], [18, 18, 21, 24, "USED-FOR"]]},
{"sentence": "Speech-based depression detection has gained importance in recent years , but most research has used relatively quiet conditions or examined a single corpus per study .", "ner": [["Speech-based depression detection", 0, 2, "Task"]], "relations": []},
{"sentence": "Little is thus known about the robustness of speech cues in the wild .", "ner": [["robustness", 6, 6, "Metric"], ["speech cues", 8, 9, "OtherScientificTerm"]], "relations": [[8, 9, 6, 6, "FEATURE-OF"]]},
{"sentence": "This study compares the effect of noise and reverberation on depression prediction using 1 -RRB- standard mel-frequency cepstral coefficients -LRB- MFCCs -RRB- , and 2 -RRB- features designed for noise robustness , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "ner": [["noise", 6, 6, "OtherScientificTerm"], ["reverberation", 8, 8, "OtherScientificTerm"], ["depression prediction", 10, 11, "Task"], ["mel-frequency cepstral coefficients -LRB- MFCCs -RRB-", 16, 21, "Method"], ["features", 26, 26, "Generic"], ["noise robustness", 29, 30, "Metric"], ["damped oscillator cepstral coefficients -LRB- DOCCs -RRB-", 32, 38, "Method"]], "relations": [[6, 6, 8, 8, "CONJUNCTION"], [6, 6, 10, 11, "FEATURE-OF"], [8, 8, 10, 11, "FEATURE-OF"], [16, 21, 10, 11, "USED-FOR"], [16, 21, 26, 26, "CONJUNCTION"], [26, 26, 29, 30, "USED-FOR"], [26, 26, 32, 38, "CONJUNCTION"]]},
{"sentence": "Data come from the 2014 AudioVisual Emotion Recognition Challenge -LRB- AVEC -RRB- .", "ner": [["AudioVisual Emotion Recognition Challenge -LRB- AVEC -RRB-", 5, 11, "Material"]], "relations": []},
{"sentence": "Results using additive noise and reverberation reveal a consistent pattern of findings for multiple evaluation metrics under both matched and mismatched conditions .", "ner": [["additive noise", 2, 3, "OtherScientificTerm"], ["reverberation", 5, 5, "OtherScientificTerm"], ["evaluation metrics", 14, 15, "Metric"]], "relations": [[2, 3, 5, 5, "CONJUNCTION"]]},
{"sentence": "First and most notably : standard MFCC features suffer dramatically under test/train mismatch for both noise and reverberation ; DOCC features are far more robust .", "ner": [["MFCC features", 6, 7, "Method"], ["noise", 15, 15, "OtherScientificTerm"], ["reverberation", 17, 17, "OtherScientificTerm"], ["DOCC features", 19, 20, "Method"]], "relations": [[15, 15, 17, 17, "CONJUNCTION"], [19, 20, 6, 7, "COMPARE"]]},
{"sentence": "Second , including higher-order cepstral coefficients is generally beneficial .", "ner": [["higher-order cepstral coefficients", 3, 5, "OtherScientificTerm"]], "relations": []},
{"sentence": "Third , artificial neural networks tend to outperform support vector regression .", "ner": [["artificial neural networks", 2, 4, "Method"], ["support vector regression", 8, 10, "Method"]], "relations": [[2, 4, 8, 10, "COMPARE"]]},
{"sentence": "Fourth , spontaneous speech appears to offer better robustness than read speech .", "ner": [["spontaneous speech", 2, 3, "Material"], ["read speech", 10, 11, "Material"]], "relations": [[2, 3, 10, 11, "COMPARE"]]},
{"sentence": "Finally , a cross-corpus -LRB- and cross-language -RRB- experiment reveals better noise and reverberation robustness for DOCCs than for MFCCs .", "ner": [["cross-corpus -LRB- and cross-language -RRB- experiment", 3, 8, "OtherScientificTerm"], ["noise and reverberation robustness", 11, 14, "Metric"], ["DOCCs", 16, 16, "Method"], ["MFCCs", 19, 19, "Method"]], "relations": [[3, 8, 16, 16, "EVALUATE-FOR"], [3, 8, 19, 19, "EVALUATE-FOR"], [11, 14, 16, 16, "EVALUATE-FOR"], [11, 14, 19, 19, "EVALUATE-FOR"], [16, 16, 19, 19, "COMPARE"]]},
{"sentence": "Implications and future directions for real-world robust depression detection are discussed .", "ner": [["real-world robust depression detection", 5, 8, "Task"]], "relations": []},
{"sentence": "This paper proposes document oriented preference sets -LRB- DoPS -RRB- for the disambiguation of the dependency structure of sentences .", "ner": [["document oriented preference sets -LRB- DoPS -RRB-", 3, 9, "Method"], ["disambiguation of the dependency structure", 12, 16, "Task"]], "relations": [[3, 9, 12, 16, "USED-FOR"]]},
{"sentence": "The DoPS system extracts preference knowledge from a target document or other documents automatically .", "ner": [["DoPS system", 1, 2, "Method"]], "relations": []},
{"sentence": "Sentence ambiguities can be resolved by using domain targeted preference knowledge without using complicated large knowledgebases .", "ner": [["Sentence ambiguities", 0, 1, "OtherScientificTerm"], ["domain targeted preference knowledge", 7, 10, "OtherScientificTerm"], ["knowledgebases", 15, 15, "Material"]], "relations": [[7, 10, 0, 1, "USED-FOR"], [7, 10, 15, 15, "COMPARE"]]},
{"sentence": "Implementation and empirical results are described for the the analysis of dependency structures of Japanese patent claim sentences .", "ner": [["dependency structures", 11, 12, "OtherScientificTerm"], ["Japanese patent claim sentences", 14, 17, "Material"]], "relations": [[11, 12, 14, 17, "FEATURE-OF"]]},
{"sentence": "Multimodal interfaces require effective parsing and understanding of utterances whose content is distributed across multiple input modes .", "ner": [["Multimodal interfaces", 0, 1, "Task"], ["parsing", 4, 4, "Method"]], "relations": [[4, 4, 0, 1, "USED-FOR"]]},
{"sentence": "Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a unification-based grammar that is used by a multidimensional chart parser to compose inputs .", "ner": [["approach", 4, 4, "Generic"], ["multimodal integration", 9, 10, "Task"], ["unification-based grammar", 16, 17, "Method"], ["multidimensional chart parser", 23, 25, "Method"]], "relations": [[4, 4, 9, 10, "USED-FOR"], [16, 17, 9, 10, "USED-FOR"], [16, 17, 23, 25, "USED-FOR"]]},
{"sentence": "This approach is highly expressive and supports a broad class of interfaces , but offers only limited potential for mutual compensation among the input modes , is subject to significant concerns in terms of computational complexity , and complicates selection among alternative multimodal interpretations of the input .", "ner": [["approach", 1, 1, "Generic"], ["interfaces", 11, 11, "OtherScientificTerm"], ["computational complexity", 34, 35, "Metric"]], "relations": []},
{"sentence": "In this paper , we present an alternative approach in which multimodal parsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation .", "ner": [["approach", 8, 8, "Generic"], ["multimodal parsing and understanding", 11, 14, "Task"], ["weighted finite-state device", 19, 21, "Method"], ["speech and gesture streams", 24, 27, "Material"]], "relations": [[8, 8, 11, 14, "USED-FOR"], [19, 21, 11, 14, "USED-FOR"], [24, 27, 19, 21, "USED-FOR"]]},
{"sentence": "This approach is significantly more efficient , enables tight-coupling of multimodal understanding with speech recognition , and provides a general probabilistic framework for multimodal ambiguity resolution .", "ner": [["approach", 1, 1, "Generic"], ["multimodal understanding", 10, 11, "Task"], ["speech recognition", 13, 14, "Task"], ["multimodal ambiguity resolution", 23, 25, "Task"]], "relations": [[1, 1, 23, 25, "USED-FOR"], [13, 14, 10, 11, "CONJUNCTION"]]},
{"sentence": "Recently , we initiated a project to develop a phonetically-based spoken language understanding system called SUMMIT .", "ner": [["phonetically-based spoken language understanding system", 9, 13, "Method"], ["SUMMIT", 15, 15, "Method"]], "relations": [[15, 15, 9, 13, "HYPONYM-OF"]]},
{"sentence": "In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering , our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools .", "ner": [["heuristic rules", 12, 13, "Method"], ["knowledge engineering", 18, 19, "Method"], ["approach", 22, 22, "Generic"], ["speech knowledge", 27, 28, "OtherScientificTerm"], ["mathematical tools", 35, 36, "Method"]], "relations": [[18, 19, 12, 13, "USED-FOR"], [22, 22, 27, 28, "USED-FOR"], [35, 36, 27, 28, "USED-FOR"]]},
{"sentence": "In our system , features and decision strategies are discovered and trained automatically , using a large body of speech data .", "ner": [["system", 2, 2, "Generic"], ["features", 4, 4, "OtherScientificTerm"], ["decision strategies", 6, 7, "Method"], ["speech data", 19, 20, "Material"]], "relations": [[4, 4, 6, 7, "CONJUNCTION"], [19, 20, 6, 7, "USED-FOR"]]},
{"sentence": "This paper describes the system , and documents its current performance .", "ner": [["system", 4, 4, "Generic"]], "relations": []},
{"sentence": "This paper describes an implemented program that takes a tagged text corpus and generates a partial list of the subcategorization frames in which each verb occurs .", "ner": [["program", 5, 5, "Generic"], ["tagged text corpus", 9, 11, "Material"], ["subcategorization frames", 19, 20, "OtherScientificTerm"]], "relations": [[9, 11, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "The completeness of the output list increases monotonically with the total occurrences of each verb in the training corpus .", "ner": [], "relations": []},
{"sentence": "False positive rates are one to three percent .", "ner": [["False positive rates", 0, 2, "Metric"]], "relations": []},
{"sentence": "Five subcategorization frames are currently detected and we foresee no impediment to detecting many more .", "ner": [["subcategorization frames", 1, 2, "OtherScientificTerm"]], "relations": []},
{"sentence": "Ultimately , we expect to provide a large subcategorization dictionary to the NLP community and to train dictionaries for specific corpora .", "ner": [["subcategorization dictionary", 8, 9, "Material"]], "relations": []},
{"sentence": "We present a method for estimating the relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras from observing a moving object .", "ner": [["method", 3, 3, "Generic"], ["relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras", 7, 16, "Task"]], "relations": [[3, 3, 7, 16, "USED-FOR"]]},
{"sentence": "We show how to tackle the problem of missing point correspondences heavily required by SfM pipelines and how to go beyond this basic paradigm .", "ner": [["missing point correspondences", 8, 10, "Task"], ["SfM pipelines", 14, 15, "Method"], ["paradigm", 23, 23, "Generic"]], "relations": [[14, 15, 8, 10, "USED-FOR"]]},
{"sentence": "We relax the non-linear nature of the problem by accepting two assumptions which surveillance scenarios offer , ie .", "ner": [["non-linear nature", 3, 4, "OtherScientificTerm"], ["problem", 7, 7, "Generic"], ["assumptions", 11, 11, "Generic"], ["surveillance scenarios", 13, 14, "OtherScientificTerm"]], "relations": [[3, 4, 7, 7, "FEATURE-OF"]]},
{"sentence": "the presence of a moving object and easily estimable gravity vector .", "ner": [["gravity vector", 9, 10, "OtherScientificTerm"]], "relations": []},
{"sentence": "By those assumptions we cast the problem as a Quadratic Eigenvalue Problem offering an elegant way of treating nonlinear monomials and delivering a quasi closed-form solution as a reliable starting point for a further bundle adjustment .", "ner": [["assumptions", 2, 2, "Generic"], ["problem", 6, 6, "Generic"], ["Quadratic Eigenvalue Problem", 9, 11, "Task"], ["nonlinear monomials", 18, 19, "OtherScientificTerm"], ["quasi closed-form solution", 23, 25, "Method"], ["bundle adjustment", 34, 35, "Task"]], "relations": [[9, 11, 6, 6, "USED-FOR"], [9, 11, 18, 19, "USED-FOR"], [9, 11, 23, 25, "USED-FOR"], [23, 25, 34, 35, "USED-FOR"]]},
{"sentence": "We are the first to bring the closed form solution to such a very practical problem arising in video surveillance .", "ner": [["closed form solution", 7, 9, "Method"], ["problem", 15, 15, "Generic"], ["video surveillance", 18, 19, "Task"]], "relations": [[7, 9, 15, 15, "USED-FOR"], [18, 19, 15, 15, "FEATURE-OF"]]},
{"sentence": "Results in different camera setups demonstrate the feasibility of the approach .", "ner": [["approach", 10, 10, "Generic"]], "relations": []},
{"sentence": "In this paper , we propose a human action recognition system suitable for embedded computer vision applications in security systems , human-computer interaction and intelligent environments .", "ner": [["human action recognition system", 7, 10, "Method"], ["embedded computer vision applications", 13, 16, "Task"], ["security systems", 18, 19, "Task"], ["human-computer interaction", 21, 22, "Task"], ["intelligent environments", 24, 25, "Task"]], "relations": [[7, 10, 13, 16, "USED-FOR"], [13, 16, 18, 19, "USED-FOR"], [13, 16, 21, 22, "USED-FOR"], [13, 16, 24, 25, "USED-FOR"], [18, 19, 21, 22, "CONJUNCTION"], [21, 22, 24, 25, "CONJUNCTION"]]},
{"sentence": "Our system is suitable for embedded computer vision application based on three reasons .", "ner": [["system", 1, 1, "Generic"], ["embedded computer vision application", 5, 8, "Task"]], "relations": [[1, 1, 5, 8, "USED-FOR"]]},
{"sentence": "Firstly , the system was based on a linear Support Vector Machine -LRB- SVM -RRB- classifier where classification progress can be implemented easily and quickly in embedded hardware .", "ner": [["system", 3, 3, "Generic"], ["linear Support Vector Machine -LRB- SVM -RRB- classifier", 8, 15, "Method"], ["classification progress", 17, 18, "Method"], ["embedded hardware", 26, 27, "OtherScientificTerm"]], "relations": [[8, 15, 3, 3, "USED-FOR"], [26, 27, 17, 18, "USED-FOR"]]},
{"sentence": "Secondly , we use compacted motion features easily obtained from videos .", "ner": [["compacted motion features", 4, 6, "OtherScientificTerm"], ["videos", 10, 10, "Material"]], "relations": [[10, 10, 4, 6, "USED-FOR"]]},
{"sentence": "We address the limitations of the well known Motion History Image -LRB- MHI -RRB- and propose a new Hierarchical Motion History Histogram -LRB- HMHH -RRB- feature to represent the motion information .", "ner": [["Motion History Image -LRB- MHI -RRB-", 8, 13, "Method"], ["Hierarchical Motion History Histogram -LRB- HMHH -RRB- feature", 18, 25, "Method"], ["motion information", 29, 30, "OtherScientificTerm"]], "relations": [[18, 25, 29, 30, "USED-FOR"]]},
{"sentence": "HMHH not only provides rich motion information , but also remains computationally inexpensive .", "ner": [["HMHH", 0, 0, "Method"], ["rich motion information", 4, 6, "OtherScientificTerm"]], "relations": [[0, 0, 4, 6, "USED-FOR"]]},
{"sentence": "Finally , we combine MHI and HMHH together and extract a low dimension feature vector to be used in the SVM classifiers .", "ner": [["MHI", 4, 4, "Method"], ["HMHH", 6, 6, "Method"], ["low dimension feature vector", 11, 14, "OtherScientificTerm"], ["SVM classifiers", 20, 21, "Method"]], "relations": [[4, 4, 6, 6, "CONJUNCTION"], [4, 4, 11, 14, "USED-FOR"], [6, 6, 11, 14, "USED-FOR"], [11, 14, 20, 21, "USED-FOR"]]},
{"sentence": "Experimental results show that our system achieves significant improvement on the recognition performance .", "ner": [["system", 5, 5, "Generic"], ["recognition", 11, 11, "Task"]], "relations": [[11, 11, 5, 5, "EVALUATE-FOR"]]},
{"sentence": "In this paper I will argue for a model of grammatical processing that is based on uniform processing and knowledge sources .", "ner": [["model of grammatical processing", 8, 11, "Method"], ["uniform processing", 16, 17, "Method"], ["knowledge sources", 19, 20, "Material"]], "relations": [[16, 17, 8, 11, "USED-FOR"], [19, 20, 8, 11, "USED-FOR"], [19, 20, 16, 17, "CONJUNCTION"]]},
{"sentence": "The main feature of this model is to view parsing and generation as two strongly interleaved tasks performed by a single parametrized deduction process .", "ner": [["model", 5, 5, "Generic"], ["parsing", 9, 9, "Task"], ["generation", 11, 11, "Task"], ["tasks", 16, 16, "Generic"], ["parametrized deduction process", 21, 23, "Method"]], "relations": [[9, 9, 11, 11, "CONJUNCTION"], [9, 9, 16, 16, "HYPONYM-OF"], [11, 11, 16, 16, "HYPONYM-OF"], [21, 23, 16, 16, "USED-FOR"]]},
{"sentence": "It will be shown that this view supports flexible and efficient natural language processing .", "ner": [["natural language processing", 11, 13, "Task"]], "relations": []},
{"sentence": "Link detection has been regarded as a core technology for the Topic Detection and Tracking tasks of new event detection .", "ner": [["Link detection", 0, 1, "Task"], ["Topic Detection and Tracking tasks of new event detection", 11, 19, "Task"]], "relations": [[0, 1, 11, 19, "USED-FOR"]]},
{"sentence": "In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both systems .", "ner": [["story link detection", 5, 7, "Task"], ["new event detection", 9, 11, "Task"], ["information retrieval task", 13, 15, "Task"], ["precision", 22, 22, "Metric"], ["recall", 24, 24, "Metric"], ["systems", 27, 27, "Generic"]], "relations": [[5, 7, 9, 11, "CONJUNCTION"], [5, 7, 27, 27, "HYPONYM-OF"], [9, 11, 27, 27, "HYPONYM-OF"], [13, 15, 5, 7, "USED-FOR"], [13, 15, 9, 11, "USED-FOR"], [22, 22, 24, 24, "CONJUNCTION"], [22, 22, 27, 27, "EVALUATE-FOR"], [24, 24, 27, 27, "EVALUATE-FOR"]]},
{"sentence": "Motivated by these arguments , we introduce a number of new performance enhancing techniques including part of speech tagging , new similarity measures and expanded stop lists .", "ner": [["performance enhancing techniques", 11, 13, "Method"], ["part of speech tagging", 15, 18, "Method"], ["similarity measures", 21, 22, "Method"], ["expanded stop lists", 24, 26, "OtherScientificTerm"]], "relations": [[15, 18, 11, 13, "PART-OF"], [15, 18, 21, 22, "CONJUNCTION"], [21, 22, 11, 13, "PART-OF"], [21, 22, 24, 26, "CONJUNCTION"], [24, 26, 11, 13, "PART-OF"]]},
{"sentence": "Experimental results validate our hypothesis .", "ner": [], "relations": []},
{"sentence": "We attempt to understand visual classification in humans using both psy-chophysical and machine learning techniques .", "ner": [["visual classification", 4, 5, "Task"], ["psy-chophysical and machine learning techniques", 10, 14, "Method"]], "relations": [[10, 14, 4, 5, "USED-FOR"]]},
{"sentence": "Frontal views of human faces were used for a gender classification task .", "ner": [["Frontal views of human faces", 0, 4, "OtherScientificTerm"], ["gender classification task", 9, 11, "Task"]], "relations": [[0, 4, 9, 11, "USED-FOR"]]},
{"sentence": "Human subjects classified the faces and their gender judgment , reaction time and confidence rating were recorded .", "ner": [["gender judgment", 7, 8, "OtherScientificTerm"], ["reaction time", 10, 11, "OtherScientificTerm"], ["confidence rating", 13, 14, "OtherScientificTerm"]], "relations": []},
{"sentence": "Several hyperplane learning algorithms were used on the same classification task using the Principal Components of the texture and flowfield representation of the faces .", "ner": [["hyperplane learning algorithms", 1, 3, "Method"], ["classification task", 9, 10, "Task"], ["Principal Components of the texture", 13, 17, "OtherScientificTerm"], ["flowfield representation of the faces", 19, 23, "Method"]], "relations": [[1, 3, 9, 10, "USED-FOR"], [13, 17, 1, 3, "USED-FOR"], [19, 23, 1, 3, "USED-FOR"], [19, 23, 13, 17, "CONJUNCTION"]]},
{"sentence": "The classification performance of the learning algorithms was estimated using the face database with the true gender of the faces as labels , and also with the gender estimated by the subjects .", "ner": [["classification", 1, 1, "Task"], ["learning algorithms", 5, 6, "Method"], ["face database", 11, 12, "Task"]], "relations": [[5, 6, 1, 1, "USED-FOR"], [11, 12, 5, 6, "EVALUATE-FOR"]]},
{"sentence": "We then correlated the human responses to the distance of the stimuli to the separating hyperplane of the learning algorithms .", "ner": [["hyperplane of the learning algorithms", 15, 19, "OtherScientificTerm"], ["learning algorithms", 18, 19, "Method"]], "relations": []},
{"sentence": "Our results suggest that human classification can be modeled by some hyperplane algorithms in the feature space we used .", "ner": [["human classification", 4, 5, "Task"], ["hyperplane algorithms", 11, 12, "Method"], ["feature space", 15, 16, "OtherScientificTerm"]], "relations": [[11, 12, 4, 5, "USED-FOR"], [15, 16, 11, 12, "FEATURE-OF"]]},
{"sentence": "For classification , the brain needs more processing for stimuli close to that hyperplane than for those further away .", "ner": [["classification", 1, 1, "Task"], ["hyperplane", 13, 13, "OtherScientificTerm"], ["those", 16, 16, "Generic"]], "relations": [[13, 13, 16, 16, "COMPARE"]]},
{"sentence": "In this paper , we present a corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system for Dutch which combines statistical classification -LRB- maximum entropy -RRB- with linguistic information .", "ner": [["corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system", 7, 15, "Method"], ["Dutch", 17, 17, "Material"], ["statistical classification", 20, 21, "Method"], ["maximum entropy", 23, 24, "Method"], ["linguistic information", 27, 28, "OtherScientificTerm"]], "relations": [[7, 15, 17, 17, "USED-FOR"], [20, 21, 7, 15, "PART-OF"], [23, 24, 7, 15, "PART-OF"], [27, 28, 7, 15, "PART-OF"], [27, 28, 23, 24, "CONJUNCTION"]]},
{"sentence": "Instead of building individual classifiers per ambiguous wordform , we introduce a lemma-based approach .", "ner": [["classifiers", 4, 4, "Method"], ["ambiguous wordform", 6, 7, "OtherScientificTerm"], ["lemma-based approach", 12, 13, "Method"]], "relations": [[4, 4, 12, 13, "COMPARE"], [6, 7, 4, 4, "USED-FOR"]]},
{"sentence": "The advantage of this novel method is that it clusters all inflected forms of an ambiguous word in one classifier , therefore augmenting the training material available to the algorithm .", "ner": [["method", 5, 5, "Generic"], ["it", 8, 8, "Generic"], ["inflected forms", 11, 12, "OtherScientificTerm"], ["ambiguous word", 15, 16, "OtherScientificTerm"], ["classifier", 19, 19, "Method"], ["algorithm", 29, 29, "Generic"]], "relations": [[11, 12, 15, 16, "FEATURE-OF"]]},
{"sentence": "Testing the lemma-based model on the Dutch Senseval-2 test data , we achieve a significant increase in accuracy over the wordform model .", "ner": [["lemma-based model", 2, 3, "Method"], ["Dutch Senseval-2 test data", 6, 9, "Material"], ["wordform model", 20, 21, "Method"]], "relations": [[2, 3, 20, 21, "COMPARE"], [6, 9, 2, 3, "EVALUATE-FOR"]]},
{"sentence": "Also , the WSD system based on lemmas is smaller and more robust .", "ner": [["WSD system based on lemmas", 3, 7, "Method"]], "relations": []},
{"sentence": "We propose an exact , general and efficient coarse-to-fine energy minimization strategy for semantic video segmenta-tion .", "ner": [["coarse-to-fine energy minimization strategy", 8, 11, "Method"], ["semantic video segmenta-tion", 13, 15, "Task"]], "relations": [[8, 11, 13, 15, "USED-FOR"]]},
{"sentence": "Our strategy is based on a hierarchical abstraction of the supervoxel graph that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs .", "ner": [["strategy", 1, 1, "Generic"], ["hierarchical abstraction of the supervoxel graph", 6, 11, "Task"], ["hierarchy", 26, 26, "OtherScientificTerm"], ["coarser graphs", 36, 37, "OtherScientificTerm"]], "relations": [[6, 11, 1, 1, "USED-FOR"]]},
{"sentence": "The strategy is exact , i.e. , it produces the same solution as minimizing over the finest graph .", "ner": [["strategy", 1, 1, "Generic"], ["it", 7, 7, "Generic"], ["finest graph", 16, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "It is general , i.e. , it can be used to minimize any energy function -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing energy minimization algorithm -LRB- e.g. , graph cuts and belief propagation -RRB- .", "ner": [["It", 0, 0, "Generic"], ["it", 6, 6, "Generic"], ["energy function", 13, 14, "OtherScientificTerm"], ["energy minimization algorithm", 29, 31, "Method"], ["graph cuts", 35, 36, "Method"], ["belief propagation", 38, 39, "Method"]], "relations": [[6, 6, 13, 14, "USED-FOR"], [6, 6, 29, 31, "CONJUNCTION"], [29, 31, 13, 14, "USED-FOR"], [35, 36, 29, 31, "HYPONYM-OF"], [35, 36, 38, 39, "CONJUNCTION"], [38, 39, 29, 31, "HYPONYM-OF"]]},
{"sentence": "It also gives significant speedups in inference for several datasets with varying degrees of spatio-temporal continuity .", "ner": [["It", 0, 0, "Generic"], ["inference", 6, 6, "Task"], ["datasets", 9, 9, "Generic"], ["spatio-temporal continuity", 14, 15, "OtherScientificTerm"]], "relations": [[0, 0, 6, 6, "USED-FOR"], [9, 9, 0, 0, "EVALUATE-FOR"], [14, 15, 9, 9, "FEATURE-OF"]]},
{"sentence": "We also discuss the strengths and weaknesses of our strategy relative to existing hierarchical approaches , and the kinds of image and video data that provide the best speedups .", "ner": [["strategy", 9, 9, "Generic"], ["hierarchical approaches", 13, 14, "Method"], ["image and video data", 20, 23, "Material"]], "relations": [[9, 9, 13, 14, "COMPARE"]]},
{"sentence": "Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora .", "ner": [["ensemble methods", 5, 6, "Method"], ["machine learning", 8, 9, "Task"], ["natural language processing", 14, 16, "Task"], ["multi-strategy and multi-source approach", 21, 24, "Method"], ["question answering", 26, 27, "Task"], ["answering agents", 37, 38, "Method"]], "relations": [[5, 6, 8, 9, "USED-FOR"], [5, 6, 14, 16, "USED-FOR"], [21, 24, 26, 27, "USED-FOR"]]},
{"sentence": "The answering agents adopt fundamentally different strategies , one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .", "ner": [["answering agents", 1, 2, "Method"], ["strategies", 6, 6, "Generic"], ["one", 8, 8, "Generic"], ["knowledge-based mechanisms", 11, 12, "Method"], ["other", 15, 15, "Generic"], ["statistical techniques", 17, 18, "Method"]], "relations": [[6, 6, 1, 2, "USED-FOR"], [8, 8, 6, 6, "HYPONYM-OF"], [11, 12, 8, 8, "USED-FOR"], [15, 15, 6, 6, "HYPONYM-OF"], [17, 18, 15, 15, "USED-FOR"]]},
{"sentence": "We present our multi-level answer resolution algorithm that combines results from the answering agents at the question , passage , and/or answer levels .", "ner": [["multi-level answer resolution algorithm", 3, 6, "Method"], ["answering agents", 12, 13, "Method"], ["question , passage , and/or answer levels", 16, 22, "OtherScientificTerm"]], "relations": [[12, 13, 3, 6, "USED-FOR"]]},
{"sentence": "Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0 % relative improvement over our baseline system in the number of questions correctly answered , and a 32.8 % improvement according to the average precision metric .", "ner": [["answer resolution algorithm", 6, 8, "Method"], ["baseline system", 17, 18, "Generic"], ["average precision metric", 35, 37, "Metric"]], "relations": [[6, 8, 17, 18, "COMPARE"], [35, 37, 6, 8, "EVALUATE-FOR"], [35, 37, 17, 18, "EVALUATE-FOR"]]},
{"sentence": "Word Identification has been an important and active issue in Chinese Natural Language Processing .", "ner": [["Word Identification", 0, 1, "Task"], ["Chinese Natural Language Processing", 10, 13, "Task"]], "relations": [[0, 1, 10, 13, "HYPONYM-OF"]]},
{"sentence": "In this paper , a new mechanism , based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .", "ner": [["mechanism", 6, 6, "Generic"], ["sublanguage", 13, 13, "OtherScientificTerm"], ["unknown words", 19, 20, "OtherScientificTerm"], ["personal names", 23, 24, "OtherScientificTerm"], ["Chinese newspapers", 27, 28, "Material"]], "relations": [[6, 6, 19, 20, "USED-FOR"], [13, 13, 6, 6, "USED-FOR"], [23, 24, 19, 20, "HYPONYM-OF"], [27, 28, 6, 6, "USED-FOR"]]},
{"sentence": "The proposed mechanism includes title-driven name recognition , adaptive dynamic word formation , identification of 2-character and 3-character Chinese names without title .", "ner": [["mechanism", 2, 2, "Generic"], ["title-driven name recognition", 4, 6, "Task"], ["adaptive dynamic word formation", 8, 11, "Task"], ["identification of 2-character and 3-character Chinese names without title", 13, 21, "Task"]], "relations": [[4, 6, 2, 2, "PART-OF"], [4, 6, 8, 11, "CONJUNCTION"], [8, 11, 2, 2, "PART-OF"], [8, 11, 13, 21, "CONJUNCTION"], [13, 21, 2, 2, "PART-OF"]]},
{"sentence": "We will show the experimental results for two corpora and compare them with the results by the NTHU 's statistic-based system , the only system that we know has attacked the same problem .", "ner": [["NTHU 's statistic-based system", 17, 20, "Method"], ["system", 24, 24, "Generic"]], "relations": []},
{"sentence": "The experimental results have shown significant improvements over the WI systems without the name identification capability .", "ner": [["WI systems", 9, 10, "Method"], ["name identification", 13, 14, "Task"]], "relations": []},
{"sentence": "In this paper we summarize the experiences gained from a field trial of a speaker verification system .", "ner": [["speaker verification system", 14, 16, "Method"]], "relations": []},
{"sentence": "In the test implementation access to two rooms at the University of Frankfurt had been controlled by a speaker verification system .", "ner": [["speaker verification system", 18, 20, "Method"]], "relations": []},
{"sentence": "The paper is organized as follows : Firstly , we will describe the system concepts and implementation issues .", "ner": [], "relations": []},
{"sentence": "Secondly , results of the user evaluation are reported .", "ner": [["user evaluation", 5, 6, "Method"]], "relations": []},
{"sentence": "During the field trial all speech data was recorded .", "ner": [["speech data", 5, 6, "Material"]], "relations": []},
{"sentence": "The data base created in this way has been used extensively for simulation experiments .", "ner": [], "relations": []},
{"sentence": "In chapter 4 we will describe recent experiments focusing on the use of Hidden Markov Models .", "ner": [["Hidden Markov Models", 13, 15, "Method"]], "relations": []},
{"sentence": "This report describes Paul , a computer text generation system designed to create cohesive text through the use of lexical substitutions .", "ner": [["Paul", 3, 3, "Method"], ["computer text generation system", 6, 9, "Method"], ["cohesive text", 13, 14, "Material"], ["lexical substitutions", 19, 20, "OtherScientificTerm"]], "relations": [[3, 3, 6, 9, "HYPONYM-OF"], [6, 9, 13, 14, "USED-FOR"], [19, 20, 3, 3, "USED-FOR"]]},
{"sentence": "Specifically , this system is designed to deterministically choose between pronominalization , superordinate substitution , and definite noun phrase reiteration .", "ner": [["system", 3, 3, "Generic"], ["pronominalization", 10, 10, "OtherScientificTerm"], ["superordinate substitution", 12, 13, "OtherScientificTerm"], ["definite noun phrase reiteration", 16, 19, "OtherScientificTerm"]], "relations": [[10, 10, 12, 13, "COMPARE"], [12, 13, 16, 19, "COMPARE"]]},
{"sentence": "The system identifies a strength of antecedence recovery for each of the lexical substitutions .", "ner": [["system", 1, 1, "Generic"], ["antecedence recovery", 6, 7, "Task"], ["lexical substitutions", 12, 13, "OtherScientificTerm"]], "relations": [[1, 1, 6, 7, "USED-FOR"], [6, 7, 12, 13, "USED-FOR"]]},
{"sentence": "This paper addresses the problem of identifying likely topics of texts by their position in the text .", "ner": [], "relations": []},
{"sentence": "It describes the automated training and evaluation of an Optimal Position Policy , a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure .", "ner": [["Optimal Position Policy", 9, 11, "Method"], ["method", 14, 14, "Generic"], ["positions of topic-bearing sentences", 19, 22, "OtherScientificTerm"], ["genre-specific regularities of discourse structure", 25, 29, "OtherScientificTerm"]], "relations": [[14, 14, 19, 22, "USED-FOR"], [25, 29, 14, 14, "USED-FOR"]]},
{"sentence": "This method can be used in applications such as information retrieval , routing , and text summarization .", "ner": [["method", 1, 1, "Generic"], ["applications", 6, 6, "Generic"], ["information retrieval", 9, 10, "Task"], ["routing", 12, 12, "Task"], ["text summarization", 15, 16, "Task"]], "relations": [[1, 1, 6, 6, "USED-FOR"], [9, 10, 6, 6, "HYPONYM-OF"], [9, 10, 12, 12, "CONJUNCTION"], [12, 12, 6, 6, "HYPONYM-OF"], [12, 12, 15, 16, "CONJUNCTION"], [15, 16, 6, 6, "HYPONYM-OF"]]},
{"sentence": "We describe a general framework for online multiclass learning based on the notion of hypothesis sharing .", "ner": [["framework", 4, 4, "Generic"], ["online multiclass learning", 6, 8, "Task"], ["notion of hypothesis sharing", 12, 15, "OtherScientificTerm"]], "relations": [[4, 4, 6, 8, "USED-FOR"], [12, 15, 4, 4, "USED-FOR"]]},
{"sentence": "In our framework sets of classes are associated with hypotheses .", "ner": [["framework", 2, 2, "Generic"]], "relations": []},
{"sentence": "Thus , all classes within a given set share the same hypothesis .", "ner": [], "relations": []},
{"sentence": "This framework includes as special cases commonly used constructions for multiclass categorization such as allocating a unique hypothesis for each class and allocating a single common hypothesis for all classes .", "ner": [["framework", 1, 1, "Generic"], ["multiclass categorization", 10, 11, "Task"]], "relations": []},
{"sentence": "We generalize the multiclass Perceptron to our framework and derive a unifying mistake bound analysis .", "ner": [["multiclass Perceptron", 3, 4, "Method"], ["framework", 7, 7, "Generic"], ["unifying mistake bound analysis", 11, 14, "Method"]], "relations": [[3, 4, 7, 7, "USED-FOR"]]},
{"sentence": "Our construction naturally extends to settings where the number of classes is not known in advance but , rather , is revealed along the online learning process .", "ner": [["online learning process", 24, 26, "Method"]], "relations": []},
{"sentence": "We demonstrate the merits of our approach by comparing it to previous methods on both synthetic and natural datasets .", "ner": [["approach", 6, 6, "Generic"], ["it", 9, 9, "Generic"], ["methods", 12, 12, "Generic"], ["synthetic and natural datasets", 15, 18, "Material"]], "relations": [[9, 9, 12, 12, "COMPARE"], [15, 18, 9, 9, "EVALUATE-FOR"], [15, 18, 12, 12, "EVALUATE-FOR"]]},
{"sentence": "We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions .", "ner": [["supervised machine learning", 5, 7, "Method"], ["statistical models of WH-questions", 14, 17, "Method"]], "relations": [[5, 7, 14, 17, "USED-FOR"]]},
{"sentence": "These models , which are built from shallow linguistic features of questions , are employed to predict target variables which represent a user 's informational goals .", "ner": [["models", 1, 1, "Generic"], ["shallow linguistic features of questions", 7, 11, "OtherScientificTerm"], ["user 's informational goals", 22, 25, "OtherScientificTerm"]], "relations": [[7, 11, 1, 1, "USED-FOR"]]},
{"sentence": "We report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables .", "ner": [["models", 11, 11, "Generic"], ["training and testing factors", 18, 21, "OtherScientificTerm"]], "relations": []},
{"sentence": "We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures , and illustrate how this allows one to view NLP tasks as graph transformations .", "ner": [["labeled directed graph", 9, 11, "Method"], ["linguistic structures", 17, 18, "OtherScientificTerm"], ["this", 23, 23, "Generic"], ["NLP tasks", 28, 29, "Task"], ["graph transformations", 31, 32, "Task"]], "relations": [[9, 11, 17, 18, "USED-FOR"], [9, 11, 28, 29, "USED-FOR"], [23, 23, 28, 29, "USED-FOR"]]},
{"sentence": "We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies -LRB- using Penn Treebank data -RRB- and semantic role labeling -LRB- using Proposition Bank data -RRB- .", "ner": [["method", 4, 4, "Method"], ["transformations", 8, 8, "Generic"], ["annotated corpus", 11, 12, "Material"], ["applications", 18, 18, "Generic"], ["method", 21, 21, "Generic"], ["identification of non-local depenencies", 23, 26, "Task"], ["Penn Treebank data", 29, 31, "Material"], ["semantic role labeling", 34, 36, "Task"], ["Proposition Bank data", 39, 41, "Material"]], "relations": [[4, 4, 8, 8, "USED-FOR"], [11, 12, 4, 4, "USED-FOR"], [21, 21, 18, 18, "USED-FOR"], [23, 26, 18, 18, "HYPONYM-OF"], [29, 31, 23, 26, "USED-FOR"], [34, 36, 18, 18, "HYPONYM-OF"], [39, 41, 34, 36, "USED-FOR"]]},
{"sentence": "We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed linguistic information to resolve ambiguity .", "ner": [["generative probabilistic model of natural language", 3, 8, "Method"], ["HBG", 13, 13, "Method"], ["linguistic information", 20, 21, "OtherScientificTerm"], ["ambiguity", 24, 24, "Task"]], "relations": [[20, 21, 24, 24, "USED-FOR"]]},
{"sentence": "HBG incorporates lexical , syntactic , semantic , and structural information from the parse tree into the disambiguation process in a novel way .", "ner": [["HBG", 0, 0, "Method"], ["lexical , syntactic , semantic , and structural information", 2, 10, "OtherScientificTerm"], ["parse tree", 13, 14, "OtherScientificTerm"], ["disambiguation process", 17, 18, "Task"]], "relations": [[0, 0, 17, 18, "USED-FOR"], [2, 10, 0, 0, "USED-FOR"]]},
{"sentence": "We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence .", "ner": [["corpus of bracketed sentences", 3, 6, "Material"], ["Treebank", 10, 10, "Material"], ["decision tree building", 15, 17, "Method"], ["parse tree", 26, 27, "OtherScientificTerm"], ["parse", 33, 33, "OtherScientificTerm"]], "relations": [[3, 6, 15, 17, "CONJUNCTION"], [3, 6, 26, 27, "USED-FOR"], [15, 17, 26, 27, "USED-FOR"], [26, 27, 33, 33, "USED-FOR"]]},
{"sentence": "This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse .", "ner": [["grammar tailoring", 10, 11, "Task"], ["linguistic introspection", 15, 16, "Method"], ["parse", 24, 24, "OtherScientificTerm"]], "relations": [[10, 11, 24, 24, "USED-FOR"], [15, 16, 10, 11, "USED-FOR"]]},
{"sentence": "In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG , the HBG model significantly outperforms P-CFG , increasing the parsing accuracy rate from 60 % to 75 % , a 37 % reduction in error .", "ner": [["head-to-head tests", 1, 2, "OtherScientificTerm"], ["robust probabilistic parsing models", 9, 12, "Method"], ["P-CFG", 17, 17, "Method"], ["HBG model", 20, 21, "Method"], ["P-CFG", 24, 24, "Method"], ["parsing accuracy rate", 28, 30, "Metric"]], "relations": [[17, 17, 9, 12, "HYPONYM-OF"], [20, 21, 24, 24, "COMPARE"], [28, 30, 20, 21, "EVALUATE-FOR"]]},
{"sentence": "The verb forms are often claimed to convey two kinds of information : 1 .", "ner": [["verb forms", 1, 2, "OtherScientificTerm"], ["information", 11, 11, "Generic"]], "relations": []},
{"sentence": "whether the event described in a sentence is present , past or future -LRB- = deictic information -RRB- 2 .", "ner": [["deictic information", 15, 16, "OtherScientificTerm"]], "relations": []},
{"sentence": "whether the event described in a sentence is presented as completed , going on , just starting or being finished -LRB- = aspectual information -RRB- .", "ner": [["aspectual information", 22, 23, "OtherScientificTerm"]], "relations": []},
{"sentence": "It will be demonstrated in this paper that one has to add a third component to the analysis of verb form meanings , namely whether or not they express habituality .", "ner": [["analysis of verb form meanings", 17, 21, "Task"]], "relations": []},
{"sentence": "The framework of the analysis is model-theoretic semantics .", "ner": [["analysis", 4, 4, "Generic"], ["model-theoretic semantics", 6, 7, "OtherScientificTerm"]], "relations": [[6, 7, 4, 4, "USED-FOR"]]},
{"sentence": "This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources .", "ner": [["word-sense ambiguity", 6, 7, "Task"], ["machine-readable resources", 11, 12, "Material"], ["construction of large-scale knowledge sources", 15, 19, "Task"]], "relations": [[11, 12, 6, 7, "USED-FOR"], [11, 12, 15, 19, "USED-FOR"]]},
{"sentence": "We describe two experiments : one which ignored word-sense distinctions , resulting in 6.3 % accuracy for semantic classification of verbs based on -LRB- Levin , 1993 -RRB- ; and one which exploited word-sense distinctions , resulting in 97.9 % accuracy .", "ner": [["word-sense distinctions", 8, 9, "OtherScientificTerm"], ["accuracy", 15, 15, "Metric"], ["semantic classification", 17, 18, "Method"], ["word-sense distinctions", 33, 34, "OtherScientificTerm"], ["accuracy", 40, 40, "Metric"]], "relations": [[15, 15, 17, 18, "EVALUATE-FOR"]]},
{"sentence": "These experiments were dual purpose : -LRB- 1 -RRB- to validate the central thesis of the work of -LRB- Levin , 1993 -RRB- , i.e. , that verb semantics and syntactic behavior are predictably related ; -LRB- 2 -RRB- to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses .", "ner": [["verb semantics", 27, 28, "OtherScientificTerm"], ["syntactic behavior", 30, 31, "OtherScientificTerm"], ["semantic information", 50, 51, "OtherScientificTerm"], ["syntactic cues", 53, 54, "OtherScientificTerm"], ["syntactic cues", 60, 61, "OtherScientificTerm"], ["word senses", 69, 70, "OtherScientificTerm"]], "relations": [[27, 28, 30, 31, "CONJUNCTION"], [53, 54, 50, 51, "USED-FOR"]]},
{"sentence": "Finally , we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources .", "ner": [["techniques", 10, 10, "Generic"], ["word senses", 13, 14, "OtherScientificTerm"], ["online sources", 19, 20, "Material"]], "relations": [[10, 10, 13, 14, "USED-FOR"], [19, 20, 10, 10, "USED-FOR"]]},
{"sentence": "The TIPSTER Architecture has been designed to enable a variety of different text applications to use a set of common text processing modules .", "ner": [["TIPSTER Architecture", 1, 2, "Method"], ["text applications", 12, 13, "Task"], ["common text processing modules", 19, 22, "Method"]], "relations": [[1, 2, 12, 13, "USED-FOR"], [19, 22, 12, 13, "USED-FOR"]]},
{"sentence": "Since user interfaces work best when customized for particular applications , it is appropriator that no particular user interface styles or conventions are described in the TIPSTER Architecture specification .", "ner": [["user interfaces", 1, 2, "OtherScientificTerm"], ["applications", 9, 9, "Generic"], ["user interface styles or conventions", 17, 21, "OtherScientificTerm"], ["TIPSTER Architecture specification", 26, 28, "OtherScientificTerm"]], "relations": [[1, 2, 9, 9, "USED-FOR"]]},
{"sentence": "However , the Computing Research Laboratory -LRB- CRL -RRB- has constructed several TIPSTER applications that use a common set of configurable Graphical User Interface -LRB- GUI -RRB- functions .", "ner": [["TIPSTER applications", 12, 13, "Task"], ["Graphical User Interface -LRB- GUI -RRB- functions", 21, 27, "OtherScientificTerm"]], "relations": [[21, 27, 12, 13, "USED-FOR"]]},
{"sentence": "These GUIs were constructed using CRL 's TIPSTER User Interface Toolkit -LRB- TUIT -RRB- .", "ner": [["GUIs", 1, 1, "OtherScientificTerm"], ["CRL 's TIPSTER User Interface Toolkit -LRB- TUIT -RRB-", 5, 13, "Method"]], "relations": [[5, 13, 1, 1, "USED-FOR"]]},
{"sentence": "TUIT is a software library that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks .", "ner": [["TUIT", 0, 0, "Method"], ["software library", 3, 4, "Method"], ["multilingual TIPSTER user interfaces", 11, 14, "OtherScientificTerm"]], "relations": [[0, 0, 3, 4, "HYPONYM-OF"], [0, 0, 11, 14, "USED-FOR"]]},
{"sentence": "CRL developed TUIT to support their work to integrate TIPSTER modules for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects .", "ner": [["TUIT", 2, 2, "Method"], ["TIPSTER modules", 9, 10, "Method"]], "relations": [[2, 2, 9, 10, "USED-FOR"]]},
{"sentence": "This paper briefly describes TUIT and its capabilities .", "ner": [["TUIT", 4, 4, "Method"]], "relations": []},
{"sentence": "Current natural language interfaces have concentrated largely on determining the literal meaning of input from their users .", "ner": [["natural language interfaces", 1, 3, "OtherScientificTerm"]], "relations": []},
{"sentence": "While such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous non-literal aspects of communication , such as robust communication procedures .", "ner": [["decoding", 2, 2, "Method"], ["natural language interfaces", 13, 15, "OtherScientificTerm"], ["they", 23, 23, "Generic"], ["non-literal aspects of communication", 27, 30, "Method"], ["robust communication procedures", 34, 36, "Method"]], "relations": [[27, 30, 23, 23, "PART-OF"], [34, 36, 27, 30, "HYPONYM-OF"]]},
{"sentence": "This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful personal computers with integral graphics displays offers techniques superior to those of humans for these aspects , while still satisfying human communication needs .", "ner": [["non-literal aspects of communication", 24, 27, "Method"], ["personal computers", 35, 36, "OtherScientificTerm"], ["graphics displays", 39, 40, "OtherScientificTerm"]], "relations": [[39, 40, 35, 36, "PART-OF"]]},
{"sentence": "The paper proposes interfaces based on a judicious mixture of these techniques and the still valuable methods of more traditional natural language interfaces .", "ner": [["natural language interfaces", 20, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper proposes a framework in which Lagrangian Particle Dynamics is used for the segmentation of high density crowd flows and detection of flow instabilities .", "ner": [["Lagrangian Particle Dynamics", 7, 9, "Method"], ["segmentation of high density crowd flows", 14, 19, "Task"], ["detection of flow instabilities", 21, 24, "Task"]], "relations": [[7, 9, 14, 19, "USED-FOR"], [7, 9, 21, 24, "USED-FOR"], [14, 19, 21, 24, "CONJUNCTION"]]},
{"sentence": "For this purpose , a flow field generated by a moving crowd is treated as an aperiodic dynamical system .", "ner": [["flow field", 5, 6, "OtherScientificTerm"], ["moving crowd", 10, 11, "OtherScientificTerm"], ["aperiodic dynamical system", 16, 18, "Method"]], "relations": [[10, 11, 5, 6, "USED-FOR"], [16, 18, 5, 6, "USED-FOR"]]},
{"sentence": "A grid of particles is overlaid on the flow field , and is advected using a numerical integration scheme .", "ner": [["grid of particles", 1, 3, "OtherScientificTerm"], ["flow field", 8, 9, "OtherScientificTerm"], ["numerical integration scheme", 16, 18, "Method"]], "relations": [[1, 3, 8, 9, "USED-FOR"], [16, 18, 1, 3, "USED-FOR"]]},
{"sentence": "The evolution of particles through the flow is tracked using a Flow Map , whose spatial gradients are subsequently used to setup a Cauchy Green Deformation tensor for quantifying the amount by which the neighboring particles have diverged over the length of the integration .", "ner": [["evolution of particles", 1, 3, "OtherScientificTerm"], ["Flow Map", 11, 12, "Method"], ["spatial gradients", 15, 16, "OtherScientificTerm"], ["Cauchy Green Deformation tensor", 23, 26, "Method"]], "relations": [[11, 12, 1, 3, "USED-FOR"], [15, 16, 23, 26, "USED-FOR"]]},
{"sentence": "The maximum eigenvalue of the tensor is used to construct a Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field , which reveals the Lagrangian Coherent Structures -LRB- LCS -RRB- present in the underlying flow .", "ner": [["maximum eigenvalue", 1, 2, "OtherScientificTerm"], ["tensor", 5, 5, "Generic"], ["Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field", 11, 18, "OtherScientificTerm"], ["Lagrangian Coherent Structures -LRB- LCS -RRB-", 23, 28, "OtherScientificTerm"]], "relations": [[1, 2, 5, 5, "FEATURE-OF"], [1, 2, 11, 18, "USED-FOR"], [11, 18, 23, 28, "USED-FOR"]]},
{"sentence": "The LCS divide flow into regions of qualitatively different dynamics and are used to locate boundaries of the flow segments in a normalized cuts framework .", "ner": [["LCS", 1, 1, "OtherScientificTerm"], ["boundaries of the flow segments", 15, 19, "OtherScientificTerm"], ["normalized cuts framework", 22, 24, "Method"]], "relations": [[1, 1, 15, 19, "USED-FOR"], [22, 24, 15, 19, "USED-FOR"]]},
{"sentence": "Any change in the number of flow segments over time is regarded as an instability , which is detected by establishing correspondences between flow segments over time .", "ner": [], "relations": []},
{"sentence": "The experiments are conducted on a challenging set of videos taken from Google Video and a National Geographic documentary .", "ner": [["Google Video", 12, 13, "Material"], ["National Geographic documentary", 16, 18, "Material"]], "relations": [[12, 13, 16, 18, "CONJUNCTION"]]},
{"sentence": "In this paper we study a set of problems that are of considerable importance to Statistical Machine Translation -LRB- SMT -RRB- but which have not been addressed satisfactorily by the SMT research community .", "ner": [["Statistical Machine Translation -LRB- SMT -RRB-", 15, 20, "Task"]], "relations": []},
{"sentence": "Over the last decade , a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental problems of SMT .", "ner": [["SMT algorithms", 8, 9, "Method"], ["computational complexity", 22, 23, "Metric"], ["problems", 29, 29, "Generic"], ["SMT", 31, 31, "Task"]], "relations": [[22, 23, 29, 29, "EVALUATE-FOR"], [29, 29, 31, 31, "PART-OF"]]},
{"sentence": "Our work aims at providing useful insights into the the computational complexity of those problems .", "ner": [["computational complexity", 10, 11, "Metric"], ["problems", 14, 14, "Generic"]], "relations": [[10, 11, 14, 14, "EVALUATE-FOR"]]},
{"sentence": "We prove that while IBM Models 1-2 are conceptually and computationally simple , computations involving the higher -LRB- and more useful -RRB- models are hard .", "ner": [["IBM Models 1-2", 4, 6, "Method"], ["computations", 13, 13, "Generic"], ["models", 22, 22, "Generic"]], "relations": [[4, 6, 22, 22, "COMPARE"]]},
{"sentence": "Since it is unlikely that there exists a polynomial time solution for any of these hard problems -LRB- unless P = NP and P #P = P -RRB- , our results highlight and justify the need for developing polynomial time approximations for these computations .", "ner": [["polynomial time solution", 8, 10, "Method"], ["hard problems", 15, 16, "Generic"], ["polynomial time approximations", 38, 40, "Method"], ["computations", 43, 43, "Generic"]], "relations": [[8, 10, 15, 16, "USED-FOR"], [38, 40, 43, 43, "USED-FOR"]]},
{"sentence": "We also discuss some practical ways of dealing with complexity .", "ner": [["complexity", 9, 9, "Metric"]], "relations": []},
{"sentence": "Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks .", "ner": [["evaluation measures", 2, 3, "Metric"], ["machine translation", 5, 6, "Task"]], "relations": [[2, 3, 5, 6, "EVALUATE-FOR"]]},
{"sentence": "In many cases though such movements still result in correct or almost correct sentences .", "ner": [], "relations": []},
{"sentence": "In this paper , we will present a new evaluation measure which explicitly models block reordering as an edit operation .", "ner": [["evaluation measure", 9, 10, "Metric"], ["block reordering", 14, 15, "OtherScientificTerm"], ["edit operation", 18, 19, "OtherScientificTerm"]], "relations": [[9, 10, 14, 15, "USED-FOR"], [18, 19, 14, 15, "USED-FOR"]]},
{"sentence": "Our measure can be exactly calculated in quadratic time .", "ner": [["measure", 1, 1, "Generic"], ["quadratic time", 7, 8, "OtherScientificTerm"]], "relations": [[7, 8, 1, 1, "FEATURE-OF"]]},
{"sentence": "Furthermore , we will show how some evaluation measures can be improved by the introduction of word-dependent substitution costs .", "ner": [["evaluation measures", 7, 8, "Metric"], ["word-dependent substitution costs", 16, 18, "OtherScientificTerm"]], "relations": [[16, 18, 7, 8, "USED-FOR"]]},
{"sentence": "The correlation of the new measure with human judgment has been investigated systematically on two different language pairs .", "ner": [["measure", 5, 5, "Generic"], ["human judgment", 7, 8, "OtherScientificTerm"], ["language pairs", 16, 17, "Material"]], "relations": [[5, 5, 7, 8, "COMPARE"]]},
{"sentence": "The experimental results will show that it significantly outperforms state-of-the-art approaches in sentence-level correlation .", "ner": [["it", 6, 6, "Generic"], ["approaches", 10, 10, "Generic"], ["sentence-level correlation", 12, 13, "Metric"]], "relations": [[6, 6, 10, 10, "COMPARE"], [12, 13, 6, 6, "EVALUATE-FOR"], [12, 13, 10, 10, "EVALUATE-FOR"]]},
{"sentence": "Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between automatic evaluation measures and human judgment .", "ner": [["word dependent substitution costs", 4, 7, "OtherScientificTerm"], ["automatic evaluation measures", 16, 18, "Metric"], ["human judgment", 20, 21, "OtherScientificTerm"]], "relations": [[16, 18, 20, 21, "CONJUNCTION"]]},
{"sentence": "The Rete and Treat algorithms are considered the most efficient implementation techniques for Forward Chaining rule systems .", "ner": [["Rete and Treat algorithms", 1, 4, "Method"], ["implementation techniques", 10, 11, "Method"], ["Forward Chaining rule systems", 13, 16, "Task"]], "relations": [[1, 4, 10, 11, "HYPONYM-OF"], [1, 4, 13, 16, "USED-FOR"]]},
{"sentence": "These algorithms support a language of limited expressive power .", "ner": [["algorithms", 1, 1, "Generic"], ["language of limited expressive power", 4, 8, "Method"]], "relations": [[1, 1, 4, 8, "USED-FOR"]]},
{"sentence": "Assertions are not allowed to contain variables , making universal quantification impossible to express except as a rule .", "ner": [["Assertions", 0, 0, "OtherScientificTerm"], ["variables", 6, 6, "OtherScientificTerm"], ["universal quantification", 9, 10, "OtherScientificTerm"], ["rule", 17, 17, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper we show how to support full unification in these algorithms .", "ner": [["full unification", 8, 9, "Task"], ["algorithms", 12, 12, "Generic"]], "relations": [[12, 12, 8, 9, "USED-FOR"]]},
{"sentence": "We also show that : Supporting full unification is costly ; Full unification is not used frequently ; A combination of compile time and run time checks can determine when full unification is not needed .", "ner": [["full unification", 6, 7, "Task"], ["Full unification", 11, 12, "Task"], ["compile time", 21, 22, "Metric"], ["run time", 24, 25, "Metric"], ["full unification", 30, 31, "Task"]], "relations": [[21, 22, 24, 25, "CONJUNCTION"], [21, 22, 30, 31, "EVALUATE-FOR"], [24, 25, 30, 31, "EVALUATE-FOR"]]},
{"sentence": "We present data to show that the cost of supporting full unification can be reduced in proportion to the degree that it is n't employed and that for many practical systems this cost is negligible .", "ner": [["full unification", 10, 11, "Task"], ["it", 21, 21, "Generic"]], "relations": []},
{"sentence": "A method for error correction of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs .", "ner": [["method", 1, 1, "Generic"], ["error correction", 3, 4, "Task"], ["ill-formed input", 6, 7, "Material"], ["dialogue patterns", 12, 13, "OtherScientificTerm"], ["patterns", 20, 20, "Generic"]], "relations": [[1, 1, 3, 4, "USED-FOR"], [6, 7, 3, 4, "USED-FOR"]]},
{"sentence": "Error correction is done by strongly biasing parsing toward expected meanings unless clear evidence from the input shows the current sentence is not expected .", "ner": [["Error correction", 0, 1, "Task"], ["parsing", 7, 7, "Task"]], "relations": []},
{"sentence": "A dialogue acquisition and tracking algorithm is presented along with a description of its implementation in a voice interactive system .", "ner": [["dialogue acquisition and tracking algorithm", 1, 5, "Method"], ["voice interactive system", 17, 19, "Method"]], "relations": [[1, 5, 17, 19, "USED-FOR"]]},
{"sentence": "A series of tests are described that show the power of the error correction methodology when stereotypic dialogue occurs .", "ner": [["error correction methodology", 12, 14, "Method"], ["stereotypic dialogue", 16, 17, "Material"]], "relations": [[16, 17, 12, 14, "USED-FOR"]]},
{"sentence": "Traditional linear Fukunaga-Koontz Transform -LRB- FKT -RRB- -LSB- 1 -RSB- is a powerful discriminative subspaces building approach .", "ner": [["linear Fukunaga-Koontz Transform -LRB- FKT -RRB-", 1, 6, "Method"], ["discriminative subspaces building approach", 13, 16, "Method"]], "relations": [[1, 6, 13, 16, "HYPONYM-OF"]]},
{"sentence": "Previous work has successfully extended FKT to be able to deal with small-sample-size .", "ner": [["FKT", 5, 5, "Method"], ["small-sample-size", 12, 12, "OtherScientificTerm"]], "relations": [[5, 5, 12, 12, "USED-FOR"]]},
{"sentence": "In this paper , we extend traditional linear FKT to enable it to work in multi-class problem and also in higher dimensional -LRB- kernel -RRB- subspaces and therefore provide enhanced discrimination ability .", "ner": [["linear FKT", 7, 8, "Method"], ["it", 11, 11, "Generic"], ["multi-class problem", 15, 16, "Task"], ["higher dimensional -LRB- kernel -RRB- subspaces", 20, 25, "OtherScientificTerm"], ["discrimination ability", 30, 31, "OtherScientificTerm"]], "relations": [[7, 8, 11, 11, "USED-FOR"], [11, 11, 15, 16, "USED-FOR"], [11, 11, 20, 25, "USED-FOR"], [11, 11, 30, 31, "FEATURE-OF"], [15, 16, 20, 25, "CONJUNCTION"]]},
{"sentence": "We verify the effectiveness of the proposed Kernel Fukunaga-Koontz Transform by demonstrating its effectiveness in face recognition applications ; however the proposed non-linear generalization can be applied to any other domain specific problems .", "ner": [["Kernel Fukunaga-Koontz Transform", 7, 9, "Method"], ["face recognition applications", 15, 17, "Task"], ["non-linear generalization", 22, 23, "Method"], ["domain specific problems", 30, 32, "Task"]], "relations": [[15, 17, 7, 9, "EVALUATE-FOR"], [22, 23, 30, 32, "USED-FOR"]]},
{"sentence": "The study addresses the problem of automatic acquisition of entailment relations between verbs .", "ner": [["automatic acquisition of entailment relations", 6, 10, "Task"]], "relations": []},
{"sentence": "While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .", "ner": [["task", 2, 2, "Generic"], ["paraphrases acquisition", 8, 9, "Task"], ["semantic equivalence", 14, 15, "OtherScientificTerm"], ["entailment acquisition", 23, 24, "Task"], ["asymmetric , or directional , relations", 28, 33, "OtherScientificTerm"]], "relations": [[2, 2, 8, 9, "COMPARE"], [8, 9, 14, 15, "USED-FOR"], [23, 24, 28, 33, "USED-FOR"]]},
{"sentence": "Motivated by the intuition that it often underlies the local structure of coherent text , we develop a method that discovers verb entailment using evidence about discourse relations between clauses available in a parsed corpus .", "ner": [["local structure of coherent text", 9, 13, "OtherScientificTerm"], ["method", 18, 18, "Generic"], ["verb entailment", 21, 22, "Task"], ["discourse relations", 26, 27, "OtherScientificTerm"], ["parsed corpus", 33, 34, "Material"]], "relations": [[18, 18, 21, 22, "USED-FOR"], [26, 27, 18, 18, "USED-FOR"], [33, 34, 26, 27, "USED-FOR"]]},
{"sentence": "In comparison with earlier work , the proposed method covers a much wider range of verb entailment types and learns the mapping between verbs with highly varied argument structures .", "ner": [["method", 8, 8, "Generic"], ["verb entailment types", 15, 17, "OtherScientificTerm"], ["mapping between verbs", 21, 23, "Task"], ["highly varied argument structures", 25, 28, "OtherScientificTerm"]], "relations": [[8, 8, 21, 23, "USED-FOR"], [25, 28, 21, 23, "FEATURE-OF"]]},
{"sentence": "In this paper , we cast the problem of point cloud matching as a shape matching problem by transforming each of the given point clouds into a shape representation called the Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation .", "ner": [["point cloud matching", 9, 11, "Task"], ["shape matching problem", 14, 16, "Task"], ["point clouds", 23, 24, "OtherScientificTerm"], ["shape representation", 27, 28, "Method"], ["Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation", 31, 37, "Method"]], "relations": [[14, 16, 9, 11, "USED-FOR"], [27, 28, 23, 24, "USED-FOR"], [31, 37, 27, 28, "HYPONYM-OF"]]},
{"sentence": "This is achieved by solving a static Schr\u00f6dinger equation instead of the corresponding static Hamilton-Jacobi equation in this setting .", "ner": [["static Schr\u00f6dinger equation", 6, 8, "OtherScientificTerm"], ["static Hamilton-Jacobi equation", 13, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "The SDT representation is an analytic expression and following the theoretical physics literature , can be normalized to have unit L2 norm-making it a square-root density , which is identified with a point on a unit Hilbert sphere , whose intrinsic geometry is fully known .", "ner": [["SDT representation", 1, 2, "Method"], ["analytic expression", 5, 6, "OtherScientificTerm"], ["theoretical physics literature", 10, 12, "OtherScientificTerm"], ["it", 22, 22, "Generic"], ["square-root density", 24, 25, "OtherScientificTerm"], ["unit Hilbert sphere", 35, 37, "OtherScientificTerm"], ["intrinsic geometry", 40, 41, "OtherScientificTerm"]], "relations": [[1, 2, 5, 6, "HYPONYM-OF"], [24, 25, 22, 22, "USED-FOR"], [40, 41, 35, 37, "FEATURE-OF"]]},
{"sentence": "The Fisher-Rao metric , a natural metric for the space of densities leads to analytic expressions for the geodesic distance between points on this sphere .", "ner": [["Fisher-Rao metric", 1, 2, "Metric"], ["natural metric", 5, 6, "Generic"], ["space of densities", 9, 11, "OtherScientificTerm"], ["analytic expressions", 14, 15, "OtherScientificTerm"], ["geodesic distance", 18, 19, "OtherScientificTerm"], ["sphere", 24, 24, "Generic"]], "relations": [[5, 6, 9, 11, "USED-FOR"], [14, 15, 18, 19, "USED-FOR"]]},
{"sentence": "In this paper , we use the well known Riemannian framework never before used for point cloud matching , and present a novel matching algorithm .", "ner": [["Riemannian framework", 9, 10, "Method"], ["point cloud matching", 15, 17, "Task"], ["matching algorithm", 23, 24, "Method"]], "relations": [[9, 10, 15, 17, "USED-FOR"]]},
{"sentence": "We pose point set matching under rigid and non-rigid transformations in this framework and solve for the transformations using standard nonlinear optimization techniques .", "ner": [["point set matching", 2, 4, "Task"], ["rigid and non-rigid transformations", 6, 9, "OtherScientificTerm"], ["framework", 12, 12, "Generic"], ["transformations", 17, 17, "Generic"], ["nonlinear optimization techniques", 20, 22, "Method"]], "relations": [[6, 9, 2, 4, "USED-FOR"], [12, 12, 2, 4, "USED-FOR"], [20, 22, 17, 17, "USED-FOR"]]},
{"sentence": "Finally , to evaluate the performance of our algorithm-dubbed SDTM-we present several synthetic and real data examples along with extensive comparisons to state-of-the-art techniques .", "ner": [["synthetic and real data examples", 12, 16, "Material"], ["state-of-the-art techniques", 22, 23, "Generic"]], "relations": []},
{"sentence": "The experiments show that our algorithm outperforms state-of-the-art point set registration algorithms on many quantitative metrics .", "ner": [["algorithm", 5, 5, "Generic"], ["point set registration algorithms", 8, 11, "Method"], ["quantitative metrics", 14, 15, "Metric"]], "relations": [[5, 5, 8, 11, "COMPARE"], [14, 15, 5, 5, "EVALUATE-FOR"], [14, 15, 8, 11, "EVALUATE-FOR"]]},
{"sentence": "Using natural language processing , we carried out a trend survey on Japanese natural language processing studies that have been done over the last ten years .", "ner": [["natural language processing", 1, 3, "Method"], ["trend survey", 9, 10, "Task"], ["trend survey on Japanese natural language processing studies", 9, 16, "Task"], ["Japanese natural language processing studies", 12, 16, "Material"]], "relations": [[1, 3, 9, 16, "USED-FOR"]]},
{"sentence": "We determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas .", "ner": [], "relations": []},
{"sentence": "This paper is useful for both recognizing trends in Japanese NLP and constructing a method of supporting trend surveys using NLP .", "ner": [["Japanese NLP", 9, 10, "Material"], ["trend surveys", 17, 18, "Task"], ["NLP", 20, 20, "Method"]], "relations": [[20, 20, 17, 18, "USED-FOR"]]},
{"sentence": "High frequency oscillations -LRB- HFOs -RRB- are a promising biomarker of epileptic brain tissue and activity .", "ner": [["High frequency oscillations -LRB- HFOs -RRB-", 0, 5, "OtherScientificTerm"], ["epileptic brain tissue and activity", 11, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "HFOs additionally serve as a prototypical example of challenges in the analysis of discrete events in high-temporal resolution , intracranial EEG data .", "ner": [["HFOs", 0, 0, "OtherScientificTerm"], ["analysis of discrete events", 11, 14, "Task"], ["high-temporal resolution , intracranial EEG data", 16, 21, "Material"]], "relations": [[16, 21, 11, 14, "USED-FOR"]]},
{"sentence": "Two primary challenges are 1 -RRB- dimensionality reduction , and 2 -RRB- assessing feasibility of classification .", "ner": [["dimensionality reduction", 6, 7, "Task"], ["assessing feasibility of classification", 12, 15, "Task"], ["classification", 15, 15, "Task"]], "relations": []},
{"sentence": "Dimensionality reduction assumes that the data lie on a manifold with dimension less than that of the features space .", "ner": [["Dimensionality reduction", 0, 1, "Task"], ["manifold", 9, 9, "Material"], ["features space", 17, 18, "OtherScientificTerm"]], "relations": []},
{"sentence": "However , previous HFO analysis have assumed a linear manifold , global across time , space -LRB- i.e. recording electrode/channel -RRB- , and individual patients .", "ner": [["HFO analysis", 3, 4, "Task"], ["linear manifold", 8, 9, "OtherScientificTerm"]], "relations": [[8, 9, 3, 4, "USED-FOR"]]},
{"sentence": "Instead , we assess both a -RRB- whether linear methods are appropriate and b -RRB- the consistency of the manifold across time , space , and patients .", "ner": [["linear methods", 8, 9, "Method"], ["manifold", 19, 19, "OtherScientificTerm"]], "relations": []},
{"sentence": "We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of HFOs -LRB- those occurring during seizures and those occurring due to other processes -RRB- .", "ner": [["bounds", 3, 3, "OtherScientificTerm"], ["Bayes classification error", 6, 8, "OtherScientificTerm"], ["HFOs", 17, 17, "OtherScientificTerm"], ["those", 19, 19, "Generic"], ["those", 24, 24, "Generic"]], "relations": [[19, 19, 17, 17, "HYPONYM-OF"], [19, 19, 24, 24, "CONJUNCTION"], [24, 24, 17, 17, "HYPONYM-OF"]]},
{"sentence": "This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other discrete events , such as individual action potentials or multi-unit activity .", "ner": [["clinical use of HFO features", 7, 11, "Task"], ["HFO features", 10, 11, "OtherScientificTerm"], ["discrete events", 18, 19, "OtherScientificTerm"], ["action potentials", 24, 25, "OtherScientificTerm"], ["multi-unit activity", 27, 28, "OtherScientificTerm"]], "relations": [[24, 25, 18, 19, "HYPONYM-OF"], [24, 25, 27, 28, "CONJUNCTION"], [27, 28, 18, 19, "HYPONYM-OF"]]},
{"sentence": "In this paper we present ONTOSCORE , a system for scoring sets of concepts on the basis of an ontology .", "ner": [["ONTOSCORE", 5, 5, "Method"], ["system", 8, 8, "Generic"], ["ontology", 19, 19, "OtherScientificTerm"]], "relations": [[19, 19, 8, 8, "USED-FOR"]]},
{"sentence": "We apply our system to the task of scoring alternative speech recognition hypotheses -LRB- SRH -RRB- in terms of their semantic coherence .", "ner": [["system", 3, 3, "Generic"], ["speech recognition hypotheses -LRB- SRH -RRB-", 10, 15, "OtherScientificTerm"], ["semantic coherence", 20, 21, "OtherScientificTerm"]], "relations": [[3, 3, 10, 15, "USED-FOR"]]},
{"sentence": "We conducted an annotation experiment and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses .", "ner": [["speech recognition hypotheses", 18, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "An evaluation of our system against the annotated data shows that , it successfully classifies 73.2 % in a German corpus of 2.284 SRHs as either coherent or incoherent -LRB- given a baseline of 54.55 % -RRB- .", "ner": [["system", 4, 4, "Generic"], ["it", 12, 12, "Generic"], ["German corpus", 19, 20, "Material"], ["SRHs", 23, 23, "OtherScientificTerm"]], "relations": []},
{"sentence": "We propose an efficient dialogue management for an information navigation system based on a document knowledge base .", "ner": [["dialogue management", 4, 5, "Method"], ["information navigation system", 8, 10, "Task"], ["document knowledge base", 14, 16, "Material"]], "relations": [[4, 5, 8, 10, "USED-FOR"], [14, 16, 8, 10, "USED-FOR"]]},
{"sentence": "It is expected that incorporation of appropriate N-best candidates of ASR and contextual information will improve the system performance .", "ner": [["N-best candidates of ASR", 7, 10, "OtherScientificTerm"], ["contextual information", 12, 13, "OtherScientificTerm"], ["system", 17, 17, "Generic"]], "relations": [[7, 10, 12, 13, "CONJUNCTION"], [7, 10, 17, 17, "USED-FOR"], [12, 13, 17, 17, "USED-FOR"]]},
{"sentence": "The system also has several choices in generating responses or confirmations .", "ner": [["system", 1, 1, "Generic"], ["generating responses or confirmations", 7, 10, "Task"]], "relations": [[1, 1, 7, 10, "USED-FOR"]]},
{"sentence": "In this paper , this selection is optimized as minimization of Bayes risk based on reward for correct information presentation and penalty for redundant turns .", "ner": [["minimization of Bayes risk", 9, 12, "Task"], ["reward", 15, 15, "Metric"], ["correct information presentation", 17, 19, "OtherScientificTerm"], ["penalty", 21, 21, "Metric"], ["redundant turns", 23, 24, "OtherScientificTerm"]], "relations": [[15, 15, 9, 12, "USED-FOR"], [15, 15, 17, 19, "USED-FOR"], [15, 15, 21, 21, "CONJUNCTION"], [21, 21, 9, 12, "USED-FOR"], [21, 21, 23, 24, "USED-FOR"]]},
{"sentence": "We have evaluated this strategy with our spoken dialogue system '' Dialogue Navigator for Kyoto City '' , which also has question-answering capability .", "ner": [["strategy", 4, 4, "Generic"], ["spoken dialogue system '' Dialogue Navigator for Kyoto City ''", 7, 16, "Task"], ["question-answering capability", 21, 22, "OtherScientificTerm"]], "relations": [[7, 16, 4, 4, "EVALUATE-FOR"], [21, 22, 7, 16, "FEATURE-OF"]]},
{"sentence": "Effectiveness of the proposed framework was confirmed in the success rate of retrieval and the average number of turns for information access .", "ner": [["framework", 4, 4, "Generic"], ["success rate of retrieval", 9, 12, "Metric"], ["average number of turns", 15, 18, "Metric"], ["information access", 20, 21, "OtherScientificTerm"]], "relations": [[9, 12, 4, 4, "EVALUATE-FOR"], [9, 12, 15, 18, "CONJUNCTION"], [15, 18, 4, 4, "EVALUATE-FOR"], [15, 18, 20, 21, "USED-FOR"]]},
{"sentence": "Reducing language model -LRB- LM -RRB- size is a critical issue when applying a LM to realistic applications which have memory constraints .", "ner": [["Reducing language model -LRB- LM -RRB- size", 0, 6, "Task"], ["LM", 14, 14, "Method"], ["memory constraints .", 20, 22, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper , three measures are studied for the purpose of LM pruning .", "ner": [["LM pruning", 12, 13, "Task"]], "relations": []},
{"sentence": "They are probability , rank , and entropy .", "ner": [["rank", 4, 4, "OtherScientificTerm"], ["entropy", 7, 7, "OtherScientificTerm"]], "relations": [[4, 4, 7, 7, "CONJUNCTION"]]},
{"sentence": "We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate -LRB- CER -RRB- .", "ner": [["pruning criteria", 7, 8, "OtherScientificTerm"], ["Chinese text input", 14, 16, "Material"], ["character error rate -LRB- CER -RRB-", 20, 25, "Metric"]], "relations": [[7, 8, 14, 16, "USED-FOR"], [20, 25, 7, 8, "EVALUATE-FOR"]]},
{"sentence": "We first present an empirical comparison , showing that rank performs the best in most cases .", "ner": [["rank", 9, 9, "OtherScientificTerm"]], "relations": []},
{"sentence": "We also show that the high-performance of rank lies in its strong correlation with error rate .", "ner": [["rank", 7, 7, "OtherScientificTerm"], ["error rate", 14, 15, "Metric"]], "relations": [[14, 15, 7, 7, "EVALUATE-FOR"]]},
{"sentence": "We then present a novel method of combining two criteria in model pruning .", "ner": [["method", 5, 5, "Generic"], ["model pruning", 11, 12, "Task"]], "relations": [[5, 5, 11, 12, "USED-FOR"]]},
{"sentence": "Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately , at the same CER .", "ner": [["CER", 26, 26, "Metric"]], "relations": []},
{"sentence": "This paper proposes an annotating scheme that encodes honorifics -LRB- respectful words -RRB- .", "ner": [["annotating scheme", 4, 5, "Method"], ["honorifics", 8, 8, "Material"], ["respectful words", 10, 11, "OtherScientificTerm"]], "relations": [[4, 5, 8, 8, "USED-FOR"], [8, 8, 10, 11, "HYPONYM-OF"]]},
{"sentence": "Honorifics are used extensively in Japanese , reflecting the social relationship -LRB- e.g. social ranks and age -RRB- of the referents .", "ner": [["Honorifics", 0, 0, "Material"], ["Japanese", 5, 5, "Material"]], "relations": [[0, 0, 5, 5, "USED-FOR"]]},
{"sentence": "This referential information is vital for resolving zero pronouns and improving machine translation outputs .", "ner": [["referential information", 1, 2, "OtherScientificTerm"], ["zero pronouns", 7, 8, "OtherScientificTerm"], ["machine translation outputs", 11, 13, "OtherScientificTerm"]], "relations": [[1, 2, 7, 8, "USED-FOR"], [1, 2, 11, 13, "USED-FOR"]]},
{"sentence": "Annotating honorifics is a complex task that involves identifying a predicate with honorifics , assigning ranks to referents of the predicate , calibrating the ranks , and connecting referents with their predicates .", "ner": [["honorifics", 1, 1, "Material"], ["honorifics", 12, 12, "Material"], ["ranks", 15, 15, "OtherScientificTerm"], ["ranks", 24, 24, "OtherScientificTerm"]], "relations": []},
{"sentence": "Visually-guided arm reaching movements are produced by distributed neural networks within parietal and frontal regions of the cerebral cortex .", "ner": [["Visually-guided arm reaching movements", 0, 3, "OtherScientificTerm"], ["distributed neural networks", 7, 9, "Method"]], "relations": [[7, 9, 0, 3, "USED-FOR"]]},
{"sentence": "Experimental data indicate that -LRB- I -RRB- single neurons in these regions are broadly tuned to parameters of movement ; -LRB- 2 -RRB- appropriate commands are elaborated by populations of neurons ; -LRB- 3 -RRB- the coordinated action of neu-rons can be visualized using a neuronal population vector -LRB- NPV -RRB- .", "ner": [["coordinated action of neu-rons", 36, 39, "OtherScientificTerm"], ["neuronal population vector -LRB- NPV -RRB-", 45, 50, "OtherScientificTerm"]], "relations": [[45, 50, 36, 39, "USED-FOR"]]},
{"sentence": "However , the NPV provides only a rough estimate of movement parameters -LRB- direction , velocity -RRB- and may even fail to reflect the parameters of movement when arm posture is changed .", "ner": [["NPV", 3, 3, "OtherScientificTerm"], ["arm posture", 28, 29, "OtherScientificTerm"]], "relations": []},
{"sentence": "We designed a model of the cortical motor command to investigate the relation between the desired direction of the movement , the actual direction of movement and the direction of the NPV in motor cortex .", "ner": [["model", 3, 3, "Generic"], ["cortical motor command", 6, 8, "OtherScientificTerm"], ["NPV", 31, 31, "OtherScientificTerm"], ["motor cortex", 33, 34, "OtherScientificTerm"]], "relations": [[3, 3, 6, 8, "USED-FOR"], [31, 31, 33, 34, "USED-FOR"]]},
{"sentence": "The model is a two-layer self-organizing neural network which combines broadly-tuned -LRB- muscular -RRB- proprioceptive and -LRB- cartesian -RRB- visual information to calculate -LRB- angular -RRB- motor commands for the initial part of the movement of a two-link arm .", "ner": [["model", 1, 1, "Generic"], ["two-layer self-organizing neural network", 4, 7, "Method"], ["broadly-tuned -LRB- muscular -RRB- proprioceptive", 10, 14, "OtherScientificTerm"], ["-LRB- cartesian -RRB- visual information", 16, 20, "OtherScientificTerm"], ["-LRB- angular -RRB- motor commands", 23, 27, "OtherScientificTerm"], ["two-link arm", 37, 38, "OtherScientificTerm"]], "relations": [[4, 7, 23, 27, "USED-FOR"], [10, 14, 4, 7, "USED-FOR"], [10, 14, 16, 20, "CONJUNCTION"], [16, 20, 4, 7, "USED-FOR"]]},
{"sentence": "The network was trained by motor babbling in 5 positions .", "ner": [["network", 1, 1, "Generic"]], "relations": []},
{"sentence": "Simulations showed that -LRB- 1 -RRB- the network produced appropriate movement direction over a large part of the workspace ; -LRB- 2 -RRB- small deviations of the actual trajectory from the desired trajectory existed at the extremities of the workspace ; -LRB- 3 -RRB- these deviations were accompanied by large deviations of the NPV from both trajectories .", "ner": [["network", 7, 7, "Generic"], ["NPV", 53, 53, "OtherScientificTerm"]], "relations": []},
{"sentence": "These results suggest the NPV does not give a faithful image of cortical processing during arm reaching movements .", "ner": [["NPV", 4, 4, "OtherScientificTerm"], ["image of cortical processing", 10, 13, "OtherScientificTerm"], ["arm reaching movements", 15, 17, "OtherScientificTerm"]], "relations": [[15, 17, 10, 13, "FEATURE-OF"]]},
{"sentence": "It is well-known that diversity among base classifiers is crucial for constructing a strong ensemble .", "ner": [["base classifiers", 6, 7, "Method"], ["ensemble", 14, 14, "Task"]], "relations": [[6, 7, 14, 14, "USED-FOR"]]},
{"sentence": "Most existing ensemble methods obtain diverse individual learners through resampling the instances or features .", "ner": [["ensemble methods", 2, 3, "Method"]], "relations": []},
{"sentence": "In this paper , we propose an alternative way for ensemble construction by resampling pairwise constraints that specify whether a pair of instances belongs to the same class or not .", "ner": [["ensemble construction", 10, 11, "Task"], ["resampling pairwise constraints", 13, 15, "Task"]], "relations": [[13, 15, 10, 11, "USED-FOR"]]},
{"sentence": "Using pairwise constraints for ensemble construction is challenging because it remains unknown how to influence the base classifiers with the sampled pairwise constraints .", "ner": [["pairwise constraints", 1, 2, "OtherScientificTerm"], ["ensemble construction", 4, 5, "Task"], ["base classifiers", 16, 17, "Method"], ["sampled pairwise constraints", 20, 22, "OtherScientificTerm"]], "relations": [[1, 2, 4, 5, "USED-FOR"]]},
{"sentence": "We solve this problem with a two-step process .", "ner": [], "relations": []},
{"sentence": "First , we transform the original instances into a new data representation using projections learnt from pairwise constraints .", "ner": [["data representation", 10, 11, "Method"], ["projections", 13, 13, "OtherScientificTerm"], ["pairwise constraints", 16, 17, "OtherScientificTerm"]], "relations": [[13, 13, 10, 11, "USED-FOR"], [16, 17, 13, 13, "USED-FOR"]]},
{"sentence": "Then , we build the base clas-sifiers with the new data representation .", "ner": [["base clas-sifiers", 5, 6, "Method"], ["data representation", 10, 11, "Method"]], "relations": [[10, 11, 5, 6, "USED-FOR"]]},
{"sentence": "We propose two methods for resampling pairwise constraints following the standard Bagging and Boosting algorithms , respectively .", "ner": [["resampling pairwise constraints", 5, 7, "Task"], ["Bagging and Boosting algorithms", 11, 14, "Method"]], "relations": [[11, 14, 5, 7, "USED-FOR"]]},
{"sentence": "Extensive experiments validate the effectiveness of our method .", "ner": [], "relations": []},
{"sentence": "A new algorithm for solving the three dimensional container packing problem is proposed in this paper .", "ner": [["algorithm", 2, 2, "Generic"], ["dimensional container packing problem", 7, 10, "Task"]], "relations": [[2, 2, 7, 10, "USED-FOR"]]},
{"sentence": "This new algorithm deviates from the traditional approach of wall building and layering .", "ner": [["algorithm", 2, 2, "Generic"], ["approach of wall building and layering", 7, 12, "Method"]], "relations": [[2, 2, 7, 12, "COMPARE"]]},
{"sentence": "It uses the concept of '' building growing '' from multiple sides of the container .", "ner": [["It", 0, 0, "Generic"]], "relations": []},
{"sentence": "We tested our method using all 760 test cases from the OR-Library .", "ner": [["method", 3, 3, "Generic"], ["OR-Library", 11, 11, "Material"]], "relations": [[11, 11, 3, 3, "EVALUATE-FOR"]]},
{"sentence": "Experimental results indicate that the new algorithm is able to achieve an average packing utilization of more than 87 % .", "ner": [["algorithm", 6, 6, "Generic"], ["average packing utilization", 12, 14, "Metric"]], "relations": [[12, 14, 6, 6, "EVALUATE-FOR"]]},
{"sentence": "This is better than the results reported in the literature .", "ner": [], "relations": []},
{"sentence": "Current approaches to object category recognition require datasets of training images to be manually prepared , with varying degrees of supervision .", "ner": [["approaches", 1, 1, "Generic"], ["object category recognition", 3, 5, "Task"], ["datasets", 7, 7, "Generic"], ["supervision", 20, 20, "OtherScientificTerm"]], "relations": [[1, 1, 3, 5, "USED-FOR"], [7, 7, 1, 1, "USED-FOR"]]},
{"sentence": "We present an approach that can learn an object category from just its name , by utilizing the raw output of image search engines available on the Internet .", "ner": [["approach", 3, 3, "Generic"], ["object category", 8, 9, "OtherScientificTerm"], ["image search engines", 21, 23, "Method"]], "relations": [[3, 3, 8, 9, "USED-FOR"]]},
{"sentence": "We develop a new model , TSI-pLSA , which extends pLSA -LRB- as applied to visual words -RRB- to include spatial information in a translation and scale invariant manner .", "ner": [["model", 4, 4, "Generic"], ["TSI-pLSA", 6, 6, "Method"], ["pLSA", 10, 10, "Method"], ["visual words", 15, 16, "OtherScientificTerm"], ["spatial information", 20, 21, "OtherScientificTerm"]], "relations": [[10, 10, 6, 6, "USED-FOR"], [10, 10, 15, 16, "USED-FOR"], [20, 21, 6, 6, "PART-OF"]]},
{"sentence": "Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines .", "ner": [["approach", 1, 1, "Generic"], ["intra-class variability", 6, 7, "OtherScientificTerm"], ["unrelated images", 12, 13, "OtherScientificTerm"], ["search engines", 16, 17, "Method"]], "relations": [[1, 1, 6, 7, "USED-FOR"], [1, 1, 12, 13, "USED-FOR"], [6, 7, 12, 13, "CONJUNCTION"], [16, 17, 12, 13, "USED-FOR"]]},
{"sentence": "We evaluate the models on standard test sets , showing performance competitive with existing methods trained on hand prepared datasets .", "ner": [["models", 3, 3, "Generic"], ["test sets", 6, 7, "Generic"], ["methods", 14, 14, "Generic"], ["hand prepared datasets", 17, 19, "Material"]], "relations": [[6, 7, 3, 3, "EVALUATE-FOR"], [6, 7, 14, 14, "EVALUATE-FOR"], [14, 14, 3, 3, "COMPARE"], [17, 19, 14, 14, "USED-FOR"]]},
{"sentence": "The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of Human-Machine Communication , including Natural Language Processing , Non Verbal and Multimodal Communication .", "ner": [["speech processing", 15, 16, "Task"], ["Human-Machine Communication", 25, 26, "Task"], ["Natural Language Processing", 29, 31, "Task"], ["Non Verbal and Multimodal Communication", 33, 37, "Task"]], "relations": [[15, 16, 25, 26, "CONJUNCTION"], [29, 31, 25, 26, "HYPONYM-OF"], [29, 31, 33, 37, "CONJUNCTION"], [33, 37, 25, 26, "HYPONYM-OF"]]},
{"sentence": "Also presented are the commercial applications of some of the research projects .", "ner": [], "relations": []},
{"sentence": "When applicable , the discussion is placed in the framework of international collaborations .", "ner": [], "relations": []},
{"sentence": "We have calculated analytical expressions for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing Markov chains using lookup table representations .", "ner": [["analytical expressions", 3, 4, "OtherScientificTerm"], ["temporal difference value estimation algorithms", 17, 21, "Method"], ["Markov chains", 30, 31, "OtherScientificTerm"], ["lookup table representations", 33, 35, "Method"]], "relations": [[33, 35, 3, 4, "USED-FOR"]]},
{"sentence": "We illustrate classes of learning curve behavior in various chains , and show the manner in which TD is sensitive to the choice of its step-size and eligibility trace parameters .", "ner": [["learning curve behavior", 4, 6, "OtherScientificTerm"], ["step-size and eligibility trace parameters", 25, 29, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper , we describe the pronominal anaphora resolution module of Lucy , a portable English understanding system .", "ner": [["pronominal anaphora resolution module", 7, 10, "Method"], ["Lucy", 12, 12, "Method"], ["English understanding system", 16, 18, "Method"]], "relations": [[7, 10, 12, 12, "PART-OF"], [12, 12, 16, 18, "HYPONYM-OF"]]},
{"sentence": "The design of this module was motivated by the observation that , although there exist many theories of anaphora resolution , no one of these theories is complete .", "ner": [["module", 4, 4, "Generic"], ["anaphora resolution", 18, 19, "Task"]], "relations": []},
{"sentence": "Thus we have implemented a blackboard-like architecture in which individual partial theories can be encoded as separate modules that can interact to propose candidate antecedents and to evaluate each other 's proposals .", "ner": [["blackboard-like architecture", 5, 6, "Method"]], "relations": []},
{"sentence": "In this paper , we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames -LRB- SCFs -RRB- from general and domain corpora .", "ner": [["unsupervised automatic acquisition of Italian and English verb subcategorization frames -LRB- SCFs -RRB-", 8, 20, "Task"], ["Italian and English verb subcategorization frames -LRB- SCFs -RRB-", 12, 20, "OtherScientificTerm"], ["general and domain corpora", 22, 25, "Material"]], "relations": [[22, 25, 8, 20, "USED-FOR"]]},
{"sentence": "The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs .", "ner": [["technique", 2, 2, "Generic"], ["syntactically shallow-parsed corpora", 5, 7, "Material"], ["search heuristics", 16, 17, "Method"], ["lexico-syntactic knowledge", 23, 24, "OtherScientificTerm"], ["SCFs", 26, 26, "OtherScientificTerm"]], "relations": [[5, 7, 2, 2, "USED-FOR"], [16, 17, 2, 2, "USED-FOR"], [26, 26, 23, 24, "FEATURE-OF"]]},
{"sentence": "Although preliminary , reported results are in line with state-of-the-art lexical acquisition systems .", "ner": [["lexical acquisition systems", 10, 12, "Method"]], "relations": []},
{"sentence": "The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle -LRB- MDL -RRB- .", "ner": [["SCFs distributions", 7, 8, "OtherScientificTerm"], ["similar semantic properties", 12, 14, "OtherScientificTerm"], ["Minimum Description Length Principle -LRB- MDL -RRB-", 32, 38, "Method"]], "relations": []},
{"sentence": "First experiments in this direction were carried out on Italian verbs with encouraging results .", "ner": [], "relations": []},
{"sentence": "Graph-cuts optimization is prevalent in vision and graphics problems .", "ner": [["Graph-cuts optimization", 0, 1, "Method"], ["vision and graphics problems", 5, 8, "Task"]], "relations": [[0, 1, 5, 8, "USED-FOR"]]},
{"sentence": "It is thus of great practical importance to parallelize the graph-cuts optimization using to-day 's ubiquitous multi-core machines .", "ner": [["graph-cuts optimization", 10, 11, "Method"], ["multi-core machines", 16, 17, "OtherScientificTerm"]], "relations": [[16, 17, 10, 11, "USED-FOR"]]},
{"sentence": "However , the current best serial algorithm by Boykov and Kolmogorov -LSB- 4 -RSB- -LRB- called the BK algorithm -RRB- still has the superior empirical performance .", "ner": [["serial algorithm", 5, 6, "Method"], ["BK algorithm", 17, 18, "Method"]], "relations": [[17, 18, 5, 6, "HYPONYM-OF"]]},
{"sentence": "It is non-trivial to parallelize as expensive synchronization overhead easily offsets the advantage of parallelism .", "ner": [["synchronization overhead", 7, 8, "OtherScientificTerm"], ["parallelism", 14, 14, "OtherScientificTerm"]], "relations": []},
{"sentence": "In this paper , we propose a novel adaptive bottom-up approach to parallelize the BK algorithm .", "ner": [["adaptive bottom-up approach", 8, 10, "Method"], ["BK algorithm", 14, 15, "Method"]], "relations": [[8, 10, 14, 15, "USED-FOR"]]},
{"sentence": "We first uniformly partition the graph into a number of regularly-shaped dis-joint subgraphs and process them in parallel , then we incre-mentally merge the subgraphs in an adaptive way to obtain the global optimum .", "ner": [["graph", 5, 5, "OtherScientificTerm"], ["regularly-shaped dis-joint subgraphs", 10, 12, "OtherScientificTerm"], ["them", 15, 15, "Generic"], ["subgraphs", 24, 24, "OtherScientificTerm"], ["global optimum", 32, 33, "OtherScientificTerm"]], "relations": []},
{"sentence": "The new algorithm has three benefits : 1 -RRB- it is more cache-friendly within smaller subgraphs ; 2 -RRB- it keeps balanced workloads among computing cores ; 3 -RRB- it causes little overhead and is adaptable to the number of available cores .", "ner": [["algorithm", 2, 2, "Generic"], ["subgraphs", 15, 15, "OtherScientificTerm"], ["balanced workloads", 21, 22, "OtherScientificTerm"], ["overhead", 32, 32, "OtherScientificTerm"]], "relations": []},
{"sentence": "Extensive experiments in common applications such as 2D/3D image segmentations and 3D surface fitting demonstrate the effectiveness of our approach .", "ner": [["applications", 4, 4, "Generic"], ["2D/3D image segmentations", 7, 9, "Task"], ["3D surface fitting", 11, 13, "Task"], ["approach", 19, 19, "Generic"]], "relations": [[4, 4, 19, 19, "EVALUATE-FOR"], [7, 9, 4, 4, "HYPONYM-OF"], [7, 9, 11, 13, "CONJUNCTION"], [11, 13, 4, 4, "HYPONYM-OF"]]},
{"sentence": "We study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the Intersection-over-Union -LRB- IoU -RRB- measure that is used widely in evaluating image segmentation systems .", "ner": [["loss-aware predictions", 8, 9, "Task"], ["loss-aware predictions in image segmentation settings", 8, 13, "Task"], ["image segmentation settings", 11, 13, "Task"], ["evaluation function", 16, 17, "OtherScientificTerm"], ["Intersection-over-Union -LRB- IoU -RRB- measure", 20, 24, "Metric"], ["image segmentation systems", 31, 33, "Method"]], "relations": [[20, 24, 16, 17, "HYPONYM-OF"], [20, 24, 31, 33, "EVALUATE-FOR"]]},
{"sentence": "Currently , there are two dominant approaches : the first approximates the Expected-IoU -LRB- EIoU -RRB- score as Expected-Intersection-over-Expected-Union -LRB- EIoEU -RRB- ; and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions .", "ner": [["dominant approaches", 5, 6, "Generic"], ["first", 9, 9, "Generic"], ["Expected-IoU -LRB- EIoU -RRB- score", 12, 16, "OtherScientificTerm"], ["Expected-Intersection-over-Expected-Union -LRB- EIoEU -RRB-", 18, 21, "OtherScientificTerm"], ["second approach", 25, 26, "Generic"], ["EIoU", 31, 31, "OtherScientificTerm"]], "relations": [[9, 9, 5, 6, "HYPONYM-OF"], [25, 26, 5, 6, "HYPONYM-OF"]]},
{"sentence": "We begin by asking which approach we should favor for two typical image seg-mentation tasks .", "ner": [["image seg-mentation tasks", 12, 14, "Task"]], "relations": []},
{"sentence": "Studying this question leads to two new methods that draw ideas from both existing approaches .", "ner": [], "relations": []},
{"sentence": "Our new methods use the EIoEU approximation paired with high quality candidate solutions .", "ner": [["methods", 2, 2, "Generic"], ["EIoEU", 5, 5, "OtherScientificTerm"], ["EIoEU approximation", 5, 6, "Method"]], "relations": [[5, 6, 2, 2, "USED-FOR"]]},
{"sentence": "Experimentally we show that our new approaches lead to improved performance on both image segmentation tasks .", "ner": [["approaches", 6, 6, "Generic"], ["image segmentation tasks", 13, 15, "Task"]], "relations": [[13, 15, 6, 6, "EVALUATE-FOR"]]},
{"sentence": "Boosting methods are known not to usually overfit training data even as the size of the generated classifiers becomes large .", "ner": [["Boosting methods", 0, 1, "Method"], ["classifiers", 17, 17, "Method"]], "relations": []},
{"sentence": "Schapire et al. attempted to explain this phenomenon in terms of the margins the classifier achieves on training examples .", "ner": [["classifier", 14, 14, "Method"]], "relations": []},
{"sentence": "Later , however , Breiman cast serious doubt on this explanation by introducing a boosting algorithm , arc-gv , that can generate a higher margins distribution than AdaBoost and yet performs worse .", "ner": [["boosting algorithm", 14, 15, "Method"], ["arc-gv", 17, 17, "Method"], ["margins distribution", 24, 25, "OtherScientificTerm"], ["AdaBoost", 27, 27, "Method"]], "relations": [[17, 17, 14, 15, "HYPONYM-OF"], [17, 17, 24, 25, "USED-FOR"], [17, 17, 27, 27, "COMPARE"]]},
{"sentence": "In this paper , we take a close look at Breiman 's compelling but puzzling results .", "ner": [], "relations": []},
{"sentence": "Although we can reproduce his main finding , we find that the poorer performance of arc-gv can be explained by the increased complexity of the base classifiers it uses , an explanation supported by our experiments and entirely consistent with the margins theory .", "ner": [["arc-gv", 15, 15, "Method"], ["complexity", 22, 22, "Metric"], ["base classifiers", 25, 26, "Method"], ["margins theory", 41, 42, "Method"]], "relations": [[22, 22, 25, 26, "EVALUATE-FOR"], [25, 26, 15, 15, "HYPONYM-OF"]]},
{"sentence": "Thus , we find maximizing the margins is desirable , but not necessarily at the expense of other factors , especially base-classifier complexity .", "ner": [["base-classifier complexity", 21, 22, "Metric"]], "relations": []},
{"sentence": "The transfer phase in machine translation -LRB- MT -RRB- systems has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .", "ner": [["transfer phase", 1, 2, "Task"], ["machine translation -LRB- MT -RRB- systems", 4, 9, "Task"], ["analysis", 18, 18, "Task"], ["generation", 20, 20, "Task"], ["it", 23, 23, "Generic"], ["lexical rules", 30, 31, "OtherScientificTerm"]], "relations": [[1, 2, 4, 9, "PART-OF"], [1, 2, 18, 18, "COMPARE"], [1, 2, 20, 20, "COMPARE"], [18, 18, 20, 20, "CONJUNCTION"]]},
{"sentence": "Currently some attempts are being made to use case-based reasoning in machine translation , that is , to make decisions on the basis of translation examples at appropriate pints in MT .", "ner": [["case-based reasoning", 8, 9, "Method"], ["machine translation", 11, 12, "Task"], ["MT", 30, 30, "Task"]], "relations": [[8, 9, 11, 12, "USED-FOR"]]},
{"sentence": "This paper proposes a new type of transfer system , called a Similarity-driven Transfer System -LRB- SimTran -RRB- , for use in such case-based MT -LRB- CBMT -RRB- .", "ner": [["transfer system", 7, 8, "Method"], ["Similarity-driven Transfer System -LRB- SimTran -RRB-", 12, 17, "Method"], ["case-based MT -LRB- CBMT -RRB-", 23, 27, "Task"]], "relations": [[12, 17, 7, 8, "HYPONYM-OF"], [12, 17, 23, 27, "USED-FOR"]]},
{"sentence": "This paper addresses the problem of optimal alignment of non-rigid surfaces from multi-view video observations to obtain a temporally consistent representation .", "ner": [["optimal alignment of non-rigid surfaces", 6, 10, "Task"], ["multi-view video observations", 12, 14, "OtherScientificTerm"], ["temporally consistent representation", 18, 20, "Method"]], "relations": [[6, 10, 18, 20, "USED-FOR"], [12, 14, 6, 10, "USED-FOR"]]},
{"sentence": "Conventional non-rigid surface tracking performs frame-to-frame alignment which is subject to the accumulation of errors resulting in a drift over time .", "ner": [["non-rigid surface tracking", 1, 3, "Method"], ["frame-to-frame alignment", 5, 6, "Method"]], "relations": [[5, 6, 1, 3, "USED-FOR"]]},
{"sentence": "Recently , non-sequential tracking approaches have been introduced which reorder the input data based on a dissimilarity measure .", "ner": [["non-sequential tracking approaches", 2, 4, "Method"], ["dissimilarity measure", 16, 17, "Metric"]], "relations": [[16, 17, 2, 4, "USED-FOR"]]},
{"sentence": "One or more input sequences are represented in a tree with reduced alignment path length .", "ner": [], "relations": []},
{"sentence": "They demonstrate a reduced drift and increased robustness to large non-rigid deformations .", "ner": [["reduced drift", 3, 4, "OtherScientificTerm"], ["robustness", 7, 7, "Metric"], ["non-rigid deformations", 10, 11, "OtherScientificTerm"]], "relations": [[7, 7, 10, 11, "FEATURE-OF"]]},
{"sentence": "However , jumps may occur in the aligned mesh sequence where branches of the tree meet due to different error accumulation along them .", "ner": [["branches", 11, 11, "OtherScientificTerm"], ["tree", 14, 14, "OtherScientificTerm"], ["error accumulation", 19, 20, "OtherScientificTerm"]], "relations": []},
{"sentence": "Optimisation of the tree for non-sequential tracking , which minimises the errors in temporal consistency due to both the drift and the jumps , is proposed .", "ner": [["Optimisation of the tree", 0, 3, "Task"], ["non-sequential tracking", 5, 6, "Method"], ["temporal consistency", 13, 14, "Metric"]], "relations": [[0, 3, 5, 6, "USED-FOR"], [13, 14, 0, 3, "EVALUATE-FOR"]]},
{"sentence": "A novel cluster tree enforces sequential tracking in local segments of the sequence while allowing global non-sequential traversal among these segments .", "ner": [["cluster tree", 2, 3, "Method"], ["sequential tracking in local segments", 5, 9, "Task"], ["local segments", 8, 9, "OtherScientificTerm"], ["global non-sequential traversal", 15, 17, "OtherScientificTerm"], ["segments", 20, 20, "Generic"]], "relations": [[2, 3, 5, 9, "USED-FOR"], [2, 3, 15, 17, "USED-FOR"]]},
{"sentence": "This provides a mechanism to create a tree structure which reduces the number and size of jumps between branches and limits the lenght of branches as well .", "ner": [["tree structure", 7, 8, "OtherScientificTerm"]], "relations": []},
{"sentence": "Comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including face , cloth and people .", "ner": [["non-rigid surfaces", 9, 10, "OtherScientificTerm"], ["face", 12, 12, "OtherScientificTerm"], ["cloth", 14, 14, "OtherScientificTerm"], ["people", 16, 16, "OtherScientificTerm"]], "relations": [[12, 12, 9, 10, "HYPONYM-OF"], [12, 12, 14, 14, "CONJUNCTION"], [14, 14, 9, 10, "HYPONYM-OF"], [14, 14, 16, 16, "CONJUNCTION"], [16, 16, 9, 10, "HYPONYM-OF"]]},
{"sentence": "It demonstrates that the proposed cluster tree achieves better temporal consistency than the previous sequential and non-sequential tracking approaches .", "ner": [["cluster tree", 5, 6, "Method"], ["temporal consistency", 9, 10, "Metric"], ["sequential and non-sequential tracking approaches", 14, 18, "Method"]], "relations": [[5, 6, 14, 18, "COMPARE"], [9, 10, 5, 6, "EVALUATE-FOR"]]},
{"sentence": "Quantitative analysis on a created synthetic facial performance also shows an improvement by the cluster tree .", "ner": [["synthetic facial performance", 5, 7, "Metric"], ["cluster tree", 14, 15, "Method"]], "relations": [[5, 7, 14, 15, "EVALUATE-FOR"]]},
{"sentence": "The translation of English text into American Sign Language -LRB- ASL -RRB- animation tests the limits of traditional MT architectural designs .", "ner": [["translation of English text into American Sign Language -LRB- ASL -RRB- animation", 1, 12, "Task"], ["MT architectural designs", 18, 20, "Method"]], "relations": [[18, 20, 1, 12, "USED-FOR"]]},
{"sentence": "A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called '' classifier predicates . ''", "ner": [["semantic representation", 2, 3, "Method"], ["virtual reality 3D scene modeling software", 8, 13, "Method"], ["spatially complex ASL phenomena", 16, 19, "OtherScientificTerm"], ["classifier predicates", 22, 23, "OtherScientificTerm"]], "relations": [[2, 3, 16, 19, "USED-FOR"], [8, 13, 2, 3, "USED-FOR"], [22, 23, 16, 19, "HYPONYM-OF"]]},
{"sentence": "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and direct approaches into a single system .", "ner": [["model", 1, 1, "Generic"], ["interlingua", 5, 5, "OtherScientificTerm"], ["multi-pathway MT architecture design", 9, 12, "Method"], ["transfer", 16, 16, "OtherScientificTerm"], ["direct approaches", 18, 19, "Generic"], ["system", 23, 23, "Generic"]], "relations": [[16, 16, 18, 19, "CONJUNCTION"], [16, 16, 23, 23, "PART-OF"], [18, 19, 23, 23, "PART-OF"]]},
{"sentence": "An extension to the GPSG grammatical formalism is proposed , allowing non-terminals to consist of finite sequences of category labels , and allowing schematic variables to range over such sequences .", "ner": [["extension", 1, 1, "Generic"], ["GPSG grammatical formalism", 4, 6, "Method"], ["non-terminals", 11, 11, "OtherScientificTerm"], ["schematic variables", 23, 24, "OtherScientificTerm"]], "relations": [[4, 6, 1, 1, "USED-FOR"]]},
{"sentence": "The extension is shown to be sufficient to provide a strongly adequate grammar for crossed serial dependencies , as found in e.g. Dutch subordinate clauses .", "ner": [["extension", 1, 1, "Generic"], ["grammar", 12, 12, "Method"], ["crossed serial dependencies", 14, 16, "OtherScientificTerm"], ["Dutch subordinate clauses", 22, 24, "OtherScientificTerm"]], "relations": [[1, 1, 12, 12, "USED-FOR"], [12, 12, 14, 16, "USED-FOR"]]},
{"sentence": "The structures induced for such constructions are argued to be more appropriate to data involving conjunction than some previous proposals have been .", "ner": [["constructions", 5, 5, "Task"]], "relations": []},
{"sentence": "The extension is shown to be parseable by a simple extension to an existing parsing method for GPSG .", "ner": [["extension", 1, 1, "Generic"], ["extension", 10, 10, "Generic"], ["parsing method", 14, 15, "Method"], ["GPSG", 17, 17, "Method"]], "relations": [[10, 10, 1, 1, "USED-FOR"], [14, 15, 10, 10, "USED-FOR"], [14, 15, 17, 17, "USED-FOR"]]},
{"sentence": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene .", "ner": [["approach", 4, 4, "Generic"], ["localizing functional objects", 6, 8, "Task"], ["surveillance videos", 10, 11, "Material"], ["domain knowledge", 13, 14, "OtherScientificTerm"], ["semantic object classes", 16, 18, "OtherScientificTerm"]], "relations": [[4, 4, 6, 8, "USED-FOR"], [10, 11, 6, 8, "USED-FOR"], [16, 18, 13, 14, "FEATURE-OF"]]},
{"sentence": "Functional objects do not have discriminative appearance and shape , but they affect behavior of people in the scene .", "ner": [["Functional objects", 0, 1, "OtherScientificTerm"], ["discriminative appearance and shape", 5, 8, "OtherScientificTerm"], ["they", 11, 11, "Generic"]], "relations": []},
{"sentence": "For example , they '' attract '' people to approach them for satisfying certain needs -LRB- e.g. , vending machines could quench thirst -RRB- , or '' repel '' people to avoid them -LRB- e.g. , grass lawns -RRB- .", "ner": [], "relations": []},
{"sentence": "Therefore , functional objects can be viewed as '' dark matter '' , emanating '' dark energy '' that affects people 's trajectories in the video .", "ner": [["functional objects", 2, 3, "OtherScientificTerm"]], "relations": []},
{"sentence": "To detect '' dark matter '' and infer their '' dark energy '' field , we extend the La-grangian mechanics .", "ner": [["La-grangian mechanics", 18, 19, "Method"]], "relations": []},
{"sentence": "People are treated as particle-agents with latent intents to approach '' dark matter '' and thus satisfy their needs , where their motions are subject to a composite '' dark energy '' field of all functional objects in the scene .", "ner": [["functional objects", 35, 36, "OtherScientificTerm"]], "relations": []},
{"sentence": "We make the assumption that people take globally optimal paths toward the intended '' dark matter '' while avoiding latent obstacles .", "ner": [], "relations": []},
{"sentence": "A Bayesian framework is used to probabilistically model : people 's trajectories and intents , constraint map of the scene , and locations of functional objects .", "ner": [["Bayesian framework", 1, 2, "Method"], ["people 's trajectories and intents", 9, 13, "OtherScientificTerm"], ["constraint map of the scene", 15, 19, "OtherScientificTerm"], ["locations of functional objects", 22, 25, "OtherScientificTerm"]], "relations": [[1, 2, 9, 13, "USED-FOR"], [1, 2, 15, 19, "USED-FOR"], [1, 2, 22, 25, "USED-FOR"], [9, 13, 15, 19, "CONJUNCTION"], [15, 19, 22, 25, "CONJUNCTION"]]},
{"sentence": "A data-driven Markov Chain Monte Carlo -LRB- MCMC -RRB- process is used for inference .", "ner": [["data-driven Markov Chain Monte Carlo -LRB- MCMC -RRB- process", 1, 9, "Method"], ["inference", 13, 13, "Task"]], "relations": [[1, 9, 13, 13, "USED-FOR"]]},
{"sentence": "Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people 's trajectories in unobserved parts of the video footage .", "ner": [["videos of public squares and courtyards", 3, 8, "Material"], ["localizing functional objects", 13, 15, "Task"], ["predicting people 's trajectories", 17, 20, "Task"], ["video footage", 26, 27, "OtherScientificTerm"]], "relations": [[3, 8, 13, 15, "EVALUATE-FOR"], [3, 8, 17, 20, "EVALUATE-FOR"], [13, 15, 17, 20, "CONJUNCTION"]]},
{"sentence": "We propose a process model for hierarchical perceptual sound organization , which recognizes perceptual sounds included in incoming sound signals .", "ner": [["process model", 3, 4, "Method"], ["hierarchical perceptual sound organization", 6, 9, "Task"], ["perceptual sounds", 13, 14, "OtherScientificTerm"], ["incoming sound signals", 17, 19, "OtherScientificTerm"]], "relations": [[3, 4, 6, 9, "USED-FOR"], [13, 14, 17, 19, "PART-OF"]]},
{"sentence": "We consider perceptual sound organization as a scene analysis problem in the auditory domain .", "ner": [["perceptual sound organization", 2, 4, "Task"], ["scene analysis problem", 7, 9, "Task"], ["auditory domain", 12, 13, "Material"]], "relations": [[7, 9, 2, 4, "USED-FOR"], [12, 13, 7, 9, "FEATURE-OF"]]},
{"sentence": "Our model consists of multiple processing modules and a hypothesis network for quantitative integration of multiple sources of information .", "ner": [["model", 1, 1, "Generic"], ["processing modules", 5, 6, "Method"], ["hypothesis network", 9, 10, "Method"]], "relations": [[5, 6, 1, 1, "PART-OF"], [5, 6, 9, 10, "CONJUNCTION"], [9, 10, 1, 1, "PART-OF"]]},
{"sentence": "When input information for each processing module is available , the module rises to process it and asynchronously writes output information to the hypothesis network .", "ner": [["processing module", 5, 6, "Method"], ["module", 11, 11, "Generic"], ["hypothesis network", 23, 24, "Method"]], "relations": []},
{"sentence": "On the hypothesis network , individual information is integrated and an optimal internal model of perceptual sounds is automatically constructed .", "ner": [["hypothesis network", 2, 3, "Method"], ["internal model", 12, 13, "Method"], ["perceptual sounds", 15, 16, "Material"]], "relations": [[12, 13, 2, 3, "PART-OF"], [12, 13, 15, 16, "USED-FOR"]]},
{"sentence": "Based on the model , a music scene analysis system has been developed for acoustic signals of ensemble music , which recognizes rhythm , chords , and source-separated musical notes .", "ner": [["model", 3, 3, "Method"], ["music scene analysis system", 6, 9, "Method"], ["acoustic signals of ensemble music", 14, 18, "OtherScientificTerm"], ["rhythm", 22, 22, "Material"], ["chords", 24, 24, "Material"], ["source-separated musical notes", 27, 29, "Material"]], "relations": [[6, 9, 14, 18, "USED-FOR"], [6, 9, 22, 22, "USED-FOR"], [6, 9, 24, 24, "USED-FOR"], [6, 9, 27, 29, "USED-FOR"], [22, 22, 24, 24, "CONJUNCTION"], [24, 24, 27, 29, "CONJUNCTION"]]},
{"sentence": "Experimental results show that our method has permitted autonomous , stable and effective information integration to construct the internal model of hierarchical perceptual sounds .", "ner": [["method", 5, 5, "Generic"], ["information integration", 13, 14, "OtherScientificTerm"], ["internal model", 18, 19, "Method"], ["hierarchical perceptual sounds", 21, 23, "Material"]], "relations": [[13, 14, 5, 5, "FEATURE-OF"], [13, 14, 18, 19, "USED-FOR"], [18, 19, 21, 23, "USED-FOR"]]},
{"sentence": "We directly investigate a subject of much recent debate : do word sense disambigation models help statistical machine translation quality ?", "ner": [["word sense disambigation models", 11, 14, "Method"], ["statistical machine translation quality", 16, 19, "Metric"]], "relations": [[11, 14, 16, 19, "USED-FOR"]]},
{"sentence": "We present empirical results casting doubt on this common , but unproved , assumption .", "ner": [], "relations": []},
{"sentence": "Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .", "ner": [["Chinese word sense disambiguation model", 3, 7, "Method"], ["translation candidates", 10, 11, "OtherScientificTerm"], ["IBM statistical MT system", 15, 18, "Method"], ["word sense disambiguation", 23, 25, "Method"], ["translation quality", 31, 32, "Metric"], ["statistical machine translation system", 35, 38, "Method"]], "relations": [[3, 7, 10, 11, "USED-FOR"], [10, 11, 15, 18, "USED-FOR"], [23, 25, 35, 38, "COMPARE"], [31, 32, 23, 25, "EVALUATE-FOR"], [31, 32, 35, 38, "EVALUATE-FOR"]]},
{"sentence": "Error analysis suggests several key factors behind this surprising finding , including inherent limitations of current statistical MT architectures .", "ner": [["Error analysis", 0, 1, "Method"], ["statistical MT architectures", 16, 18, "Method"]], "relations": []},
{"sentence": "Image sequence processing techniques are used to study exchange , growth , and transport processes and to tackle key questions in environmental physics and biology .", "ner": [["Image sequence processing techniques", 0, 3, "Method"], ["exchange , growth , and transport processes", 8, 14, "Task"], ["environmental physics", 21, 22, "Material"], ["biology", 24, 24, "Material"]], "relations": [[0, 3, 8, 14, "USED-FOR"], [21, 22, 24, 24, "CONJUNCTION"]]},
{"sentence": "These applications require high accuracy for the estimation of the motion field since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects .", "ner": [["applications", 1, 1, "Generic"], ["accuracy", 4, 4, "Metric"], ["estimation of the motion field", 7, 11, "Task"], ["dynamical processes", 19, 20, "Method"], ["first-order derivatives of the motion field", 25, 30, "OtherScientificTerm"], ["dynamical changes of the moving objects", 33, 38, "OtherScientificTerm"]], "relations": [[4, 4, 7, 11, "EVALUATE-FOR"], [7, 11, 1, 1, "USED-FOR"], [25, 30, 33, 38, "CONJUNCTION"]]},
{"sentence": "Therefore the performance and optimization of low-level motion estimators is discussed .", "ner": [["optimization of low-level motion estimators", 4, 8, "Task"]], "relations": []},
{"sentence": "A tensor method tuned with carefully optimized derivative filters yields reliable and dense displacement vector fields -LRB- DVF -RRB- with an accuracy of up to a few hundredth pixels/frame for real-world images .", "ner": [["tensor method", 1, 2, "Method"], ["derivative filters", 7, 8, "Method"], ["displacement vector fields -LRB- DVF -RRB-", 13, 18, "OtherScientificTerm"], ["accuracy", 21, 21, "Metric"], ["pixels/frame", 28, 28, "Metric"], ["real-world images", 30, 31, "Material"]], "relations": [[7, 8, 1, 2, "USED-FOR"], [28, 28, 13, 18, "EVALUATE-FOR"], [30, 31, 28, 28, "USED-FOR"]]},
{"sentence": "The accuracy of the tensor method is verified with computer-generated sequences and a calibrated image sequence .", "ner": [["accuracy", 1, 1, "Metric"], ["tensor method", 4, 5, "Method"], ["computer-generated sequences", 9, 10, "Material"], ["calibrated image sequence", 13, 15, "Material"]], "relations": [[1, 1, 4, 5, "EVALUATE-FOR"], [9, 10, 4, 5, "EVALUATE-FOR"], [9, 10, 13, 15, "CONJUNCTION"], [13, 15, 4, 5, "EVALUATE-FOR"]]},
{"sentence": "With the improvements in accuracy the motion estimation is now rather limited by imperfections in the CCD sensors , especially the spatial nonuni-formity in the responsivity .", "ner": [["accuracy", 4, 4, "Metric"], ["motion estimation", 6, 7, "Task"], ["CCD sensors", 16, 17, "OtherScientificTerm"], ["spatial nonuni-formity", 21, 22, "OtherScientificTerm"], ["responsivity", 25, 25, "OtherScientificTerm"]], "relations": [[4, 4, 6, 7, "EVALUATE-FOR"], [16, 17, 6, 7, "USED-FOR"], [21, 22, 25, 25, "FEATURE-OF"], [25, 25, 16, 17, "FEATURE-OF"]]},
{"sentence": "With a simple two-point calibration , these effects can efficiently be suppressed .", "ner": [["two-point calibration", 3, 4, "Method"]], "relations": []},
{"sentence": "The application of the techniques to the analysis of plant growth , to ocean surface microturbulence in IR image sequences , and to sediment transport is demonstrated .", "ner": [["techniques", 4, 4, "Generic"], ["analysis of plant growth", 7, 10, "Task"], ["ocean surface microturbulence in IR image sequences", 13, 19, "Task"], ["sediment transport", 23, 24, "Task"]], "relations": [[4, 4, 7, 10, "USED-FOR"], [4, 4, 13, 19, "USED-FOR"], [4, 4, 23, 24, "USED-FOR"], [7, 10, 13, 19, "CONJUNCTION"], [13, 19, 23, 24, "CONJUNCTION"]]},
{"sentence": "We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures .", "ner": [["Czech-English statistical machine translation system", 3, 7, "Method"], ["tree-to-tree translation of dependency structures", 10, 14, "Task"]], "relations": [[3, 7, 10, 14, "USED-FOR"]]},
{"sentence": "The only bilingual resource required is a sentence-aligned parallel corpus .", "ner": [["bilingual resource", 2, 3, "Material"], ["sentence-aligned parallel corpus", 7, 9, "Material"]], "relations": [[7, 9, 2, 3, "USED-FOR"]]},
{"sentence": "All other resources are monolingual .", "ner": [], "relations": []},
{"sentence": "We also refer to an evaluation method and plan to compare our system 's output with a benchmark system .", "ner": [["system", 12, 12, "Generic"], ["benchmark system", 17, 18, "Generic"]], "relations": [[12, 12, 17, 18, "COMPARE"]]},
{"sentence": "This paper describes the understanding process of the spatial descriptions in Japanese .", "ner": [["spatial descriptions", 8, 9, "OtherScientificTerm"], ["Japanese", 11, 11, "Material"]], "relations": [[11, 11, 8, 9, "FEATURE-OF"]]},
{"sentence": "In order to understand the described world , the authors try to reconstruct the geometric model of the global scene from the scenic descriptions drawing a space .", "ner": [["geometric model", 14, 15, "Method"]], "relations": []},
{"sentence": "It is done by an experimental computer program SPRINT , which takes natural language texts and produces a model of the described world .", "ner": [["computer program SPRINT", 6, 8, "Method"], ["natural language texts", 12, 14, "Material"], ["model", 18, 18, "Generic"]], "relations": []},
{"sentence": "To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the numerical constraints on the spatial attributes of the entities .", "ner": [["model", 3, 3, "Generic"], ["qualitative spatial constraints", 9, 11, "OtherScientificTerm"], ["numerical constraints", 21, 22, "OtherScientificTerm"], ["spatial attributes of the entities", 25, 29, "OtherScientificTerm"]], "relations": [[25, 29, 21, 22, "USED-FOR"]]},
{"sentence": "This makes it possible to express the vagueness of the spatial concepts and to derive the maximally plausible interpretation from a chunk of information accumulated as the constraints .", "ner": [["spatial concepts", 10, 11, "OtherScientificTerm"]], "relations": []},
{"sentence": "The interpretation reflects the temporary belief about the world .", "ner": [], "relations": []},
{"sentence": "Learning video representation is not a trivial task , as video is an information-intensive media where each frame does not exist independently .", "ner": [["Learning video representation", 0, 2, "Task"]], "relations": []},
{"sentence": "Locally , a video frame is visually and semantically similar with its adjacent frames .", "ner": [], "relations": []},
{"sentence": "Holistically , a video has its inherent structure -- the correlations among video frames .", "ner": [], "relations": []},
{"sentence": "For example , even the frames far from each other may also hold similar semantics .", "ner": [["semantics", 14, 14, "OtherScientificTerm"]], "relations": []},
{"sentence": "Such context information is therefore important to characterize the intrinsic representation of a video frame .", "ner": [["context information", 1, 2, "OtherScientificTerm"], ["intrinsic representation of a video frame", 9, 14, "Method"]], "relations": [[1, 2, 9, 14, "USED-FOR"]]},
{"sentence": "In this paper , we present a novel approach to learn the deep video representation by exploring both local and holistic contexts .", "ner": [["approach", 8, 8, "Generic"], ["deep video representation", 12, 14, "Method"], ["local and holistic contexts", 18, 21, "OtherScientificTerm"]], "relations": [[8, 8, 12, 14, "USED-FOR"], [18, 21, 8, 8, "USED-FOR"]]},
{"sentence": "Specifically , we propose a triplet sampling mechanism to encode the local temporal relationship of adjacent frames based on their deep representations .", "ner": [["triplet sampling mechanism", 5, 7, "Method"], ["local temporal relationship of adjacent frames", 11, 16, "OtherScientificTerm"], ["deep representations", 20, 21, "Method"]], "relations": [[5, 7, 11, 16, "USED-FOR"], [20, 21, 5, 7, "USED-FOR"]]},
{"sentence": "In addition , we incorporate the graph structure of the video , as a priori , to holistically preserve the inherent correlations among video frames .", "ner": [["graph structure of the video", 6, 10, "OtherScientificTerm"], ["priori", 14, 14, "OtherScientificTerm"]], "relations": [[6, 10, 14, 14, "USED-FOR"]]},
{"sentence": "Our approach is fully unsupervised and trained in an end-to-end deep convolutional neu-ral network architecture .", "ner": [["approach", 1, 1, "Generic"], ["end-to-end deep convolutional neu-ral network architecture", 9, 14, "Method"]], "relations": [[9, 14, 1, 1, "USED-FOR"]]},
{"sentence": "By extensive experiments , we show that our learned representation can significantly boost several video recognition tasks -LRB- retrieval , classification , and highlight detection -RRB- over traditional video representations .", "ner": [["learned representation", 8, 9, "Generic"], ["video recognition tasks", 14, 16, "Task"], ["retrieval", 18, 18, "Task"], ["classification", 20, 20, "Task"], ["highlight detection", 23, 24, "Task"], ["video representations", 28, 29, "Generic"]], "relations": [[8, 9, 28, 29, "COMPARE"], [14, 16, 8, 9, "EVALUATE-FOR"], [14, 16, 28, 29, "EVALUATE-FOR"], [18, 18, 14, 16, "HYPONYM-OF"], [18, 18, 20, 20, "CONJUNCTION"], [20, 20, 14, 16, "HYPONYM-OF"], [20, 20, 23, 24, "CONJUNCTION"], [23, 24, 14, 16, "HYPONYM-OF"]]},
{"sentence": "For mobile speech application , speaker DOA estimation accuracy , interference robustness and compact physical size are three key factors .", "ner": [["mobile speech application", 1, 3, "Task"], ["speaker DOA estimation accuracy", 5, 8, "Metric"], ["interference robustness", 10, 11, "Metric"], ["compact physical size", 13, 15, "Metric"]], "relations": [[5, 8, 1, 3, "FEATURE-OF"], [5, 8, 10, 11, "CONJUNCTION"], [10, 11, 1, 3, "FEATURE-OF"], [10, 11, 13, 15, "CONJUNCTION"], [13, 15, 1, 3, "FEATURE-OF"]]},
{"sentence": "Considering the size , we utilized acoustic vector sensor -LRB- AVS -RRB- and proposed a DOA estimation algorithm previously -LSB- 1 -RSB- , offering high accuracy with larger-than-15dB SNR but is deteriorated by non-speech interferences -LRB- NSI -RRB- .", "ner": [["acoustic vector sensor -LRB- AVS -RRB-", 6, 11, "Method"], ["DOA estimation algorithm", 15, 17, "Method"], ["non-speech interferences -LRB- NSI -RRB-", 33, 37, "OtherScientificTerm"]], "relations": []},
{"sentence": "This paper develops a robust speaker DOA estimation algorithm .", "ner": [["robust speaker DOA estimation algorithm", 4, 8, "Method"]], "relations": []},
{"sentence": "It is achieved by deriving the inter-sensor data ratio model of an AVS in bispectrum domain -LRB- BISDR -RRB- and exploring the favorable properties of bispectrum , such as zero value of Gaussian process and different distribution of speech and NSI .", "ner": [["It", 0, 0, "Generic"], ["inter-sensor data ratio model", 6, 9, "Method"], ["AVS", 12, 12, "Method"], ["bispectrum domain -LRB- BISDR -RRB-", 14, 18, "Method"], ["favorable properties", 22, 23, "Generic"], ["bispectrum", 25, 25, "Method"], ["zero value of Gaussian process", 29, 33, "OtherScientificTerm"], ["distribution of speech and NSI", 36, 40, "OtherScientificTerm"], ["NSI", 40, 40, "OtherScientificTerm"]], "relations": [[0, 0, 22, 23, "USED-FOR"], [6, 9, 12, 12, "USED-FOR"], [14, 18, 12, 12, "USED-FOR"], [29, 33, 22, 23, "HYPONYM-OF"], [29, 33, 36, 40, "CONJUNCTION"], [36, 40, 22, 23, "HYPONYM-OF"]]},
{"sentence": "Specifically , a reliable bispectrum mask is generated to guarantee that the speaker DOA cues , derived from BISDR , are robust to NSI in terms of speech sparsity and large bispectrum amplitude of the captured signals .", "ner": [["bispectrum mask", 4, 5, "Method"], ["speaker DOA cues", 12, 14, "OtherScientificTerm"], ["BISDR", 18, 18, "Method"], ["NSI", 23, 23, "OtherScientificTerm"], ["speech sparsity", 27, 28, "OtherScientificTerm"], ["bispectrum amplitude", 31, 32, "OtherScientificTerm"]], "relations": [[4, 5, 12, 14, "USED-FOR"], [18, 18, 12, 14, "USED-FOR"]]},
{"sentence": "Intensive experiments demonstrate an improved performance of our proposed algorithm under various NSI conditions even when SIR is smaller than 0dB .", "ner": [["algorithm", 9, 9, "Generic"], ["NSI conditions", 12, 13, "OtherScientificTerm"], ["SIR", 16, 16, "Metric"]], "relations": [[9, 9, 12, 13, "USED-FOR"]]},
{"sentence": "In this paper , we want to show how the morphological component of an existing NLP-system for Dutch -LRB- Dutch Medical Language Processor - DMLP -RRB- has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system -LRB- Linguistic String Project - Medical Language Processor -RRB- of the New York University .", "ner": [["morphological component", 10, 11, "Method"], ["NLP-system for Dutch -LRB- Dutch Medical Language Processor - DMLP -RRB-", 15, 25, "Method"], ["language independent modules", 39, 41, "Method"], ["LSP-MLP system -LRB- Linguistic String Project - Medical Language Processor -RRB-", 44, 54, "Method"]], "relations": [[10, 11, 15, 25, "PART-OF"], [39, 41, 44, 54, "PART-OF"]]},
{"sentence": "The former can take advantage of the language independent developments of the latter , while focusing on idiosyncrasies for Dutch .", "ner": [["former", 1, 1, "Generic"], ["latter", 12, 12, "Generic"], ["idiosyncrasies", 17, 17, "OtherScientificTerm"], ["Dutch", 19, 19, "Material"]], "relations": [[12, 12, 1, 1, "USED-FOR"], [19, 19, 17, 17, "USED-FOR"]]},
{"sentence": "This general strategy will be illustrated by a practical application , namely the highlighting of relevant information in a patient discharge summary -LRB- PDS -RRB- by means of modern HyperText Mark-Up Language -LRB- HTML -RRB- technology .", "ner": [["application", 9, 9, "Generic"], ["highlighting of relevant information", 13, 16, "Task"], ["relevant information", 15, 16, "Generic"], ["patient discharge summary -LRB- PDS -RRB-", 19, 24, "OtherScientificTerm"], ["HyperText Mark-Up Language -LRB- HTML -RRB- technology", 29, 35, "Method"]], "relations": [[15, 16, 19, 24, "PART-OF"], [29, 35, 13, 16, "USED-FOR"]]},
{"sentence": "Such an application can be of use for medical administrative purposes in a hospital environment .", "ner": [["application", 2, 2, "Generic"], ["medical administrative purposes", 8, 10, "Task"]], "relations": [[2, 2, 8, 10, "USED-FOR"]]},
{"sentence": "CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements -LRB- e.g. , thesis statements -RRB- .", "ner": [["CriterionSM Online Essay Evaluation Service", 0, 4, "Task"], ["essay-based discourse elements", 15, 17, "OtherScientificTerm"], ["thesis statements", 21, 22, "OtherScientificTerm"]], "relations": [[15, 17, 0, 4, "PART-OF"], [21, 22, 15, 17, "HYPONYM-OF"]]},
{"sentence": "We describe a new system that enhances Criterion 's capability , by evaluating multiple aspects of coherence in essays .", "ner": [["system", 4, 4, "Generic"], ["Criterion 's capability", 7, 9, "OtherScientificTerm"], ["coherence in essays", 16, 18, "Metric"]], "relations": [[4, 4, 7, 9, "USED-FOR"], [16, 18, 4, 4, "EVALUATE-FOR"]]},
{"sentence": "This system identifies features of sentences based on semantic similarity measures and discourse structure .", "ner": [["system", 1, 1, "Generic"], ["features", 3, 3, "OtherScientificTerm"], ["semantic similarity measures", 8, 10, "Metric"], ["discourse structure", 12, 13, "OtherScientificTerm"]], "relations": [[1, 1, 3, 3, "USED-FOR"], [8, 10, 3, 3, "USED-FOR"], [12, 13, 3, 3, "USED-FOR"], [12, 13, 8, 10, "CONJUNCTION"]]},
{"sentence": "A support vector machine uses these features to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements .", "ner": [["support vector machine", 1, 3, "Method"], ["features", 6, 6, "OtherScientificTerm"], ["breakdowns in coherence", 9, 11, "OtherScientificTerm"], ["discourse elements", 22, 23, "OtherScientificTerm"]], "relations": [[6, 6, 1, 3, "USED-FOR"], [6, 6, 9, 11, "USED-FOR"]]},
{"sentence": "Intra-sentential quality is evaluated with rule-based heuristics .", "ner": [["Intra-sentential quality", 0, 1, "Metric"], ["rule-based heuristics", 5, 6, "Method"]], "relations": [[5, 6, 0, 1, "EVALUATE-FOR"]]},
{"sentence": "Results indicate that the system yields higher performance than a baseline on all three aspects .", "ner": [["system", 4, 4, "Generic"], ["baseline", 10, 10, "Generic"]], "relations": [[4, 4, 10, 10, "COMPARE"]]},
{"sentence": "This paper presents an algorithm for labeling curvilinear structure at multiple scales in line drawings and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "ner": [["algorithm", 4, 4, "Generic"], ["labeling curvilinear structure", 6, 8, "Task"], ["line drawings", 13, 14, "Material"], ["edge images", 16, 17, "Material"], ["CURVE-ELEMENT tokens", 19, 20, "OtherScientificTerm"], ["spatially-indexed and scale-indexed data structure", 24, 28, "OtherScientificTerm"], ["image data", 34, 35, "Material"]], "relations": [[4, 4, 6, 8, "USED-FOR"], [13, 14, 6, 8, "FEATURE-OF"], [13, 14, 16, 17, "CONJUNCTION"], [16, 17, 6, 8, "FEATURE-OF"], [19, 20, 24, 28, "PART-OF"]]},
{"sentence": "Tokens are computed via a small-to-large scale grouping procedure employing a '' greedy '' , best-first , strategy for choosing the support of new tokens .", "ner": [["small-to-large scale grouping procedure", 5, 8, "Method"]], "relations": []},
{"sentence": "The resulting image description is rich and redundant in that a given segment of image contour may be described by multiple tokens at different scales , and by more than one token at any given scale .", "ner": [["image description", 2, 3, "Task"], ["image contour", 14, 15, "OtherScientificTerm"]], "relations": []},
{"sentence": "This property facilitates selection and characterization of portions of the image based on local CURVE-ELEMENT attributes .", "ner": [["local CURVE-ELEMENT attributes", 13, 15, "OtherScientificTerm"]], "relations": []}]